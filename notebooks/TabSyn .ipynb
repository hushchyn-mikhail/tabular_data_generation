{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "XrjvUbRigW4t"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### download_dataset.py"
      ],
      "metadata": {
        "id": "SzEnjXKGY3Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy\n",
        "import pandas as pd\n",
        "from urllib import request\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "DATA_DIR = 'data'\n",
        "\n",
        "\n",
        "NAME_URL_DICT_UCI = {\n",
        "    'adult': 'https://archive.ics.uci.edu/static/public/2/adult.zip'\n",
        "}\n",
        "\n",
        "def unzip_file(zip_filepath, dest_path):\n",
        "    with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dest_path)\n",
        "\n",
        "\n",
        "def download_from_uci(name):\n",
        "\n",
        "    print(f'Start processing dataset {name} from UCI.')\n",
        "    save_dir = f'{DATA_DIR}/{name}'\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "        url = NAME_URL_DICT_UCI[name]\n",
        "        request.urlretrieve(url, f'{save_dir}/{name}.zip')\n",
        "        print(f'Finish downloading dataset from {url}, data has been saved to {save_dir}.')\n",
        "\n",
        "        unzip_file(f'{save_dir}/{name}.zip', save_dir)\n",
        "        print(f'Finish unzipping {name}.')\n",
        "\n",
        "    else:\n",
        "        print('Aready downloaded.')"
      ],
      "metadata": {
        "id": "z4SjgPstZDcT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_from_uci('adult')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcGDczOupXiP",
        "outputId": "a8ccd9b6-ca5a-462d-e98e-8c8ad0b79979"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start processing dataset adult from UCI.\n",
            "Finish downloading dataset from https://archive.ics.uci.edu/static/public/2/adult.zip, data has been saved to data/adult.\n",
            "Finish unzipping adult.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGsACNBSHvff",
        "outputId": "ff24258a-5d2d-4bb0-b2be-f8aa9f0facf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### process_dataset.py"
      ],
      "metadata": {
        "id": "9Y4KFR9kYhVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import argparse\n",
        "\n",
        "TYPE_TRANSFORM ={\n",
        "    'float', np.float32,\n",
        "    'str', str,\n",
        "    'int', int\n",
        "}\n",
        "\n",
        "INFO_PATH = 'data/Info'\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='process dataset')\n",
        "\n",
        "# # General configs\n",
        "# parser.add_argument('--dataname', type=str, default=None, help='Name of dataset.')\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# def preprocess_beijing():\n",
        "#     # with open(f'{INFO_PATH}/beijing.json', 'r') as f:\n",
        "#     #     info = json.load(f)\n",
        "\n",
        "#     # data_path = info['raw_data_path']\n",
        "\n",
        "#     data_df = pd.read_csv(data_path)\n",
        "#     columns = data_df.columns\n",
        "\n",
        "#     data_df = data_df[columns[1:]]\n",
        "\n",
        "\n",
        "#     df_cleaned = data_df.dropna()\n",
        "#     df_cleaned.to_csv(info['data_path'], index = False)\n",
        "\n",
        "# def preprocess_news():\n",
        "#     with open(f'{INFO_PATH}/news.json', 'r') as f:\n",
        "#         info = json.load(f)\n",
        "\n",
        "#     data_path = info['raw_data_path']\n",
        "#     data_df = pd.read_csv(data_path)\n",
        "#     data_df = data_df.drop('url', axis=1)\n",
        "\n",
        "#     columns = np.array(data_df.columns.tolist())\n",
        "\n",
        "#     cat_columns1 = columns[list(range(12,18))]\n",
        "#     cat_columns2 = columns[list(range(30,38))]\n",
        "\n",
        "#     cat_col1 = data_df[cat_columns1].astype(int).to_numpy().argmax(axis = 1)\n",
        "#     cat_col2 = data_df[cat_columns2].astype(int).to_numpy().argmax(axis = 1)\n",
        "\n",
        "#     data_df = data_df.drop(cat_columns2, axis=1)\n",
        "#     data_df = data_df.drop(cat_columns1, axis=1)\n",
        "\n",
        "#     data_df['data_channel'] = cat_col1\n",
        "#     data_df['weekday'] = cat_col2\n",
        "\n",
        "#     data_save_path = 'data/news/news.csv'\n",
        "#     data_df.to_csv(f'{data_save_path}', index = False)\n",
        "\n",
        "#     columns = np.array(data_df.columns.tolist())\n",
        "#     num_columns = columns[list(range(45))]\n",
        "#     cat_columns = ['data_channel', 'weekday']\n",
        "#     target_columns = columns[[45]]\n",
        "\n",
        "#     info['num_col_idx'] = list(range(45))\n",
        "#     info['cat_col_idx'] = [46, 47]\n",
        "#     info['target_col_idx'] = [45]\n",
        "#     info['data_path'] = data_save_path\n",
        "\n",
        "#     name = 'news'\n",
        "#     with open(f'{INFO_PATH}/{name}.json', 'w') as file:\n",
        "#         json.dump(info, file, indent=4)\n",
        "\n",
        "\n",
        "def get_column_name_mapping(data_df, num_col_idx, cat_col_idx, target_col_idx, column_names = None):\n",
        "\n",
        "    if not column_names:\n",
        "        column_names = np.array(data_df.columns.tolist())\n",
        "\n",
        "\n",
        "    idx_mapping = {}\n",
        "\n",
        "    curr_num_idx = 0\n",
        "    curr_cat_idx = len(num_col_idx)\n",
        "    curr_target_idx = curr_cat_idx + len(cat_col_idx)\n",
        "\n",
        "    for idx in range(len(column_names)):\n",
        "\n",
        "        if idx in num_col_idx:\n",
        "            idx_mapping[int(idx)] = curr_num_idx\n",
        "            curr_num_idx += 1\n",
        "        elif idx in cat_col_idx:\n",
        "            idx_mapping[int(idx)] = curr_cat_idx\n",
        "            curr_cat_idx += 1\n",
        "        else:\n",
        "            idx_mapping[int(idx)] = curr_target_idx\n",
        "            curr_target_idx += 1\n",
        "\n",
        "\n",
        "    inverse_idx_mapping = {}\n",
        "    for k, v in idx_mapping.items():\n",
        "        inverse_idx_mapping[int(v)] = k\n",
        "\n",
        "    idx_name_mapping = {}\n",
        "\n",
        "    for i in range(len(column_names)):\n",
        "        idx_name_mapping[int(i)] = column_names[i]\n",
        "\n",
        "    return idx_mapping, inverse_idx_mapping, idx_name_mapping\n",
        "\n",
        "\n",
        "def train_val_test_split(data_df, cat_columns, num_train = 0, num_test = 0):\n",
        "    total_num = data_df.shape[0]\n",
        "    idx = np.arange(total_num)\n",
        "\n",
        "\n",
        "    seed = 1234\n",
        "\n",
        "    while True:\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(idx)\n",
        "\n",
        "        train_idx = idx[:num_train]\n",
        "        test_idx = idx[-num_test:]\n",
        "\n",
        "\n",
        "        train_df = data_df.loc[train_idx]\n",
        "        test_df = data_df.loc[test_idx]\n",
        "\n",
        "\n",
        "\n",
        "        flag = 0\n",
        "        for i in cat_columns:\n",
        "            if len(set(train_df[i])) != len(set(data_df[i])):\n",
        "                flag = 1\n",
        "                break\n",
        "\n",
        "        if flag == 0:\n",
        "            break\n",
        "        else:\n",
        "            seed += 1\n",
        "\n",
        "    return train_df, test_df, seed\n",
        "\n",
        "\n",
        "def process_data(name):\n",
        "\n",
        "    # if name == 'news':\n",
        "    #     preprocess_news()\n",
        "    # elif name == 'beijing':\n",
        "    #     preprocess_beijing()\n",
        "\n",
        "    with open(f'{INFO_PATH}/{name}.json', 'r') as f:\n",
        "        info = json.load(f)\n",
        "\n",
        "    data_path = info['data_path']\n",
        "    if info['file_type'] == 'csv':\n",
        "        data_df = pd.read_csv(data_path, header = info['header'])\n",
        "\n",
        "    elif info['file_type'] == 'xls':\n",
        "        data_df = pd.read_excel(data_path, sheet_name='Data', header=1)\n",
        "        data_df = data_df.drop('ID', axis=1)\n",
        "\n",
        "    num_data = data_df.shape[0]\n",
        "\n",
        "    column_names = info['column_names'] if info['column_names'] else data_df.columns.tolist()\n",
        "\n",
        "    num_col_idx = info['num_col_idx']\n",
        "    cat_col_idx = info['cat_col_idx']\n",
        "    target_col_idx = info['target_col_idx']\n",
        "\n",
        "    idx_mapping, inverse_idx_mapping, idx_name_mapping = get_column_name_mapping(data_df, num_col_idx, cat_col_idx, target_col_idx, column_names)\n",
        "\n",
        "    num_columns = [column_names[i] for i in num_col_idx]\n",
        "    cat_columns = [column_names[i] for i in cat_col_idx]\n",
        "    target_columns = [column_names[i] for i in target_col_idx]\n",
        "\n",
        "    if info['test_path']:\n",
        "\n",
        "        # if testing data is given\n",
        "        test_path = info['test_path']\n",
        "\n",
        "        with open(test_path, 'r') as f:\n",
        "            lines = f.readlines()[1:]\n",
        "            test_save_path = f'data/{name}/test.data'\n",
        "            if not os.path.exists(test_save_path):\n",
        "                with open(test_save_path, 'a') as f1:\n",
        "                    for line in lines:\n",
        "                        save_line = line.strip('\\n').strip('.')\n",
        "                        f1.write(f'{save_line}\\n')\n",
        "\n",
        "        test_df = pd.read_csv(test_save_path, header = None)\n",
        "        train_df = data_df\n",
        "\n",
        "    else:\n",
        "        # Train/ Test Split, 90% Training, 10% Testing (Validation set will be selected from Training set)\n",
        "\n",
        "        num_train = int(num_data*0.9)\n",
        "        num_test = num_data - num_train\n",
        "\n",
        "        train_df, test_df, seed = train_val_test_split(data_df, cat_columns, num_train, num_test)\n",
        "\n",
        "\n",
        "    train_df.columns = range(len(train_df.columns))\n",
        "    test_df.columns = range(len(test_df.columns))\n",
        "\n",
        "    print(name, train_df.shape, test_df.shape, data_df.shape)\n",
        "\n",
        "    col_info = {}\n",
        "\n",
        "    for col_idx in num_col_idx:\n",
        "        col_info[col_idx] = {}\n",
        "        col_info['type'] = 'numerical'\n",
        "        col_info['max'] = float(train_df[col_idx].max())\n",
        "        col_info['min'] = float(train_df[col_idx].min())\n",
        "\n",
        "    for col_idx in cat_col_idx:\n",
        "        col_info[col_idx] = {}\n",
        "        col_info['type'] = 'categorical'\n",
        "        col_info['categorizes'] = list(set(train_df[col_idx]))\n",
        "\n",
        "    for col_idx in target_col_idx:\n",
        "        if info['task_type'] == 'regression':\n",
        "            col_info[col_idx] = {}\n",
        "            col_info['type'] = 'numerical'\n",
        "            col_info['max'] = float(train_df[col_idx].max())\n",
        "            col_info['min'] = float(train_df[col_idx].min())\n",
        "        else:\n",
        "            col_info[col_idx] = {}\n",
        "            col_info['type'] = 'categorical'\n",
        "            col_info['categorizes'] = list(set(train_df[col_idx]))\n",
        "\n",
        "    info['column_info'] = col_info\n",
        "\n",
        "    train_df.rename(columns = idx_name_mapping, inplace=True)\n",
        "    test_df.rename(columns = idx_name_mapping, inplace=True)\n",
        "\n",
        "    for col in num_columns:\n",
        "        train_df.loc[train_df[col] == '?', col] = np.nan\n",
        "    for col in cat_columns:\n",
        "        train_df.loc[train_df[col] == '?', col] = 'nan'\n",
        "    for col in num_columns:\n",
        "        test_df.loc[test_df[col] == '?', col] = np.nan\n",
        "    for col in cat_columns:\n",
        "        test_df.loc[test_df[col] == '?', col] = 'nan'\n",
        "\n",
        "\n",
        "\n",
        "    X_num_train = train_df[num_columns].to_numpy().astype(np.float32)\n",
        "    X_cat_train = train_df[cat_columns].to_numpy()\n",
        "    y_train = train_df[target_columns].to_numpy()\n",
        "\n",
        "\n",
        "    X_num_test = test_df[num_columns].to_numpy().astype(np.float32)\n",
        "    X_cat_test = test_df[cat_columns].to_numpy()\n",
        "    y_test = test_df[target_columns].to_numpy()\n",
        "\n",
        "\n",
        "    save_dir = f'data/{name}'\n",
        "    np.save(f'{save_dir}/X_num_train.npy', X_num_train)\n",
        "    np.save(f'{save_dir}/X_cat_train.npy', X_cat_train)\n",
        "    np.save(f'{save_dir}/y_train.npy', y_train)\n",
        "\n",
        "    np.save(f'{save_dir}/X_num_test.npy', X_num_test)\n",
        "    np.save(f'{save_dir}/X_cat_test.npy', X_cat_test)\n",
        "    np.save(f'{save_dir}/y_test.npy', y_test)\n",
        "\n",
        "    train_df[num_columns] = train_df[num_columns].astype(np.float32)\n",
        "    test_df[num_columns] = test_df[num_columns].astype(np.float32)\n",
        "\n",
        "\n",
        "    train_df.to_csv(f'{save_dir}/train.csv', index = False)\n",
        "    test_df.to_csv(f'{save_dir}/test.csv', index = False)\n",
        "\n",
        "    if not os.path.exists(f'synthetic/{name}'):\n",
        "        os.makedirs(f'synthetic/{name}')\n",
        "\n",
        "    train_df.to_csv(f'synthetic/{name}/real.csv', index = False)\n",
        "    test_df.to_csv(f'synthetic/{name}/test.csv', index = False)\n",
        "\n",
        "    print('Numerical', X_num_train.shape)\n",
        "    print('Categorical', X_cat_train.shape)\n",
        "\n",
        "    info['column_names'] = column_names\n",
        "    info['train_num'] = train_df.shape[0]\n",
        "    info['test_num'] = test_df.shape[0]\n",
        "\n",
        "    info['idx_mapping'] = idx_mapping\n",
        "    info['inverse_idx_mapping'] = inverse_idx_mapping\n",
        "    info['idx_name_mapping'] = idx_name_mapping\n",
        "\n",
        "    metadata = {'columns': {}}\n",
        "    task_type = info['task_type']\n",
        "    num_col_idx = info['num_col_idx']\n",
        "    cat_col_idx = info['cat_col_idx']\n",
        "    target_col_idx = info['target_col_idx']\n",
        "\n",
        "    for i in num_col_idx:\n",
        "        metadata['columns'][i] = {}\n",
        "        metadata['columns'][i]['sdtype'] = 'numerical'\n",
        "        metadata['columns'][i]['computer_representation'] = 'Float'\n",
        "\n",
        "    for i in cat_col_idx:\n",
        "        metadata['columns'][i] = {}\n",
        "        metadata['columns'][i]['sdtype'] = 'categorical'\n",
        "\n",
        "\n",
        "    if task_type == 'regression':\n",
        "\n",
        "        for i in target_col_idx:\n",
        "            metadata['columns'][i] = {}\n",
        "            metadata['columns'][i]['sdtype'] = 'numerical'\n",
        "            metadata['columns'][i]['computer_representation'] = 'Float'\n",
        "\n",
        "    else:\n",
        "        for i in target_col_idx:\n",
        "            metadata['columns'][i] = {}\n",
        "            metadata['columns'][i]['sdtype'] = 'categorical'\n",
        "\n",
        "    info['metadata'] = metadata\n",
        "\n",
        "    with open(f'{save_dir}/info.json', 'w') as file:\n",
        "        json.dump(info, file, indent=4)\n",
        "\n",
        "    print(f'Processing and Saving {name} Successfully!')\n",
        "\n",
        "    print(name)\n",
        "    print('Total', info['train_num'] + info['test_num'])\n",
        "    print('Train', info['train_num'])\n",
        "    print('Test', info['test_num'])\n",
        "    if info['task_type'] == 'regression':\n",
        "        num = len(info['num_col_idx'] + info['target_col_idx'])\n",
        "        cat = len(info['cat_col_idx'])\n",
        "    else:\n",
        "        cat = len(info['cat_col_idx'] + info['target_col_idx'])\n",
        "        num = len(info['num_col_idx'])\n",
        "    print('Num', num)\n",
        "    print('Cat', cat)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "\n",
        "#     if args.dataname:\n",
        "#         process_data(args.dataname)\n",
        "#     else:\n",
        "#         for name in ['adult', 'default', 'shoppers', 'magic', 'beijing', 'news']:\n",
        "#             process_data(name)\n"
      ],
      "metadata": {
        "id": "9Ovhe41SYZ1Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "!gdown 11L0bU7Gf_dQ7eo56Ru4PdCjRuPkuFoKj\n",
        "os.makedirs('/content/data/Info')\n",
        "os.makedirs('/content/model')\n",
        "os.makedirs('/content/sample')"
      ],
      "metadata": {
        "id": "Q7Gey1I-sHBJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv adult.json data/Info/adult.json"
      ],
      "metadata": {
        "id": "oiOdt24sKaBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_data('adult')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV6D0PZQT7X_",
        "outputId": "ab54496b-a6d5-45c5-b078-85f1f9ec81e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adult (32561, 15) (16281, 15) (32561, 15)\n",
            "Numerical (32561, 6)\n",
            "Categorical (32561, 8)\n",
            "Processing and Saving adult Successfully!\n",
            "adult\n",
            "Total 48842\n",
            "Train 32561\n",
            "Test 16281\n",
            "Num 6\n",
            "Cat 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train VAE first python main.py --dataname [NAME_OF_DATASET] --method vae --mode train\n",
        "\n",
        "# after the VAE is trained, train the diffusion model python main.py --dataname [NAME_OF_DATASET] --method tabsyn --mode train"
      ],
      "metadata": {
        "id": "4-3QGWtiZek7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tabsyn.vae.main\n"
      ],
      "metadata": {
        "id": "hakYR-w7b3fY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### import"
      ],
      "metadata": {
        "id": "sgrudkNlm3Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "# import argparse\n",
        "# import warnings\n",
        "\n",
        "# import os\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import time\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "from copy import deepcopy\n",
        "from dataclasses import astuple, dataclass, replace\n",
        "from importlib.resources import path\n",
        "from pathlib import Path\n",
        "from typing import Any, Literal, Optional, Union, cast, Tuple, Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import sklearn.preprocessing\n",
        "import torch\n",
        "import os\n",
        "# from category_encoders import LeaveOneOutEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.spatial.distance import cdist\n",
        "import enum\n",
        "# from . import env, util\n",
        "# from .metrics import calculate_metrics as calculate_metrics_\n",
        "# from .util import TaskType, load_json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as nn_init\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "import typing as ty\n",
        "import math\n",
        "ArrayDict = Dict[str, np.ndarray]\n",
        "TensorDict = Dict[str, torch.Tensor]\n",
        "\n",
        "\n",
        "CAT_MISSING_VALUE = 'nan'\n",
        "CAT_RARE_VALUE = '__rare__'\n",
        "Normalization = Literal['standard', 'quantile', 'minmax']\n",
        "NumNanPolicy = Literal['drop-rows', 'mean']\n",
        "CatNanPolicy = Literal['most_frequent']\n",
        "CatEncoding = Literal['one-hot', 'counter']\n",
        "YPolicy = Literal['default']"
      ],
      "metadata": {
        "id": "u5GERBafb9U4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8AVQyvg9zjZD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### def load_json"
      ],
      "metadata": {
        "id": "_JFxuRFFzzHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json(path: Union[Path, str], **kwargs) -> Any:\n",
        "    return json.loads(Path(path).read_text(), **kwargs)"
      ],
      "metadata": {
        "id": "j1q9PHdOzmay"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder"
      ],
      "metadata": {
        "id": "GAnAGmXqcgii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_model(nn.Module):\n",
        "    def __init__(self, num_layers, d_numerical, categories, d_token, n_head, factor, bias = True):\n",
        "        super(Encoder_model, self).__init__()\n",
        "        self.Tokenizer = Tokenizer(d_numerical, categories, d_token, bias)\n",
        "        self.VAE_Encoder = Transformer(num_layers, d_token, n_head, d_token, factor)\n",
        "\n",
        "    def load_weights(self, Pretrained_VAE):\n",
        "        self.Tokenizer.load_state_dict(Pretrained_VAE.VAE.Tokenizer.state_dict())\n",
        "        self.VAE_Encoder.load_state_dict(Pretrained_VAE.VAE.encoder_mu.state_dict())\n",
        "\n",
        "    def forward(self, x_num, x_cat):\n",
        "        x = self.Tokenizer(x_num, x_cat)\n",
        "        z = self.VAE_Encoder(x)\n",
        "\n",
        "        return z"
      ],
      "metadata": {
        "id": "cBOEXDklcZG2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder"
      ],
      "metadata": {
        "id": "c0LPWB9wcswV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder_model(nn.Module):\n",
        "    def __init__(self, num_layers, d_numerical, categories, d_token, n_head, factor, bias = True):\n",
        "        super(Decoder_model, self).__init__()\n",
        "        self.VAE_Decoder = Transformer(num_layers, d_token, n_head, d_token, factor)\n",
        "        self.Detokenizer = Reconstructor(d_numerical, categories, d_token)\n",
        "\n",
        "    def load_weights(self, Pretrained_VAE):\n",
        "        self.VAE_Decoder.load_state_dict(Pretrained_VAE.VAE.decoder.state_dict())\n",
        "        self.Detokenizer.load_state_dict(Pretrained_VAE.Reconstructor.state_dict())\n",
        "\n",
        "    def forward(self, z):\n",
        "\n",
        "        h = self.VAE_Decoder(z)\n",
        "        x_hat_num, x_hat_cat = self.Detokenizer(h)\n",
        "\n",
        "        return x_hat_num, x_hat_cat"
      ],
      "metadata": {
        "id": "8Yv7TrAccqGS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### def get_categories"
      ],
      "metadata": {
        "id": "EMpXdiyddZuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_categories(X_train_cat):\n",
        "    return (\n",
        "        None\n",
        "        if X_train_cat is None\n",
        "        else [\n",
        "            len(set(X_train_cat[:, i]))\n",
        "            for i in range(X_train_cat.shape[1])\n",
        "        ]\n",
        "    )"
      ],
      "metadata": {
        "id": "adeblzaZeDZD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### class Transformations"
      ],
      "metadata": {
        "id": "qYYGYmrgyzRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass(frozen=True)\n",
        "class Transformations:\n",
        "    seed: int = 0\n",
        "    normalization: Optional[Normalization] = None\n",
        "    num_nan_policy: Optional[NumNanPolicy] = None\n",
        "    cat_nan_policy: Optional[CatNanPolicy] = None\n",
        "    cat_min_frequency: Optional[float] = None\n",
        "    cat_encoding: Optional[CatEncoding] = None\n",
        "    y_policy: Optional[YPolicy] = 'default'"
      ],
      "metadata": {
        "id": "SWUZxsk3e5Mz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### def read_pure_data"
      ],
      "metadata": {
        "id": "9SM0nAYyyu-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pure_data(path, split='train'):\n",
        "    y = np.load(os.path.join(path, f'y_{split}.npy'), allow_pickle=True)\n",
        "    X_num = None\n",
        "    X_cat = None\n",
        "    if os.path.exists(os.path.join(path, f'X_num_{split}.npy')):\n",
        "        X_num = np.load(os.path.join(path, f'X_num_{split}.npy'), allow_pickle=True)\n",
        "    if os.path.exists(os.path.join(path, f'X_cat_{split}.npy')):\n",
        "        X_cat = np.load(os.path.join(path, f'X_cat_{split}.npy'), allow_pickle=True)\n",
        "\n",
        "    return X_num, X_cat, y"
      ],
      "metadata": {
        "id": "5R4q723xfkeV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### class TaskType"
      ],
      "metadata": {
        "id": "2S7rA1zhz5Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TaskType(enum.Enum):\n",
        "    BINCLASS = 'binclass'\n",
        "    MULTICLASS = 'multiclass'\n",
        "    REGRESSION = 'regression'\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return self.value"
      ],
      "metadata": {
        "id": "EwIpNQcJgAdL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### def get_category_sizes"
      ],
      "metadata": {
        "id": "4Vl_Mfj9yqPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_category_sizes(X: Union[torch.Tensor, np.ndarray]) -> List[int]:\n",
        "    XT = X.T.cpu().tolist() if isinstance(X, torch.Tensor) else X.T.tolist()\n",
        "    return [len(set(x)) for x in XT]"
      ],
      "metadata": {
        "id": "WR4RzNeHgc6w"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### def metrics"
      ],
      "metadata": {
        "id": "tYxL3vsGyncX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "from typing import Any, Optional, Tuple, Dict, Union, cast\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import scipy.special\n",
        "import sklearn.metrics as skm\n",
        "class PredictionType(enum.Enum):\n",
        "    LOGITS = 'logits'\n",
        "    PROBS = 'probs'\n",
        "def calculate_rmse(\n",
        "    y_true: np.ndarray, y_pred: np.ndarray, std = None) -> float:\n",
        "    rmse = skm.mean_squared_error(y_true, y_pred) ** 0.5\n",
        "    if std is not None:\n",
        "        rmse *= std\n",
        "    return rmse\n",
        "def calculate_metrics(\n",
        "    y_true: np.ndarray,\n",
        "    y_pred: np.ndarray,\n",
        "    task_type: Union[str, TaskType],\n",
        "    prediction_type: Optional[Union[str, PredictionType]],\n",
        "    y_info: Dict[str, Any],\n",
        ") -> Dict[str, Any]:\n",
        "    # Example: calculate_metrics(y_true, y_pred, 'binclass', 'logits', {})\n",
        "    task_type = TaskType(task_type)\n",
        "    if prediction_type is not None:\n",
        "        prediction_type = PredictionType(prediction_type)\n",
        "\n",
        "    if task_type == TaskType.REGRESSION:\n",
        "        assert prediction_type is None\n",
        "        assert 'std' in y_info\n",
        "        rmse = calculate_rmse(y_true, y_pred, y_info['std'])\n",
        "        r2 = skm.r2_score(y_true, y_pred)\n",
        "        result = {'rmse': rmse, 'r2': r2}\n",
        "    else:\n",
        "        labels, probs = _get_labels_and_probs(y_pred, task_type, prediction_type)\n",
        "        result = cast(\n",
        "            Dict[str, Any], skm.classification_report(y_true, labels, output_dict=True)\n",
        "        )\n",
        "        if task_type == TaskType.BINCLASS:\n",
        "            result['roc_auc'] = skm.roc_auc_score(y_true, probs)\n",
        "    return result"
      ],
      "metadata": {
        "id": "uS8WDEBBg1Ze"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Class Dataset"
      ],
      "metadata": {
        "id": "pDxM0O5SyJX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass(frozen=False)\n",
        "class Dataset:\n",
        "    X_num: Optional[ArrayDict]\n",
        "    X_cat: Optional[ArrayDict]\n",
        "    y: ArrayDict\n",
        "    y_info: Dict[str, Any]\n",
        "    task_type: TaskType\n",
        "    n_classes: Optional[int]\n",
        "\n",
        "    @classmethod\n",
        "    def from_dir(cls, dir_: Union[Path, str]) -> 'Dataset':\n",
        "        dir_ = Path(dir_)\n",
        "        splits = [k for k in ['train', 'test'] if dir_.joinpath(f'y_{k}.npy').exists()]\n",
        "\n",
        "        def load(item) -> ArrayDict:\n",
        "            return {\n",
        "                x: cast(np.ndarray, np.load(dir_ / f'{item}_{x}.npy', allow_pickle=True))  # type: ignore[code]\n",
        "                for x in splits\n",
        "            }\n",
        "\n",
        "        if Path(dir_ / 'info.json').exists():\n",
        "            info = util.load_json(dir_ / 'info.json')\n",
        "        else:\n",
        "            info = None\n",
        "        return Dataset(\n",
        "            load('X_num') if dir_.joinpath('X_num_train.npy').exists() else None,\n",
        "            load('X_cat') if dir_.joinpath('X_cat_train.npy').exists() else None,\n",
        "            load('y'),\n",
        "            {},\n",
        "            TaskType(info['task_type']),\n",
        "            info.get('n_classes'),\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def is_binclass(self) -> bool:\n",
        "        return self.task_type == TaskType.BINCLASS\n",
        "\n",
        "    @property\n",
        "    def is_multiclass(self) -> bool:\n",
        "        return self.task_type == TaskType.MULTICLASS\n",
        "\n",
        "    @property\n",
        "    def is_regression(self) -> bool:\n",
        "        return self.task_type == TaskType.REGRESSION\n",
        "\n",
        "    @property\n",
        "    def n_num_features(self) -> int:\n",
        "        return 0 if self.X_num is None else self.X_num['train'].shape[1]\n",
        "\n",
        "    @property\n",
        "    def n_cat_features(self) -> int:\n",
        "        return 0 if self.X_cat is None else self.X_cat['train'].shape[1]\n",
        "\n",
        "    @property\n",
        "    def n_features(self) -> int:\n",
        "        return self.n_num_features + self.n_cat_features\n",
        "\n",
        "    def size(self, part: Optional[str]) -> int:\n",
        "        return sum(map(len, self.y.values())) if part is None else len(self.y[part])\n",
        "\n",
        "    @property\n",
        "    def nn_output_dim(self) -> int:\n",
        "        if self.is_multiclass:\n",
        "            assert self.n_classes is not None\n",
        "            return self.n_classes\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "    def get_category_sizes(self, part: str) -> List[int]:\n",
        "        return [] if self.X_cat is None else get_category_sizes(self.X_cat[part])\n",
        "\n",
        "    def calculate_metrics(\n",
        "        self,\n",
        "        predictions: Dict[str, np.ndarray],\n",
        "        prediction_type: Optional[str],\n",
        "    ) -> Dict[str, Any]:\n",
        "        metrics = {\n",
        "            x: calculate_metrics(\n",
        "                self.y[x], predictions[x], self.task_type, prediction_type, self.y_info\n",
        "            )\n",
        "            for x in predictions\n",
        "        }\n",
        "        if self.task_type == TaskType.REGRESSION:\n",
        "            score_key = 'rmse'\n",
        "            score_sign = -1\n",
        "        else:\n",
        "            score_key = 'accuracy'\n",
        "            score_sign = 1\n",
        "        for part_metrics in metrics.values():\n",
        "            part_metrics['score'] = score_sign * part_metrics[score_key]\n",
        "        return metrics"
      ],
      "metadata": {
        "id": "c1lxwKdTf0cZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### def num_process_nans"
      ],
      "metadata": {
        "id": "B5gWrN380n42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def raise_unknown(unknown_what: str, unknown_value: Any):\n",
        "    raise ValueError(f'Unknown {unknown_what}: {unknown_value}')"
      ],
      "metadata": {
        "id": "HH3EZOPE0195"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_process_nans(dataset: Dataset, policy: Optional[NumNanPolicy]) -> Dataset:\n",
        "\n",
        "    assert dataset.X_num is not None\n",
        "    nan_masks = {k: np.isnan(v) for k, v in dataset.X_num.items()}\n",
        "    if not any(x.any() for x in nan_masks.values()):  # type: ignore[code]\n",
        "        # assert policy is None\n",
        "        print('No NaNs in numerical features, skipping')\n",
        "        return dataset\n",
        "\n",
        "    assert policy is not None\n",
        "    if policy == 'drop-rows':\n",
        "        valid_masks = {k: ~v.any(1) for k, v in nan_masks.items()}\n",
        "        assert valid_masks[\n",
        "            'test'\n",
        "        ].all(), 'Cannot drop test rows, since this will affect the final metrics.'\n",
        "        new_data = {}\n",
        "        for data_name in ['X_num', 'X_cat', 'y']:\n",
        "            data_dict = getattr(dataset, data_name)\n",
        "            if data_dict is not None:\n",
        "                new_data[data_name] = {\n",
        "                    k: v[valid_masks[k]] for k, v in data_dict.items()\n",
        "                }\n",
        "        dataset = replace(dataset, **new_data)\n",
        "    elif policy == 'mean':\n",
        "        new_values = np.nanmean(dataset.X_num['train'], axis=0)\n",
        "        X_num = deepcopy(dataset.X_num)\n",
        "        for k, v in X_num.items():\n",
        "            num_nan_indices = np.where(nan_masks[k])\n",
        "            v[num_nan_indices] = np.take(new_values, num_nan_indices[1])\n",
        "        dataset = replace(dataset, X_num=X_num)\n",
        "    else:\n",
        "        assert raise_unknown('policy', policy)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "fwW8_WCX0pwp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### def transform_dataset"
      ],
      "metadata": {
        "id": "eDhE_VKMyNv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### def normalize"
      ],
      "metadata": {
        "id": "giANVu3O1MPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(\n",
        "    X: ArrayDict, normalization: Normalization, seed: Optional[int], return_normalizer : bool = False\n",
        ") -> ArrayDict:\n",
        "    X_train = X['train']\n",
        "    if normalization == 'standard':\n",
        "        normalizer = sklearn.preprocessing.StandardScaler()\n",
        "    elif normalization == 'minmax':\n",
        "        normalizer = sklearn.preprocessing.MinMaxScaler()\n",
        "    elif normalization == 'quantile':\n",
        "        normalizer = sklearn.preprocessing.QuantileTransformer(\n",
        "            output_distribution='normal',\n",
        "            n_quantiles=max(min(X['train'].shape[0] // 30, 1000), 10),\n",
        "            subsample=int(1e9),\n",
        "            random_state=seed,\n",
        "        )\n",
        "        # noise = 1e-3\n",
        "        # if noise > 0:\n",
        "        #     assert seed is not None\n",
        "        #     stds = np.std(X_train, axis=0, keepdims=True)\n",
        "        #     noise_std = noise / np.maximum(stds, noise)  # type: ignore[code]\n",
        "        #     X_train = X_train + noise_std * np.random.default_rng(seed).standard_normal(\n",
        "        #         X_train.shape\n",
        "        #     )\n",
        "    else:\n",
        "        raise_unknown('normalization', normalization)\n",
        "\n",
        "    normalizer.fit(X_train)\n",
        "    if return_normalizer:\n",
        "        return {k: normalizer.transform(v) for k, v in X.items()}, normalizer\n",
        "    return {k: normalizer.transform(v) for k, v in X.items()}"
      ],
      "metadata": {
        "id": "uERuca7J1PLb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### def cat_process_nans"
      ],
      "metadata": {
        "id": "bp-o8Wbx1bua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cat_process_nans(X: ArrayDict, policy: Optional[CatNanPolicy]) -> ArrayDict:\n",
        "    assert X is not None\n",
        "    nan_masks = {k: v == CAT_MISSING_VALUE for k, v in X.items()}\n",
        "    if any(x.any() for x in nan_masks.values()):  # type: ignore[code]\n",
        "        if policy is None:\n",
        "            X_new = X\n",
        "        elif policy == 'most_frequent':\n",
        "            imputer = SimpleImputer(missing_values=CAT_MISSING_VALUE, strategy=policy)  # type: ignore[code]\n",
        "            imputer.fit(X['train'])\n",
        "            X_new = {k: cast(np.ndarray, imputer.transform(v)) for k, v in X.items()}\n",
        "        else:\n",
        "            util.raise_unknown('categorical NaN policy', policy)\n",
        "    else:\n",
        "        assert policy is None\n",
        "        X_new = X\n",
        "    return X_new"
      ],
      "metadata": {
        "id": "77kRuTY81fMX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### def cat_encode"
      ],
      "metadata": {
        "id": "yW0_eMDj1x23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cat_encode(\n",
        "    X: ArrayDict,\n",
        "    encoding: Optional[CatEncoding],\n",
        "    y_train: Optional[np.ndarray],\n",
        "    seed: Optional[int],\n",
        "    return_encoder : bool = False\n",
        ") -> Tuple[ArrayDict, bool, Optional[Any]]:  # (X, is_converted_to_numerical)\n",
        "    if encoding != 'counter':\n",
        "        y_train = None\n",
        "\n",
        "    # Step 1. Map strings to 0-based ranges\n",
        "\n",
        "    if encoding is None:\n",
        "        unknown_value = np.iinfo('int64').max - 3\n",
        "        oe = sklearn.preprocessing.OrdinalEncoder(\n",
        "            handle_unknown='use_encoded_value',  # type: ignore[code]\n",
        "            unknown_value=unknown_value,  # type: ignore[code]\n",
        "            dtype='int64',  # type: ignore[code]\n",
        "        ).fit(X['train'])\n",
        "        encoder = make_pipeline(oe)\n",
        "        encoder.fit(X['train'])\n",
        "        X = {k: encoder.transform(v) for k, v in X.items()}\n",
        "        max_values = X['train'].max(axis=0)\n",
        "        for part in X.keys():\n",
        "            if part == 'train': continue\n",
        "            for column_idx in range(X[part].shape[1]):\n",
        "                X[part][X[part][:, column_idx] == unknown_value, column_idx] = (\n",
        "                    max_values[column_idx] + 1\n",
        "                )\n",
        "        if return_encoder:\n",
        "            return (X, False, encoder)\n",
        "        return (X, False)"
      ],
      "metadata": {
        "id": "ujXHqd9V1zcT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### def build_target"
      ],
      "metadata": {
        "id": "ONhigc5T18I3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_target(\n",
        "    y: ArrayDict, policy: Optional[YPolicy], task_type: TaskType\n",
        ") -> Tuple[ArrayDict, Dict[str, Any]]:\n",
        "    info: Dict[str, Any] = {'policy': policy}\n",
        "    if policy is None:\n",
        "        pass\n",
        "    elif policy == 'default':\n",
        "        if task_type == TaskType.REGRESSION:\n",
        "            mean, std = float(y['train'].mean()), float(y['train'].std())\n",
        "            y = {k: (v - mean) / std for k, v in y.items()}\n",
        "            info['mean'] = mean\n",
        "            info['std'] = std\n",
        "    else:\n",
        "        raise_unknown('policy', policy)\n",
        "    return y, info"
      ],
      "metadata": {
        "id": "drYmXUnE17i2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### -"
      ],
      "metadata": {
        "id": "NOCRW-Dw1kmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_dataset(\n",
        "    dataset: Dataset,\n",
        "    transformations: Transformations,\n",
        "    cache_dir: Optional[Path],\n",
        "    return_transforms: bool = False\n",
        ") -> Dataset:\n",
        "    # WARNING: the order of transformations matters. Moreover, the current\n",
        "    # implementation is not ideal in that sense.\n",
        "    if cache_dir is not None:\n",
        "        transformations_md5 = hashlib.md5(\n",
        "            str(transformations).encode('utf-8')\n",
        "        ).hexdigest()\n",
        "        transformations_str = '__'.join(map(str, astuple(transformations)))\n",
        "        cache_path = (\n",
        "            cache_dir / f'cache__{transformations_str}__{transformations_md5}.pickle'\n",
        "        )\n",
        "        if cache_path.exists():\n",
        "            cache_transformations, value = util.load_pickle(cache_path)\n",
        "            if transformations == cache_transformations:\n",
        "                print(\n",
        "                    f\"Using cached features: {cache_dir.name + '/' + cache_path.name}\"\n",
        "                )\n",
        "                return value\n",
        "            else:\n",
        "                raise RuntimeError(f'Hash collision for {cache_path}')\n",
        "    else:\n",
        "        cache_path = None\n",
        "\n",
        "    if dataset.X_num is not None:\n",
        "        dataset = num_process_nans(dataset, transformations.num_nan_policy)\n",
        "\n",
        "    num_transform = None\n",
        "    cat_transform = None\n",
        "    X_num = dataset.X_num\n",
        "\n",
        "    if X_num is not None and transformations.normalization is not None:\n",
        "        X_num, num_transform = normalize(\n",
        "            X_num,\n",
        "            transformations.normalization,\n",
        "            transformations.seed,\n",
        "            return_normalizer=True\n",
        "        )\n",
        "        num_transform = num_transform\n",
        "\n",
        "    if dataset.X_cat is None:\n",
        "        assert transformations.cat_nan_policy is None\n",
        "        assert transformations.cat_min_frequency is None\n",
        "        # assert transformations.cat_encoding is None\n",
        "        X_cat = None\n",
        "    else:\n",
        "        X_cat = cat_process_nans(dataset.X_cat, transformations.cat_nan_policy)\n",
        "\n",
        "        if transformations.cat_min_frequency is not None:\n",
        "            X_cat = cat_drop_rare(X_cat, transformations.cat_min_frequency)\n",
        "        X_cat, is_num, cat_transform = cat_encode(\n",
        "            X_cat,\n",
        "            transformations.cat_encoding,\n",
        "            dataset.y['train'],\n",
        "            transformations.seed,\n",
        "            return_encoder=True\n",
        "        )\n",
        "\n",
        "        if is_num:\n",
        "            X_num = (\n",
        "                X_cat\n",
        "                if X_num is None\n",
        "                else {x: np.hstack([X_num[x], X_cat[x]]) for x in X_num}\n",
        "            )\n",
        "            X_cat = None\n",
        "\n",
        "\n",
        "    y, y_info = build_target(dataset.y, transformations.y_policy, dataset.task_type)\n",
        "\n",
        "    dataset = replace(dataset, X_num=X_num, X_cat=X_cat, y=y, y_info=y_info)\n",
        "    dataset.num_transform = num_transform\n",
        "    dataset.cat_transform = cat_transform\n",
        "\n",
        "    if cache_path is not None:\n",
        "        util.dump_pickle((transformations, dataset), cache_path)\n",
        "    # if return_transforms:\n",
        "        # return dataset, num_transform, cat_transform\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "uujK5gMchpdL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### def preprocess"
      ],
      "metadata": {
        "id": "q22UaHEmyUyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# import src\n",
        "# from torch.utils.data import Dataset\n",
        "\n",
        "class TaskType(enum.Enum):\n",
        "    BINCLASS = 'binclass'\n",
        "    MULTICLASS = 'multiclass'\n",
        "    REGRESSION = 'regression'\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return self.value\n",
        "\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, X_num, X_cat):\n",
        "        self.X_num = X_num\n",
        "        self.X_cat = X_cat\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        this_num = self.X_num[index]\n",
        "        this_cat = self.X_cat[index]\n",
        "\n",
        "        sample = (this_num, this_cat)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X_num.shape[0]\n",
        "\n",
        "def preprocess(dataset_path, task_type = 'binclass', inverse = False, cat_encoding = None, concat = True):\n",
        "\n",
        "    T_dict = {}\n",
        "\n",
        "    T_dict['normalization'] = \"quantile\"\n",
        "    T_dict['num_nan_policy'] = 'mean'\n",
        "    T_dict['cat_nan_policy'] =  None\n",
        "    T_dict['cat_min_frequency'] = None\n",
        "    T_dict['cat_encoding'] = cat_encoding\n",
        "    T_dict['y_policy'] = \"default\"\n",
        "\n",
        "    T = Transformations(**T_dict)\n",
        "\n",
        "    dataset = make_dataset(\n",
        "        data_path = dataset_path,\n",
        "        T = T,\n",
        "        task_type = task_type,\n",
        "        change_val = False,\n",
        "        concat = concat\n",
        "    )\n",
        "\n",
        "    if cat_encoding is None:\n",
        "        X_num = dataset.X_num\n",
        "        X_cat = dataset.X_cat\n",
        "\n",
        "        X_train_num, X_test_num = X_num['train'], X_num['test']\n",
        "        X_train_cat, X_test_cat = X_cat['train'], X_cat['test']\n",
        "\n",
        "        categories = get_categories(X_train_cat)\n",
        "        d_numerical = X_train_num.shape[1]\n",
        "\n",
        "        X_num = (X_train_num, X_test_num)\n",
        "        X_cat = (X_train_cat, X_test_cat)\n",
        "\n",
        "\n",
        "        if inverse:\n",
        "            num_inverse = dataset.num_transform.inverse_transform\n",
        "            cat_inverse = dataset.cat_transform.inverse_transform\n",
        "\n",
        "            return X_num, X_cat, categories, d_numerical, num_inverse, cat_inverse\n",
        "        else:\n",
        "            return X_num, X_cat, categories, d_numerical\n",
        "    else:\n",
        "        return dataset\n",
        "\n",
        "\n",
        "def update_ema(target_params, source_params, rate=0.999):\n",
        "    \"\"\"\n",
        "    Update target parameters to be closer to those of source parameters using\n",
        "    an exponential moving average.\n",
        "    :param target_params: the target parameter sequence.\n",
        "    :param source_params: the source parameter sequence.\n",
        "    :param rate: the EMA rate (closer to 1 means slower).\n",
        "    \"\"\"\n",
        "    for target, source in zip(target_params, source_params):\n",
        "        target.detach().mul_(rate).add_(source.detach(), alpha=1 - rate)\n",
        "\n",
        "\n",
        "\n",
        "def concat_y_to_X(X, y):\n",
        "    if X is None:\n",
        "        return y.reshape(-1, 1)\n",
        "    return np.concatenate([y.reshape(-1, 1), X], axis=1)\n",
        "\n",
        "\n",
        "def make_dataset(\n",
        "    data_path: str,\n",
        "    T: Transformations,\n",
        "    task_type,\n",
        "    change_val: bool,\n",
        "    concat = True,\n",
        "):\n",
        "    # classification\n",
        "    if task_type == 'binclass' or task_type == 'multiclass':\n",
        "        X_cat = {} if os.path.exists(os.path.join(data_path, 'X_cat_train.npy'))  else None\n",
        "        X_num = {} if os.path.exists(os.path.join(data_path, 'X_num_train.npy')) else None\n",
        "        y = {} if os.path.exists(os.path.join(data_path, 'y_train.npy')) else None\n",
        "\n",
        "        for split in ['train', 'test']:\n",
        "            X_num_t, X_cat_t, y_t = read_pure_data(data_path, split)\n",
        "            if X_num is not None:\n",
        "                X_num[split] = X_num_t\n",
        "            if X_cat is not None:\n",
        "                if concat:\n",
        "                    X_cat_t = concat_y_to_X(X_cat_t, y_t)\n",
        "                X_cat[split] = X_cat_t\n",
        "            if y is not None:\n",
        "                y[split] = y_t\n",
        "    else:\n",
        "        # regression\n",
        "        X_cat = {} if os.path.exists(os.path.join(data_path, 'X_cat_train.npy')) else None\n",
        "        X_num = {} if os.path.exists(os.path.join(data_path, 'X_num_train.npy')) else None\n",
        "        y = {} if os.path.exists(os.path.join(data_path, 'y_train.npy')) else None\n",
        "\n",
        "        for split in ['train', 'test']:\n",
        "            X_num_t, X_cat_t, y_t = read_pure_data(data_path, split)\n",
        "\n",
        "            if X_num is not None:\n",
        "                if concat:\n",
        "                    X_num_t = concat_y_to_X(X_num_t, y_t)\n",
        "                X_num[split] = X_num_t\n",
        "            if X_cat is not None:\n",
        "                X_cat[split] = X_cat_t\n",
        "            if y is not None:\n",
        "                y[split] = y_t\n",
        "\n",
        "    info = load_json(os.path.join(data_path, 'info.json'))\n",
        "\n",
        "    D = Dataset(\n",
        "        X_num,\n",
        "        X_cat,\n",
        "        y,\n",
        "        y_info={},\n",
        "        task_type=TaskType(info['task_type']),\n",
        "        n_classes=info.get('n_classes')\n",
        "    )\n",
        "\n",
        "    if change_val:\n",
        "        D = src.change_val(D)\n",
        "\n",
        "    # def categorical_to_idx(feature):\n",
        "    #     unique_categories = np.unique(feature)\n",
        "    #     idx_mapping = {category: index for index, category in enumerate(unique_categories)}\n",
        "    #     idx_feature = np.array([idx_mapping[category] for category in feature])\n",
        "    #     return idx_feature\n",
        "\n",
        "    # for split in ['train', 'val', 'test']:\n",
        "    # D.y[split] = categorical_to_idx(D.y[split].squeeze(1))\n",
        "\n",
        "    return transform_dataset(D, T, None)"
      ],
      "metadata": {
        "id": "rO5AqW6ic75H"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### params"
      ],
      "metadata": {
        "id": "YktzfMSmyY4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 1e-3\n",
        "WD = 0\n",
        "D_TOKEN = 4\n",
        "TOKEN_BIAS = True"
      ],
      "metadata": {
        "id": "zu69nRhscHHi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_HEAD = 1\n",
        "FACTOR = 32\n",
        "NUM_LAYERS = 2"
      ],
      "metadata": {
        "id": "W4yy3q-6hzXE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### def compute_loss"
      ],
      "metadata": {
        "id": "wr-tolQNydkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(X_num, X_cat, Recon_X_num, Recon_X_cat, mu_z, logvar_z):\n",
        "    ce_loss_fn = nn.CrossEntropyLoss()\n",
        "    mse_loss = (X_num - Recon_X_num).pow(2).mean()\n",
        "    ce_loss = 0\n",
        "    acc = 0\n",
        "    total_num = 0\n",
        "\n",
        "    for idx, x_cat in enumerate(Recon_X_cat):\n",
        "        if x_cat is not None:\n",
        "            ce_loss += ce_loss_fn(x_cat, X_cat[:, idx])\n",
        "            x_hat = x_cat.argmax(dim = -1)\n",
        "        acc += (x_hat == X_cat[:,idx]).float().sum()\n",
        "        total_num += x_hat.shape[0]\n",
        "\n",
        "    ce_loss /= (idx + 1)\n",
        "    acc /= total_num\n",
        "    # loss = mse_loss + ce_loss\n",
        "\n",
        "    temp = 1 + logvar_z - mu_z.pow(2) - logvar_z.exp()\n",
        "\n",
        "    loss_kld = -0.5 * torch.mean(temp.mean(-1).mean())\n",
        "    return mse_loss, ce_loss, loss_kld, acc"
      ],
      "metadata": {
        "id": "l17KU2H9h1T1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### def tabsyn_vae_main"
      ],
      "metadata": {
        "id": "KNAbRD9Yygu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tabsyn_vae_main(dataname = 'adult', max_beta=1e-2, min_beta=1e-5, lambd=0.7, device = 'cpu', num_epochs = 10):\n",
        "\n",
        "    # dataname = args.dataname\n",
        "    data_dir = f'data/{dataname}'\n",
        "\n",
        "    # max_beta = args.max_beta\n",
        "    # min_beta = args.min_beta\n",
        "    # lambd = args.lambd\n",
        "\n",
        "    # device =  args.device\n",
        "\n",
        "\n",
        "    # info_path = f'data/{dataname}/info.json'\n",
        "\n",
        "    # with open(info_path, 'r') as f:\n",
        "    #     info = json.load(f)\n",
        "\n",
        "    # curr_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "    ckpt_dir = f'model'\n",
        "    # if not os.path.exists(ckpt_dir):\n",
        "    #     os.makedirs(ckpt_dir)\n",
        "\n",
        "    model_save_path = f'{ckpt_dir}/model.pt'\n",
        "    encoder_save_path = f'{ckpt_dir}/encoder.pt'\n",
        "    decoder_save_path = f'{ckpt_dir}/decoder.pt'\n",
        "\n",
        "    X_num, X_cat, categories, d_numerical = preprocess(data_dir)\n",
        "\n",
        "    X_train_num, _ = X_num\n",
        "    X_train_cat, _ = X_cat\n",
        "\n",
        "    X_train_num, X_test_num = X_num\n",
        "    X_train_cat, X_test_cat = X_cat\n",
        "\n",
        "    X_train_num, X_test_num = torch.tensor(X_train_num).float(), torch.tensor(X_test_num).float()\n",
        "    X_train_cat, X_test_cat =  torch.tensor(X_train_cat), torch.tensor(X_test_cat)\n",
        "\n",
        "\n",
        "    train_data = TabularDataset(X_train_num.float(), X_train_cat)\n",
        "\n",
        "    X_test_num = X_test_num.float().to(device)\n",
        "    X_test_cat = X_test_cat.to(device)\n",
        "\n",
        "    batch_size = 4096\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = True,\n",
        "        num_workers = 4,\n",
        "    )\n",
        "\n",
        "    model = Model_VAE(NUM_LAYERS, d_numerical, categories, D_TOKEN, n_head = N_HEAD, factor = FACTOR, bias = True)\n",
        "    model = model.to(device)\n",
        "\n",
        "    pre_encoder = Encoder_model(NUM_LAYERS, d_numerical, categories, D_TOKEN, n_head = N_HEAD, factor = FACTOR).to(device)\n",
        "    pre_decoder = Decoder_model(NUM_LAYERS, d_numerical, categories, D_TOKEN, n_head = N_HEAD, factor = FACTOR).to(device)\n",
        "\n",
        "    pre_encoder.eval()\n",
        "    pre_decoder.eval()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=10, verbose=True)\n",
        "\n",
        "    # num_epochs = 10\n",
        "    best_train_loss = float('inf')\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    patience = 0\n",
        "\n",
        "    beta = max_beta\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        # pbar = tqdm(train_loader, total=len(train_loader))\n",
        "        # pbar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        curr_loss_multi = 0.0\n",
        "        curr_loss_gauss = 0.0\n",
        "        curr_loss_kl = 0.0\n",
        "\n",
        "        curr_count = 0\n",
        "\n",
        "        for batch_num, batch_cat in train_loader:#pbar\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            batch_num = batch_num.to(device)\n",
        "            batch_cat = batch_cat.to(device)\n",
        "\n",
        "            Recon_X_num, Recon_X_cat, mu_z, std_z = model(batch_num, batch_cat)\n",
        "\n",
        "            loss_mse, loss_ce, loss_kld, train_acc = compute_loss(batch_num, batch_cat, Recon_X_num, Recon_X_cat, mu_z, std_z)\n",
        "\n",
        "            loss = loss_mse + loss_ce + beta * loss_kld\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_length = batch_num.shape[0]\n",
        "            curr_count += batch_length\n",
        "            curr_loss_multi += loss_ce.item() * batch_length\n",
        "            curr_loss_gauss += loss_mse.item() * batch_length\n",
        "            curr_loss_kl    += loss_kld.item() * batch_length\n",
        "\n",
        "        num_loss = curr_loss_gauss / curr_count\n",
        "        cat_loss = curr_loss_multi / curr_count\n",
        "        kl_loss = curr_loss_kl / curr_count\n",
        "\n",
        "\n",
        "        '''\n",
        "            Evaluation\n",
        "        '''\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            Recon_X_num, Recon_X_cat, mu_z, std_z = model(X_test_num, X_test_cat)\n",
        "\n",
        "            val_mse_loss, val_ce_loss, val_kl_loss, val_acc = compute_loss(X_test_num, X_test_cat, Recon_X_num, Recon_X_cat, mu_z, std_z)\n",
        "            val_loss = val_mse_loss.item() * 0 + val_ce_loss.item()\n",
        "\n",
        "            scheduler.step(val_loss)\n",
        "            new_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "            if new_lr != current_lr:\n",
        "                current_lr = new_lr\n",
        "                print(f\"Learning rate updated: {current_lr}\")\n",
        "\n",
        "            train_loss = val_loss\n",
        "            if train_loss < best_train_loss:\n",
        "                best_train_loss = train_loss\n",
        "                patience = 0\n",
        "                torch.save(model.state_dict(), model_save_path)\n",
        "            else:\n",
        "                patience += 1\n",
        "                if patience == 10:\n",
        "                    if beta > min_beta:\n",
        "                        beta = beta * lambd\n",
        "        if epoch%10 ==0:\n",
        "            # print('epoch: {}, beta = {:.6f}, Train MSE: {:.6f}, Train CE:{:.6f}, Train KL:{:.6f}, Train ACC:{:6f}'.format(epoch, beta, num_loss, cat_loss, kl_loss, train_acc.item()))\n",
        "            print('epoch: {}, beta = {:.6f}, Train MSE: {:.6f}, Train CE:{:.6f}, Train KL:{:.6f}, Val MSE:{:.6f}, Val CE:{:.6f}, Train ACC:{:6f}, Val ACC:{:6f}'.format(epoch, beta, num_loss, cat_loss, kl_loss, val_mse_loss.item(), val_ce_loss.item(), train_acc.item(), val_acc.item() ))\n",
        "\n",
        "    end_time = time.time()\n",
        "    print('Training time: {:.4f} mins'.format((end_time - start_time)/60))\n",
        "\n",
        "    # Saving latent embeddings\n",
        "    with torch.no_grad():\n",
        "        pre_encoder.load_weights(model)\n",
        "        pre_decoder.load_weights(model)\n",
        "\n",
        "        torch.save(pre_encoder.state_dict(), encoder_save_path)\n",
        "        torch.save(pre_decoder.state_dict(), decoder_save_path)\n",
        "\n",
        "        X_train_num = X_train_num.to(device)\n",
        "        X_train_cat = X_train_cat.to(device)\n",
        "\n",
        "        print('Successfully load and save the model!')\n",
        "\n",
        "        train_z = pre_encoder(X_train_num, X_train_cat).detach().cpu().numpy()\n",
        "\n",
        "        np.save(f'{ckpt_dir}/train_z.npy', train_z)\n",
        "\n",
        "        print('Successfully save pretrained embeddings in disk!')\n"
      ],
      "metadata": {
        "id": "eA86hMDaiFz-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### model\n"
      ],
      "metadata": {
        "id": "YITuKoEqjt9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_numerical, categories, d_token, bias):\n",
        "        super().__init__()\n",
        "        if categories is None:\n",
        "            d_bias = d_numerical\n",
        "            self.category_offsets = None\n",
        "            self.category_embeddings = None\n",
        "        else:\n",
        "            d_bias = d_numerical + len(categories)\n",
        "            category_offsets = torch.tensor([0] + categories[:-1]).cumsum(0)\n",
        "            self.register_buffer('category_offsets', category_offsets)\n",
        "            self.category_embeddings = nn.Embedding(sum(categories), d_token)\n",
        "            nn_init.kaiming_uniform_(self.category_embeddings.weight, a=math.sqrt(5))\n",
        "            print(f'{self.category_embeddings.weight.shape=}')\n",
        "\n",
        "        # take [CLS] token into account\n",
        "        self.weight = nn.Parameter(Tensor(d_numerical + 1, d_token))\n",
        "        self.bias = nn.Parameter(Tensor(d_bias, d_token)) if bias else None\n",
        "        # The initialization is inspired by nn.Linear\n",
        "        nn_init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            nn_init.kaiming_uniform_(self.bias, a=math.sqrt(5))\n",
        "\n",
        "    @property\n",
        "    def n_tokens(self):\n",
        "        return len(self.weight) + (\n",
        "            0 if self.category_offsets is None else len(self.category_offsets)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_num, x_cat):\n",
        "        x_some = x_num if x_cat is None else x_cat\n",
        "        assert x_some is not None\n",
        "        x_num = torch.cat(\n",
        "            [torch.ones(len(x_some), 1, device=x_some.device)]  # [CLS]\n",
        "            + ([] if x_num is None else [x_num]),\n",
        "            dim=1,\n",
        "        )\n",
        "\n",
        "        x = self.weight[None] * x_num[:, :, None]\n",
        "\n",
        "        if x_cat is not None:\n",
        "            x = torch.cat(\n",
        "                [x, self.category_embeddings(x_cat + self.category_offsets[None])],\n",
        "                dim=1,\n",
        "            )\n",
        "        if self.bias is not None:\n",
        "            bias = torch.cat(\n",
        "                [\n",
        "                    torch.zeros(1, self.bias.shape[1], device=x.device),\n",
        "                    self.bias,\n",
        "                ]\n",
        "            )\n",
        "            x = x + bias[None]\n",
        "\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.5):\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self, d, n_heads, dropout, initialization = 'kaiming'):\n",
        "\n",
        "        if n_heads > 1:\n",
        "            assert d % n_heads == 0\n",
        "        assert initialization in ['xavier', 'kaiming']\n",
        "\n",
        "        super().__init__()\n",
        "        self.W_q = nn.Linear(d, d)\n",
        "        self.W_k = nn.Linear(d, d)\n",
        "        self.W_v = nn.Linear(d, d)\n",
        "        self.W_out = nn.Linear(d, d) if n_heads > 1 else None\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout = nn.Dropout(dropout) if dropout else None\n",
        "\n",
        "        for m in [self.W_q, self.W_k, self.W_v]:\n",
        "            if initialization == 'xavier' and (n_heads > 1 or m is not self.W_v):\n",
        "                # gain is needed since W_qkv is represented with 3 separate layers\n",
        "                nn_init.xavier_uniform_(m.weight, gain=1 / math.sqrt(2))\n",
        "            nn_init.zeros_(m.bias)\n",
        "        if self.W_out is not None:\n",
        "            nn_init.zeros_(self.W_out.bias)\n",
        "\n",
        "    def _reshape(self, x):\n",
        "        batch_size, n_tokens, d = x.shape\n",
        "        d_head = d // self.n_heads\n",
        "        return (\n",
        "            x.reshape(batch_size, n_tokens, self.n_heads, d_head)\n",
        "            .transpose(1, 2)\n",
        "            .reshape(batch_size * self.n_heads, n_tokens, d_head)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_q, x_kv, key_compression = None, value_compression = None):\n",
        "\n",
        "        q, k, v = self.W_q(x_q), self.W_k(x_kv), self.W_v(x_kv)\n",
        "        for tensor in [q, k, v]:\n",
        "            assert tensor.shape[-1] % self.n_heads == 0\n",
        "        if key_compression is not None:\n",
        "            assert value_compression is not None\n",
        "            k = key_compression(k.transpose(1, 2)).transpose(1, 2)\n",
        "            v = value_compression(v.transpose(1, 2)).transpose(1, 2)\n",
        "        else:\n",
        "            assert value_compression is None\n",
        "\n",
        "        batch_size = len(q)\n",
        "        d_head_key = k.shape[-1] // self.n_heads\n",
        "        d_head_value = v.shape[-1] // self.n_heads\n",
        "        n_q_tokens = q.shape[1]\n",
        "\n",
        "        q = self._reshape(q)\n",
        "        k = self._reshape(k)\n",
        "\n",
        "        a = q @ k.transpose(1, 2)\n",
        "        b = math.sqrt(d_head_key)\n",
        "        attention = F.softmax(a/b , dim=-1)\n",
        "\n",
        "\n",
        "        if self.dropout is not None:\n",
        "            attention = self.dropout(attention)\n",
        "        x = attention @ self._reshape(v)\n",
        "        x = (\n",
        "            x.reshape(batch_size, self.n_heads, n_q_tokens, d_head_value)\n",
        "            .transpose(1, 2)\n",
        "            .reshape(batch_size, n_q_tokens, self.n_heads * d_head_value)\n",
        "        )\n",
        "        if self.W_out is not None:\n",
        "            x = self.W_out(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_layers: int,\n",
        "        d_token: int,\n",
        "        n_heads: int,\n",
        "        d_out: int,\n",
        "        d_ffn_factor: int,\n",
        "        attention_dropout = 0.0,\n",
        "        ffn_dropout = 0.0,\n",
        "        residual_dropout = 0.0,\n",
        "        activation = 'relu',\n",
        "        prenormalization = True,\n",
        "        initialization = 'kaiming',\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        def make_normalization():\n",
        "            return nn.LayerNorm(d_token)\n",
        "\n",
        "        d_hidden = int(d_token * d_ffn_factor)\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for layer_idx in range(n_layers):\n",
        "            layer = nn.ModuleDict(\n",
        "                {\n",
        "                    'attention': MultiheadAttention(\n",
        "                        d_token, n_heads, attention_dropout, initialization\n",
        "                    ),\n",
        "                    'linear0': nn.Linear(\n",
        "                        d_token, d_hidden\n",
        "                    ),\n",
        "                    'linear1': nn.Linear(d_hidden, d_token),\n",
        "                    'norm1': make_normalization(),\n",
        "                }\n",
        "            )\n",
        "            if not prenormalization or layer_idx:\n",
        "                layer['norm0'] = make_normalization()\n",
        "\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "        self.last_activation = nn.ReLU()\n",
        "        # self.activation = lib.get_activation_fn(activation)\n",
        "        # self.last_activation = lib.get_nonglu_activation_fn(activation)\n",
        "        self.prenormalization = prenormalization\n",
        "        self.last_normalization = make_normalization() if prenormalization else None\n",
        "        self.ffn_dropout = ffn_dropout\n",
        "        self.residual_dropout = residual_dropout\n",
        "        self.head = nn.Linear(d_token, d_out)\n",
        "\n",
        "\n",
        "    def _start_residual(self, x, layer, norm_idx):\n",
        "        x_residual = x\n",
        "        if self.prenormalization:\n",
        "            norm_key = f'norm{norm_idx}'\n",
        "            if norm_key in layer:\n",
        "                x_residual = layer[norm_key](x_residual)\n",
        "        return x_residual\n",
        "\n",
        "    def _end_residual(self, x, x_residual, layer, norm_idx):\n",
        "        if self.residual_dropout:\n",
        "            x_residual = F.dropout(x_residual, self.residual_dropout, self.training)\n",
        "        x = x + x_residual\n",
        "        if not self.prenormalization:\n",
        "            x = layer[f'norm{norm_idx}'](x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer_idx, layer in enumerate(self.layers):\n",
        "            is_last_layer = layer_idx + 1 == len(self.layers)\n",
        "\n",
        "            x_residual = self._start_residual(x, layer, 0)\n",
        "            x_residual = layer['attention'](\n",
        "                # for the last attention, it is enough to process only [CLS]\n",
        "                x_residual,\n",
        "                x_residual,\n",
        "            )\n",
        "\n",
        "            x = self._end_residual(x, x_residual, layer, 0)\n",
        "\n",
        "            x_residual = self._start_residual(x, layer, 1)\n",
        "            x_residual = layer['linear0'](x_residual)\n",
        "            x_residual = self.activation(x_residual)\n",
        "            if self.ffn_dropout:\n",
        "                x_residual = F.dropout(x_residual, self.ffn_dropout, self.training)\n",
        "            x_residual = layer['linear1'](x_residual)\n",
        "            x = self._end_residual(x, x_residual, layer, 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AE(nn.Module):\n",
        "    def __init__(self, hid_dim, n_head):\n",
        "        super(AE, self).__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_head = n_head\n",
        "\n",
        "\n",
        "        self.encoder = MultiheadAttention(hid_dim, n_head)\n",
        "        self.decoder = MultiheadAttention(hid_dim, n_head)\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        return self.encoder(x, x).detach()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        z = self.encoder(x, x)\n",
        "        h = self.decoder(z, z)\n",
        "\n",
        "        return h\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, d_numerical, categories, num_layers, hid_dim, n_head = 1, factor = 4, bias = True):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.d_numerical = d_numerical\n",
        "        self.categories = categories\n",
        "        self.hid_dim = hid_dim\n",
        "        d_token = hid_dim\n",
        "        self.n_head = n_head\n",
        "\n",
        "        self.Tokenizer = Tokenizer(d_numerical, categories, d_token, bias = bias)\n",
        "\n",
        "        self.encoder_mu = Transformer(num_layers, hid_dim, n_head, hid_dim, factor)\n",
        "        self.encoder_logvar = Transformer(num_layers, hid_dim, n_head, hid_dim, factor)\n",
        "\n",
        "        self.decoder = Transformer(num_layers, hid_dim, n_head, hid_dim, factor)\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        return self.encoder_mu(x, x).detach()\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x_num, x_cat):\n",
        "        x = self.Tokenizer(x_num, x_cat)\n",
        "\n",
        "        mu_z = self.encoder_mu(x)\n",
        "        std_z = self.encoder_logvar(x)\n",
        "\n",
        "        z = self.reparameterize(mu_z, std_z)\n",
        "\n",
        "\n",
        "        batch_size = x_num.size(0)\n",
        "        h = self.decoder(z[:,1:])\n",
        "\n",
        "        return h, mu_z, std_z\n",
        "\n",
        "class Reconstructor(nn.Module):\n",
        "    def __init__(self, d_numerical, categories, d_token):\n",
        "        super(Reconstructor, self).__init__()\n",
        "\n",
        "        self.d_numerical = d_numerical\n",
        "        self.categories = categories\n",
        "        self.d_token = d_token\n",
        "\n",
        "        self.weight = nn.Parameter(Tensor(d_numerical, d_token))\n",
        "        nn.init.xavier_uniform_(self.weight, gain=1 / math.sqrt(2))\n",
        "        self.cat_recons = nn.ModuleList()\n",
        "\n",
        "        for d in categories:\n",
        "            recon = nn.Linear(d_token, d)\n",
        "            nn.init.xavier_uniform_(recon.weight, gain=1 / math.sqrt(2))\n",
        "            self.cat_recons.append(recon)\n",
        "\n",
        "    def forward(self, h):\n",
        "        h_num  = h[:, :self.d_numerical]\n",
        "        h_cat  = h[:, self.d_numerical:]\n",
        "\n",
        "        recon_x_num = torch.mul(h_num, self.weight.unsqueeze(0)).sum(-1)\n",
        "        recon_x_cat = []\n",
        "\n",
        "        for i, recon in enumerate(self.cat_recons):\n",
        "\n",
        "            recon_x_cat.append(recon(h_cat[:, i]))\n",
        "\n",
        "        return recon_x_num, recon_x_cat\n",
        "\n",
        "\n",
        "class Model_VAE(nn.Module):\n",
        "    def __init__(self, num_layers, d_numerical, categories, d_token, n_head = 1, factor = 4,  bias = True):\n",
        "        super(Model_VAE, self).__init__()\n",
        "\n",
        "        self.VAE = VAE(d_numerical, categories, num_layers, d_token, n_head = n_head, factor = factor, bias = bias)\n",
        "        self.Reconstructor = Reconstructor(d_numerical, categories, d_token)\n",
        "\n",
        "    def get_embedding(self, x_num, x_cat):\n",
        "        x = self.Tokenizer(x_num, x_cat)\n",
        "        return self.VAE.get_embedding(x)\n",
        "\n",
        "    def forward(self, x_num, x_cat):\n",
        "\n",
        "        h, mu_z, std_z = self.VAE(x_num, x_cat)\n",
        "\n",
        "        # recon_x_num, recon_x_cat = self.Reconstructor(h[:, 1:])\n",
        "        recon_x_num, recon_x_cat = self.Reconstructor(h)\n",
        "\n",
        "        return recon_x_num, recon_x_cat, mu_z, std_z\n",
        "\n",
        "\n",
        "class Encoder_model(nn.Module):\n",
        "    def __init__(self, num_layers, d_numerical, categories, d_token, n_head, factor, bias = True):\n",
        "        super(Encoder_model, self).__init__()\n",
        "        self.Tokenizer = Tokenizer(d_numerical, categories, d_token, bias)\n",
        "        self.VAE_Encoder = Transformer(num_layers, d_token, n_head, d_token, factor)\n",
        "\n",
        "    def load_weights(self, Pretrained_VAE):\n",
        "        self.Tokenizer.load_state_dict(Pretrained_VAE.VAE.Tokenizer.state_dict())\n",
        "        self.VAE_Encoder.load_state_dict(Pretrained_VAE.VAE.encoder_mu.state_dict())\n",
        "\n",
        "    def forward(self, x_num, x_cat):\n",
        "        x = self.Tokenizer(x_num, x_cat)\n",
        "        z = self.VAE_Encoder(x)\n",
        "\n",
        "        return z\n",
        "\n",
        "class Decoder_model(nn.Module):\n",
        "    def __init__(self, num_layers, d_numerical, categories, d_token, n_head, factor, bias = True):\n",
        "        super(Decoder_model, self).__init__()\n",
        "        self.VAE_Decoder = Transformer(num_layers, d_token, n_head, d_token, factor)\n",
        "        self.Detokenizer = Reconstructor(d_numerical, categories, d_token)\n",
        "\n",
        "    def load_weights(self, Pretrained_VAE):\n",
        "        self.VAE_Decoder.load_state_dict(Pretrained_VAE.VAE.decoder.state_dict())\n",
        "        self.Detokenizer.load_state_dict(Pretrained_VAE.Reconstructor.state_dict())\n",
        "\n",
        "    def forward(self, z):\n",
        "\n",
        "        h = self.VAE_Decoder(z)\n",
        "        x_hat_num, x_hat_cat = self.Detokenizer(h)\n",
        "\n",
        "        return x_hat_num, x_hat_cat"
      ],
      "metadata": {
        "id": "b8yxhp_bjdWo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_function(method, mode, epoch):\n",
        "    if method == 'vae':\n",
        "        mode = 'train'\n",
        "    mode = 'main' if mode == 'train' else 'sample'\n",
        "\n",
        "    if method == 'vae':\n",
        "        train_function = tabsyn_vae_main(device = my_device, num_epochs = epoch)\n",
        "    elif method == 'tabsyn':\n",
        "        train_function = tabsyn_main(device = my_device, num_epochs = epoch)\n",
        "    # elif method == 'tabddpm':\n",
        "    #     module_name = f\"baselines.tabddpm.main_train\" if mode == 'main' else f\"baselines.tabddpm.main_sample\"\n",
        "    # else:\n",
        "    #     module_name = f\"baselines.{method}.{mode}\"\n",
        "\n",
        "    # try:\n",
        "    #     train_module =\n",
        "    #     train_function = getattr(train_module, 'main')\n",
        "    # except ModuleNotFoundError:\n",
        "    #     print(f\"Module {module_name} not found.\")\n",
        "    #     exit(1)\n",
        "    # except AttributeError:\n",
        "    #     print(f\"Function 'main' not found in module {module_name}.\")\n",
        "    #     exit(1)\n",
        "    return train_function"
      ],
      "metadata": {
        "id": "T2gKZhl5aZBr"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "main.py"
      ],
      "metadata": {
        "id": "tY-sctO8aNAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "l0SC8mmpaEpG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    my_device = torch.device('cuda')\n",
        "else:\n",
        "    my_device = torch.device('cpu')\n",
        "print('Device: {}'.format(my_device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAe0UdYLbWI0",
        "outputId": "f7d39ee8-2712-4f64-eb58-7521724f8540"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_fn = execute_function('vae', 'train', 2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L584TtfTa5rt",
        "outputId": "18f814f3-5b41-4970-ce84-81874a437d20"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No NaNs in numerical features, skipping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self.category_embeddings.weight.shape=torch.Size([104, 4])\n",
            "self.category_embeddings.weight.shape=torch.Size([104, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, beta = 0.010000, Train MSE: 10.725852, Train CE:2.271110, Train KL:0.363286, Val MSE:8.412203, Val CE:2.296952, Train ACC:0.174624, Val ACC:0.172116\n",
            "epoch: 10, beta = 0.010000, Train MSE: 0.240761, Train CE:1.330678, Train KL:2.959115, Val MSE:0.211182, Val CE:1.260226, Train ACC:0.597469, Val ACC:0.607832\n",
            "epoch: 20, beta = 0.010000, Train MSE: 0.088412, Train CE:0.619188, Train KL:3.951137, Val MSE:0.086055, Val CE:0.600081, Train ACC:0.815520, Val ACC:0.818561\n",
            "epoch: 30, beta = 0.010000, Train MSE: 0.047089, Train CE:0.389552, Train KL:4.301502, Val MSE:0.046549, Val CE:0.379227, Train ACC:0.903488, Val ACC:0.902067\n",
            "epoch: 40, beta = 0.010000, Train MSE: 0.033915, Train CE:0.259195, Train KL:4.503017, Val MSE:0.033589, Val CE:0.254756, Train ACC:0.928716, Val ACC:0.930642\n",
            "epoch: 50, beta = 0.010000, Train MSE: 0.027300, Train CE:0.188894, Train KL:4.471830, Val MSE:0.027296, Val CE:0.185203, Train ACC:0.952744, Val ACC:0.952467\n",
            "epoch: 60, beta = 0.010000, Train MSE: 0.022106, Train CE:0.133138, Train KL:4.435376, Val MSE:0.021861, Val CE:0.130369, Train ACC:0.974115, Val ACC:0.975513\n",
            "epoch: 70, beta = 0.010000, Train MSE: 0.018272, Train CE:0.095003, Train KL:4.365444, Val MSE:0.018423, Val CE:0.093309, Train ACC:0.985029, Val ACC:0.986064\n",
            "epoch: 80, beta = 0.010000, Train MSE: 0.015541, Train CE:0.070819, Train KL:4.248567, Val MSE:0.015534, Val CE:0.069769, Train ACC:0.989029, Val ACC:0.989531\n",
            "epoch: 90, beta = 0.010000, Train MSE: 0.013508, Train CE:0.053922, Train KL:4.088234, Val MSE:0.013319, Val CE:0.053345, Train ACC:0.991372, Val ACC:0.991558\n",
            "epoch: 100, beta = 0.010000, Train MSE: 0.011908, Train CE:0.041972, Train KL:3.913046, Val MSE:0.011928, Val CE:0.041464, Train ACC:0.994343, Val ACC:0.994622\n",
            "epoch: 110, beta = 0.010000, Train MSE: 0.010722, Train CE:0.033363, Train KL:3.738760, Val MSE:0.010612, Val CE:0.033003, Train ACC:0.995943, Val ACC:0.995653\n",
            "epoch: 120, beta = 0.010000, Train MSE: 0.009719, Train CE:0.027211, Train KL:3.561898, Val MSE:0.009650, Val CE:0.026915, Train ACC:0.996257, Val ACC:0.995885\n",
            "epoch: 130, beta = 0.010000, Train MSE: 0.008931, Train CE:0.022556, Train KL:3.368761, Val MSE:0.008931, Val CE:0.022440, Train ACC:0.996543, Val ACC:0.996376\n",
            "epoch: 140, beta = 0.010000, Train MSE: 0.008059, Train CE:0.019034, Train KL:3.185586, Val MSE:0.007838, Val CE:0.018919, Train ACC:0.997000, Val ACC:0.996908\n",
            "epoch: 150, beta = 0.010000, Train MSE: 0.007232, Train CE:0.016292, Train KL:3.032595, Val MSE:0.007182, Val CE:0.016159, Train ACC:0.997800, Val ACC:0.997639\n",
            "epoch: 160, beta = 0.010000, Train MSE: 0.006642, Train CE:0.014189, Train KL:2.908417, Val MSE:0.006574, Val CE:0.013980, Train ACC:0.997886, Val ACC:0.998000\n",
            "epoch: 170, beta = 0.010000, Train MSE: 0.006175, Train CE:0.012569, Train KL:2.792140, Val MSE:0.006019, Val CE:0.012365, Train ACC:0.998314, Val ACC:0.998178\n",
            "epoch: 180, beta = 0.010000, Train MSE: 0.005744, Train CE:0.011227, Train KL:2.681719, Val MSE:0.005666, Val CE:0.011148, Train ACC:0.998086, Val ACC:0.998246\n",
            "epoch: 190, beta = 0.010000, Train MSE: 0.005430, Train CE:0.010158, Train KL:2.585025, Val MSE:0.005186, Val CE:0.009841, Train ACC:0.998400, Val ACC:0.998424\n",
            "epoch: 200, beta = 0.010000, Train MSE: 0.005135, Train CE:0.009122, Train KL:2.499133, Val MSE:0.004785, Val CE:0.008710, Train ACC:0.998886, Val ACC:0.998792\n",
            "epoch: 210, beta = 0.010000, Train MSE: 0.004789, Train CE:0.008147, Train KL:2.410620, Val MSE:0.004768, Val CE:0.008085, Train ACC:0.999086, Val ACC:0.998990\n",
            "epoch: 220, beta = 0.010000, Train MSE: 0.004422, Train CE:0.007340, Train KL:2.342798, Val MSE:0.004296, Val CE:0.007103, Train ACC:0.999000, Val ACC:0.999161\n",
            "epoch: 230, beta = 0.010000, Train MSE: 0.004052, Train CE:0.006622, Train KL:2.264868, Val MSE:0.003990, Val CE:0.006486, Train ACC:0.999314, Val ACC:0.999195\n",
            "epoch: 240, beta = 0.010000, Train MSE: 0.003978, Train CE:0.006032, Train KL:2.190194, Val MSE:0.004007, Val CE:0.006100, Train ACC:0.999171, Val ACC:0.999283\n",
            "epoch: 250, beta = 0.010000, Train MSE: 0.003793, Train CE:0.005449, Train KL:2.130212, Val MSE:0.003718, Val CE:0.005545, Train ACC:0.999457, Val ACC:0.999454\n",
            "epoch: 260, beta = 0.010000, Train MSE: 0.003594, Train CE:0.005017, Train KL:2.111697, Val MSE:0.003511, Val CE:0.004889, Train ACC:0.999429, Val ACC:0.999468\n",
            "epoch: 270, beta = 0.010000, Train MSE: 0.003471, Train CE:0.004659, Train KL:2.069951, Val MSE:0.003502, Val CE:0.004862, Train ACC:0.999257, Val ACC:0.999440\n",
            "epoch: 280, beta = 0.010000, Train MSE: 0.003912, Train CE:0.004274, Train KL:2.094640, Val MSE:0.004102, Val CE:0.004749, Train ACC:0.999314, Val ACC:0.999372\n",
            "epoch: 290, beta = 0.010000, Train MSE: 0.003165, Train CE:0.003850, Train KL:2.021802, Val MSE:0.003171, Val CE:0.003872, Train ACC:0.999543, Val ACC:0.999584\n",
            "epoch: 300, beta = 0.010000, Train MSE: 0.003168, Train CE:0.003630, Train KL:2.001006, Val MSE:0.003174, Val CE:0.003665, Train ACC:0.999600, Val ACC:0.999529\n",
            "epoch: 310, beta = 0.010000, Train MSE: 0.002994, Train CE:0.003315, Train KL:1.981783, Val MSE:0.003016, Val CE:0.003299, Train ACC:0.999486, Val ACC:0.999597\n",
            "epoch: 320, beta = 0.010000, Train MSE: 0.002909, Train CE:0.003068, Train KL:1.942574, Val MSE:0.002862, Val CE:0.003280, Train ACC:0.999771, Val ACC:0.999666\n",
            "epoch: 330, beta = 0.010000, Train MSE: 0.002816, Train CE:0.002873, Train KL:1.935898, Val MSE:0.002814, Val CE:0.003066, Train ACC:0.999714, Val ACC:0.999625\n",
            "epoch: 340, beta = 0.007000, Train MSE: 0.002280, Train CE:0.002507, Train KL:1.983628, Val MSE:0.002336, Val CE:0.002918, Train ACC:0.999829, Val ACC:0.999631\n",
            "epoch: 350, beta = 0.007000, Train MSE: 0.002135, Train CE:0.002244, Train KL:1.994473, Val MSE:0.002221, Val CE:0.002396, Train ACC:0.999829, Val ACC:0.999754\n",
            "epoch: 360, beta = 0.007000, Train MSE: 0.002116, Train CE:0.002172, Train KL:1.977709, Val MSE:0.002230, Val CE:0.002352, Train ACC:0.999829, Val ACC:0.999713\n",
            "epoch: 370, beta = 0.007000, Train MSE: 0.002064, Train CE:0.001988, Train KL:1.951910, Val MSE:0.002071, Val CE:0.001998, Train ACC:0.999771, Val ACC:0.999816\n",
            "epoch: 380, beta = 0.007000, Train MSE: 0.001982, Train CE:0.001864, Train KL:1.951465, Val MSE:0.002014, Val CE:0.001971, Train ACC:0.999829, Val ACC:0.999775\n",
            "epoch: 390, beta = 0.007000, Train MSE: 0.001882, Train CE:0.001809, Train KL:1.940001, Val MSE:0.001809, Val CE:0.001681, Train ACC:0.999800, Val ACC:0.999823\n",
            "epoch: 400, beta = 0.007000, Train MSE: 0.001881, Train CE:0.001658, Train KL:1.931705, Val MSE:0.002081, Val CE:0.001854, Train ACC:0.999800, Val ACC:0.999782\n",
            "epoch: 410, beta = 0.004900, Train MSE: 0.001461, Train CE:0.001444, Train KL:1.994297, Val MSE:0.001471, Val CE:0.001499, Train ACC:0.999886, Val ACC:0.999829\n",
            "epoch: 420, beta = 0.004900, Train MSE: 0.001458, Train CE:0.001358, Train KL:1.968292, Val MSE:0.001377, Val CE:0.001342, Train ACC:0.999886, Val ACC:0.999823\n",
            "epoch: 430, beta = 0.004900, Train MSE: 0.001352, Train CE:0.001231, Train KL:2.000132, Val MSE:0.001213, Val CE:0.001077, Train ACC:0.999829, Val ACC:0.999891\n",
            "epoch: 440, beta = 0.003430, Train MSE: 0.001385, Train CE:0.001237, Train KL:1.966206, Val MSE:0.001325, Val CE:0.001160, Train ACC:1.000000, Val ACC:0.999884\n",
            "Learning rate updated: 0.00095\n",
            "epoch: 450, beta = 0.003430, Train MSE: 0.001026, Train CE:0.001006, Train KL:2.051268, Val MSE:0.001071, Val CE:0.001029, Train ACC:0.999857, Val ACC:0.999891\n",
            "epoch: 460, beta = 0.003430, Train MSE: 0.001063, Train CE:0.000932, Train KL:2.032946, Val MSE:0.001005, Val CE:0.000899, Train ACC:0.999943, Val ACC:0.999925\n",
            "epoch: 470, beta = 0.003430, Train MSE: 0.001005, Train CE:0.000889, Train KL:2.041173, Val MSE:0.000937, Val CE:0.000820, Train ACC:0.999943, Val ACC:0.999952\n",
            "Learning rate updated: 0.0009025\n",
            "epoch: 480, beta = 0.002401, Train MSE: 0.000813, Train CE:0.000752, Train KL:2.098227, Val MSE:0.000793, Val CE:0.000764, Train ACC:1.000000, Val ACC:0.999980\n",
            "epoch: 490, beta = 0.002401, Train MSE: 0.000832, Train CE:0.000719, Train KL:2.090062, Val MSE:0.000860, Val CE:0.000666, Train ACC:1.000000, Val ACC:0.999993\n",
            "epoch: 500, beta = 0.002401, Train MSE: 0.000730, Train CE:0.000655, Train KL:2.117543, Val MSE:0.000697, Val CE:0.000625, Train ACC:1.000000, Val ACC:0.999993\n",
            "epoch: 510, beta = 0.002401, Train MSE: 0.000767, Train CE:0.000619, Train KL:2.150301, Val MSE:0.000818, Val CE:0.000805, Train ACC:0.999971, Val ACC:0.999980\n",
            "epoch: 520, beta = 0.002401, Train MSE: 0.000712, Train CE:0.000586, Train KL:2.081405, Val MSE:0.000682, Val CE:0.000636, Train ACC:1.000000, Val ACC:0.999973\n",
            "epoch: 530, beta = 0.002401, Train MSE: 0.000736, Train CE:0.000581, Train KL:2.097950, Val MSE:0.000817, Val CE:0.000543, Train ACC:0.999971, Val ACC:0.999980\n",
            "epoch: 540, beta = 0.002401, Train MSE: 0.000860, Train CE:0.000549, Train KL:2.073804, Val MSE:0.000838, Val CE:0.000502, Train ACC:0.999971, Val ACC:1.000000\n",
            "epoch: 550, beta = 0.002401, Train MSE: 0.000656, Train CE:0.000507, Train KL:2.113544, Val MSE:0.000626, Val CE:0.000481, Train ACC:1.000000, Val ACC:0.999993\n",
            "epoch: 560, beta = 0.001681, Train MSE: 0.000538, Train CE:0.000416, Train KL:2.232431, Val MSE:0.000540, Val CE:0.000460, Train ACC:1.000000, Val ACC:0.999980\n",
            "epoch: 570, beta = 0.001681, Train MSE: 0.000684, Train CE:0.000411, Train KL:2.178079, Val MSE:0.001086, Val CE:0.000415, Train ACC:1.000000, Val ACC:0.999993\n",
            "epoch: 580, beta = 0.001681, Train MSE: 0.000510, Train CE:0.000398, Train KL:2.167601, Val MSE:0.000504, Val CE:0.000456, Train ACC:1.000000, Val ACC:0.999980\n",
            "epoch: 590, beta = 0.001176, Train MSE: 0.000466, Train CE:0.000391, Train KL:2.244885, Val MSE:0.000374, Val CE:0.000318, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 600, beta = 0.000824, Train MSE: 0.000424, Train CE:0.000360, Train KL:2.230003, Val MSE:0.000430, Val CE:0.000325, Train ACC:1.000000, Val ACC:0.999986\n",
            "epoch: 610, beta = 0.000824, Train MSE: 0.000365, Train CE:0.000307, Train KL:2.298139, Val MSE:0.000313, Val CE:0.000290, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 620, beta = 0.000824, Train MSE: 0.000424, Train CE:0.000292, Train KL:2.308948, Val MSE:0.000568, Val CE:0.000292, Train ACC:0.999971, Val ACC:1.000000\n",
            "epoch: 630, beta = 0.000824, Train MSE: 0.000367, Train CE:0.000274, Train KL:2.321822, Val MSE:0.000343, Val CE:0.000263, Train ACC:0.999971, Val ACC:0.999993\n",
            "epoch: 640, beta = 0.000824, Train MSE: 0.000270, Train CE:0.000242, Train KL:2.393401, Val MSE:0.000319, Val CE:0.000295, Train ACC:1.000000, Val ACC:0.999980\n",
            "epoch: 650, beta = 0.000824, Train MSE: 0.000315, Train CE:0.000237, Train KL:2.354360, Val MSE:0.000295, Val CE:0.000234, Train ACC:1.000000, Val ACC:0.999993\n",
            "epoch: 660, beta = 0.000824, Train MSE: 0.000290, Train CE:0.000248, Train KL:2.294199, Val MSE:0.000270, Val CE:0.000252, Train ACC:1.000000, Val ACC:0.999986\n",
            "epoch: 670, beta = 0.000824, Train MSE: 0.000304, Train CE:0.000228, Train KL:2.294498, Val MSE:0.000263, Val CE:0.000210, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 680, beta = 0.000824, Train MSE: 0.000283, Train CE:0.000245, Train KL:2.280845, Val MSE:0.000257, Val CE:0.000204, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 690, beta = 0.000824, Train MSE: 0.000307, Train CE:0.000203, Train KL:2.320364, Val MSE:0.000422, Val CE:0.000208, Train ACC:1.000000, Val ACC:0.999993\n",
            "epoch: 700, beta = 0.000576, Train MSE: 0.000243, Train CE:0.000189, Train KL:2.336479, Val MSE:0.000254, Val CE:0.000189, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 710, beta = 0.000576, Train MSE: 0.000218, Train CE:0.000174, Train KL:2.523901, Val MSE:0.000234, Val CE:0.000179, Train ACC:1.000000, Val ACC:0.999993\n",
            "epoch: 720, beta = 0.000576, Train MSE: 0.000215, Train CE:0.000171, Train KL:2.381645, Val MSE:0.000196, Val CE:0.000168, Train ACC:1.000000, Val ACC:0.999993\n",
            "epoch: 730, beta = 0.000576, Train MSE: 0.000739, Train CE:0.000221, Train KL:2.490727, Val MSE:0.000792, Val CE:0.000154, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 740, beta = 0.000576, Train MSE: 0.000228, Train CE:0.000160, Train KL:2.535515, Val MSE:0.000288, Val CE:0.000162, Train ACC:0.999971, Val ACC:1.000000\n",
            "Learning rate updated: 0.000857375\n",
            "epoch: 750, beta = 0.000404, Train MSE: 0.000165, Train CE:0.000141, Train KL:2.494505, Val MSE:0.000163, Val CE:0.000136, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 760, beta = 0.000404, Train MSE: 0.000147, Train CE:0.000148, Train KL:2.645450, Val MSE:0.000176, Val CE:0.000146, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 770, beta = 0.000404, Train MSE: 0.000170, Train CE:0.000143, Train KL:2.439179, Val MSE:0.000166, Val CE:0.000123, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 780, beta = 0.000404, Train MSE: 0.000162, Train CE:0.000129, Train KL:2.469973, Val MSE:0.000147, Val CE:0.000118, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 790, beta = 0.000404, Train MSE: 0.000165, Train CE:0.000128, Train KL:2.462003, Val MSE:0.000140, Val CE:0.000112, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 800, beta = 0.000404, Train MSE: 0.000200, Train CE:0.000122, Train KL:2.426258, Val MSE:0.000166, Val CE:0.000109, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 810, beta = 0.000282, Train MSE: 0.000305, Train CE:0.000107, Train KL:2.580064, Val MSE:0.000410, Val CE:0.000102, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 820, beta = 0.000282, Train MSE: 0.000129, Train CE:0.000105, Train KL:2.504931, Val MSE:0.000156, Val CE:0.000100, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 830, beta = 0.000282, Train MSE: 0.000114, Train CE:0.000102, Train KL:2.559784, Val MSE:0.000107, Val CE:0.000093, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 840, beta = 0.000282, Train MSE: 0.000173, Train CE:0.000096, Train KL:2.571017, Val MSE:0.000135, Val CE:0.000102, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 850, beta = 0.000282, Train MSE: 0.000146, Train CE:0.000100, Train KL:2.490097, Val MSE:0.000101, Val CE:0.000087, Train ACC:0.999971, Val ACC:1.000000\n",
            "epoch: 860, beta = 0.000198, Train MSE: 0.000100, Train CE:0.000087, Train KL:2.590918, Val MSE:0.000091, Val CE:0.000081, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 870, beta = 0.000138, Train MSE: 0.000195, Train CE:0.000084, Train KL:2.623550, Val MSE:0.000122, Val CE:0.000075, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 880, beta = 0.000138, Train MSE: 0.000095, Train CE:0.000074, Train KL:2.979916, Val MSE:0.000093, Val CE:0.000068, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.0008145062499999999\n",
            "epoch: 890, beta = 0.000097, Train MSE: 0.000079, Train CE:0.000074, Train KL:2.749759, Val MSE:0.000070, Val CE:0.000067, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 900, beta = 0.000097, Train MSE: 0.000080, Train CE:0.000071, Train KL:2.770094, Val MSE:0.000064, Val CE:0.000064, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 910, beta = 0.000097, Train MSE: 0.000079, Train CE:0.000068, Train KL:2.771447, Val MSE:0.000066, Val CE:0.000062, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 920, beta = 0.000097, Train MSE: 0.000238, Train CE:0.000066, Train KL:2.778931, Val MSE:0.000229, Val CE:0.000060, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 930, beta = 0.000097, Train MSE: 0.000084, Train CE:0.000064, Train KL:2.738446, Val MSE:0.000080, Val CE:0.000057, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 940, beta = 0.000097, Train MSE: 0.000170, Train CE:0.000061, Train KL:2.757960, Val MSE:0.000291, Val CE:0.000055, Train ACC:0.999971, Val ACC:1.000000\n",
            "epoch: 950, beta = 0.000097, Train MSE: 0.000060, Train CE:0.000056, Train KL:3.051998, Val MSE:0.000054, Val CE:0.000051, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 960, beta = 0.000068, Train MSE: 0.000077, Train CE:0.000059, Train KL:2.811965, Val MSE:0.000052, Val CE:0.000050, Train ACC:0.999971, Val ACC:1.000000\n",
            "epoch: 970, beta = 0.000068, Train MSE: 0.000454, Train CE:0.000056, Train KL:2.927016, Val MSE:0.000493, Val CE:0.000051, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.0007737809374999998\n",
            "epoch: 980, beta = 0.000047, Train MSE: 0.000053, Train CE:0.000051, Train KL:2.974543, Val MSE:0.000043, Val CE:0.000046, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 990, beta = 0.000047, Train MSE: 0.000051, Train CE:0.000051, Train KL:2.967168, Val MSE:0.000040, Val CE:0.000044, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1000, beta = 0.000047, Train MSE: 0.000048, Train CE:0.000046, Train KL:3.236860, Val MSE:0.000040, Val CE:0.000042, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1010, beta = 0.000033, Train MSE: 0.000059, Train CE:0.000046, Train KL:2.980149, Val MSE:0.000042, Val CE:0.000041, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1020, beta = 0.000033, Train MSE: 0.000046, Train CE:0.000044, Train KL:3.054973, Val MSE:0.000038, Val CE:0.000039, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1030, beta = 0.000033, Train MSE: 0.000047, Train CE:0.000042, Train KL:3.067363, Val MSE:0.000042, Val CE:0.000038, Train ACC:0.999971, Val ACC:1.000000\n",
            "epoch: 1040, beta = 0.000033, Train MSE: 0.000072, Train CE:0.000041, Train KL:3.073331, Val MSE:0.000045, Val CE:0.000037, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1050, beta = 0.000033, Train MSE: 0.000464, Train CE:0.000040, Train KL:3.084846, Val MSE:0.000196, Val CE:0.000035, Train ACC:0.999971, Val ACC:1.000000\n",
            "epoch: 1060, beta = 0.000033, Train MSE: 0.000049, Train CE:0.000038, Train KL:3.059476, Val MSE:0.000035, Val CE:0.000034, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1070, beta = 0.000033, Train MSE: 0.000045, Train CE:0.000039, Train KL:3.139327, Val MSE:0.000034, Val CE:0.000034, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1080, beta = 0.000033, Train MSE: 0.000070, Train CE:0.000037, Train KL:3.084413, Val MSE:0.000038, Val CE:0.000033, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1090, beta = 0.000033, Train MSE: 0.000044, Train CE:0.000035, Train KL:3.061328, Val MSE:0.000034, Val CE:0.000031, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1100, beta = 0.000033, Train MSE: 0.000115, Train CE:0.000033, Train KL:3.082802, Val MSE:0.000099, Val CE:0.000030, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1110, beta = 0.000033, Train MSE: 0.000147, Train CE:0.000032, Train KL:3.088473, Val MSE:0.000217, Val CE:0.000029, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1120, beta = 0.000033, Train MSE: 0.000085, Train CE:0.000031, Train KL:3.067932, Val MSE:0.000036, Val CE:0.000029, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1130, beta = 0.000033, Train MSE: 0.000053, Train CE:0.000030, Train KL:3.075176, Val MSE:0.000031, Val CE:0.000028, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1140, beta = 0.000033, Train MSE: 0.000039, Train CE:0.000029, Train KL:3.085527, Val MSE:0.000043, Val CE:0.000031, Train ACC:1.000000, Val ACC:0.999993\n",
            "epoch: 1150, beta = 0.000033, Train MSE: 0.000041, Train CE:0.000028, Train KL:3.135679, Val MSE:0.000031, Val CE:0.000025, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1160, beta = 0.000033, Train MSE: 0.000045, Train CE:0.000027, Train KL:3.626802, Val MSE:0.000030, Val CE:0.000024, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1170, beta = 0.000033, Train MSE: 0.000043, Train CE:0.000027, Train KL:3.300943, Val MSE:0.000030, Val CE:0.000023, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1180, beta = 0.000023, Train MSE: 0.000037, Train CE:0.000025, Train KL:3.277797, Val MSE:0.000029, Val CE:0.000022, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1190, beta = 0.000023, Train MSE: 0.000164, Train CE:0.000024, Train KL:3.232536, Val MSE:0.000395, Val CE:0.000022, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1200, beta = 0.000023, Train MSE: 0.000074, Train CE:0.000023, Train KL:3.218174, Val MSE:0.000118, Val CE:0.000021, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1210, beta = 0.000023, Train MSE: 0.000041, Train CE:0.000023, Train KL:3.213692, Val MSE:0.000026, Val CE:0.000020, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1220, beta = 0.000023, Train MSE: 0.000034, Train CE:0.000022, Train KL:3.286754, Val MSE:0.000030, Val CE:0.000020, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1230, beta = 0.000016, Train MSE: 0.000040, Train CE:0.000021, Train KL:3.322904, Val MSE:0.000023, Val CE:0.000019, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1240, beta = 0.000016, Train MSE: 0.000029, Train CE:0.000020, Train KL:3.286545, Val MSE:0.000025, Val CE:0.000018, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1250, beta = 0.000016, Train MSE: 0.000034, Train CE:0.000019, Train KL:3.450924, Val MSE:0.000031, Val CE:0.000017, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1260, beta = 0.000016, Train MSE: 0.000040, Train CE:0.000019, Train KL:3.318354, Val MSE:0.000034, Val CE:0.000017, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1270, beta = 0.000016, Train MSE: 0.000054, Train CE:0.000018, Train KL:3.318059, Val MSE:0.000057, Val CE:0.000018, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1280, beta = 0.000016, Train MSE: 0.000208, Train CE:0.000018, Train KL:3.331335, Val MSE:0.000051, Val CE:0.000016, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1290, beta = 0.000016, Train MSE: 0.000413, Train CE:0.000017, Train KL:3.399587, Val MSE:0.000252, Val CE:0.000015, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1300, beta = 0.000016, Train MSE: 0.000028, Train CE:0.000016, Train KL:3.349325, Val MSE:0.000019, Val CE:0.000015, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1310, beta = 0.000016, Train MSE: 0.000063, Train CE:0.000016, Train KL:3.311700, Val MSE:0.000027, Val CE:0.000014, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1320, beta = 0.000016, Train MSE: 0.000562, Train CE:0.000016, Train KL:3.533316, Val MSE:0.000125, Val CE:0.000015, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1330, beta = 0.000011, Train MSE: 0.000025, Train CE:0.000015, Train KL:3.457705, Val MSE:0.000018, Val CE:0.000013, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1340, beta = 0.000011, Train MSE: 0.000151, Train CE:0.000014, Train KL:3.470713, Val MSE:0.000057, Val CE:0.000013, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1350, beta = 0.000011, Train MSE: 0.000071, Train CE:0.000014, Train KL:3.454313, Val MSE:0.000032, Val CE:0.000012, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1360, beta = 0.000011, Train MSE: 0.000356, Train CE:0.000013, Train KL:3.440181, Val MSE:0.000107, Val CE:0.000012, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1370, beta = 0.000011, Train MSE: 0.000077, Train CE:0.000013, Train KL:3.539933, Val MSE:0.000050, Val CE:0.000012, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1380, beta = 0.000008, Train MSE: 0.000033, Train CE:0.000012, Train KL:3.523405, Val MSE:0.000049, Val CE:0.000011, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1390, beta = 0.000008, Train MSE: 0.000025, Train CE:0.000012, Train KL:3.558790, Val MSE:0.000017, Val CE:0.000011, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1400, beta = 0.000008, Train MSE: 0.000022, Train CE:0.000011, Train KL:3.608998, Val MSE:0.000016, Val CE:0.000010, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1410, beta = 0.000008, Train MSE: 0.000032, Train CE:0.000011, Train KL:3.571237, Val MSE:0.000022, Val CE:0.000010, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1420, beta = 0.000008, Train MSE: 0.000030, Train CE:0.000011, Train KL:3.534978, Val MSE:0.000015, Val CE:0.000010, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1430, beta = 0.000008, Train MSE: 0.000046, Train CE:0.000010, Train KL:3.538391, Val MSE:0.000037, Val CE:0.000009, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1440, beta = 0.000008, Train MSE: 0.000039, Train CE:0.000010, Train KL:3.566573, Val MSE:0.000024, Val CE:0.000011, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1450, beta = 0.000008, Train MSE: 0.000042, Train CE:0.000010, Train KL:3.545762, Val MSE:0.000090, Val CE:0.000009, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1460, beta = 0.000008, Train MSE: 0.000027, Train CE:0.000009, Train KL:3.541920, Val MSE:0.000014, Val CE:0.000008, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1470, beta = 0.000008, Train MSE: 0.000067, Train CE:0.000009, Train KL:3.539541, Val MSE:0.000026, Val CE:0.000008, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1480, beta = 0.000008, Train MSE: 0.000022, Train CE:0.000011, Train KL:3.986553, Val MSE:0.000020, Val CE:0.000008, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1490, beta = 0.000008, Train MSE: 0.000023, Train CE:0.000008, Train KL:3.670289, Val MSE:0.000015, Val CE:0.000008, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.0007350918906249997\n",
            "epoch: 1500, beta = 0.000008, Train MSE: 0.000020, Train CE:0.000008, Train KL:3.605496, Val MSE:0.000013, Val CE:0.000007, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1510, beta = 0.000008, Train MSE: 0.000050, Train CE:0.000008, Train KL:3.589096, Val MSE:0.000132, Val CE:0.000007, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1520, beta = 0.000008, Train MSE: 0.000023, Train CE:0.000008, Train KL:3.583085, Val MSE:0.000013, Val CE:0.000007, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1530, beta = 0.000008, Train MSE: 0.000039, Train CE:0.000007, Train KL:3.574505, Val MSE:0.000015, Val CE:0.000007, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1540, beta = 0.000008, Train MSE: 0.000020, Train CE:0.000007, Train KL:3.563646, Val MSE:0.000015, Val CE:0.000007, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1550, beta = 0.000008, Train MSE: 0.000037, Train CE:0.000007, Train KL:3.552643, Val MSE:0.000020, Val CE:0.000006, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1560, beta = 0.000008, Train MSE: 0.000021, Train CE:0.000007, Train KL:3.545620, Val MSE:0.000012, Val CE:0.000006, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1570, beta = 0.000008, Train MSE: 0.000024, Train CE:0.000007, Train KL:3.560114, Val MSE:0.000017, Val CE:0.000006, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1580, beta = 0.000008, Train MSE: 0.000211, Train CE:0.000006, Train KL:3.566669, Val MSE:0.000423, Val CE:0.000006, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1590, beta = 0.000008, Train MSE: 0.000020, Train CE:0.000006, Train KL:3.544968, Val MSE:0.000018, Val CE:0.000006, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1600, beta = 0.000008, Train MSE: 0.000251, Train CE:0.000006, Train KL:3.572234, Val MSE:0.000098, Val CE:0.000006, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1610, beta = 0.000008, Train MSE: 0.000019, Train CE:0.000006, Train KL:3.582893, Val MSE:0.000012, Val CE:0.000005, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.0006983372960937497\n",
            "epoch: 1620, beta = 0.000008, Train MSE: 0.000035, Train CE:0.000008, Train KL:6.560580, Val MSE:0.000029, Val CE:0.000007, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.0006634204312890621\n",
            "epoch: 1630, beta = 0.000008, Train MSE: 0.000018, Train CE:0.000006, Train KL:5.424777, Val MSE:0.000012, Val CE:0.000005, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1640, beta = 0.000008, Train MSE: 0.000017, Train CE:0.000006, Train KL:4.946510, Val MSE:0.000011, Val CE:0.000005, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1650, beta = 0.000008, Train MSE: 0.000017, Train CE:0.000005, Train KL:4.533743, Val MSE:0.000012, Val CE:0.000005, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1660, beta = 0.000008, Train MSE: 0.000435, Train CE:0.000005, Train KL:4.509468, Val MSE:0.000189, Val CE:0.000005, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1670, beta = 0.000008, Train MSE: 0.000019, Train CE:0.000005, Train KL:4.363607, Val MSE:0.000014, Val CE:0.000004, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1680, beta = 0.000008, Train MSE: 0.000034, Train CE:0.000005, Train KL:5.585531, Val MSE:0.000038, Val CE:0.000004, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1690, beta = 0.000008, Train MSE: 0.000068, Train CE:0.000004, Train KL:5.165308, Val MSE:0.000083, Val CE:0.000004, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.000630249409724609\n",
            "epoch: 1700, beta = 0.000008, Train MSE: 0.000036, Train CE:0.000004, Train KL:4.869419, Val MSE:0.000017, Val CE:0.000004, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1710, beta = 0.000008, Train MSE: 0.000016, Train CE:0.000004, Train KL:4.651685, Val MSE:0.000011, Val CE:0.000004, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1720, beta = 0.000008, Train MSE: 0.000016, Train CE:0.000004, Train KL:4.531980, Val MSE:0.000011, Val CE:0.000004, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1730, beta = 0.000008, Train MSE: 0.000059, Train CE:0.000005, Train KL:4.791793, Val MSE:0.000028, Val CE:0.000004, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1740, beta = 0.000008, Train MSE: 0.000054, Train CE:0.000004, Train KL:4.489342, Val MSE:0.000087, Val CE:0.000004, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.0005987369392383785\n",
            "epoch: 1750, beta = 0.000008, Train MSE: 0.000017, Train CE:0.000004, Train KL:4.352384, Val MSE:0.000011, Val CE:0.000004, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.0005688000922764595\n",
            "epoch: 1760, beta = 0.000008, Train MSE: 0.000016, Train CE:0.000004, Train KL:4.762665, Val MSE:0.000012, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.0005403600876626365\n",
            "epoch: 1770, beta = 0.000008, Train MSE: 0.000018, Train CE:0.000004, Train KL:4.306612, Val MSE:0.000013, Val CE:0.000004, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.0005133420832795047\n",
            "epoch: 1780, beta = 0.000008, Train MSE: 0.000015, Train CE:0.000004, Train KL:4.194940, Val MSE:0.000011, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1790, beta = 0.000008, Train MSE: 0.000015, Train CE:0.000004, Train KL:4.125411, Val MSE:0.000010, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1800, beta = 0.000008, Train MSE: 0.000018, Train CE:0.000004, Train KL:4.047767, Val MSE:0.000019, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1810, beta = 0.000008, Train MSE: 0.000017, Train CE:0.000004, Train KL:4.002363, Val MSE:0.000013, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1820, beta = 0.000008, Train MSE: 0.000028, Train CE:0.000003, Train KL:3.962532, Val MSE:0.000013, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1830, beta = 0.000008, Train MSE: 0.000021, Train CE:0.000003, Train KL:3.916188, Val MSE:0.000024, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.00048767497911552944\n",
            "epoch: 1840, beta = 0.000008, Train MSE: 0.000014, Train CE:0.000003, Train KL:4.528862, Val MSE:0.000010, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.00046329123015975297\n",
            "epoch: 1850, beta = 0.000008, Train MSE: 0.000015, Train CE:0.000003, Train KL:4.079647, Val MSE:0.000009, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.0004401266686517653\n",
            "epoch: 1860, beta = 0.000008, Train MSE: 0.000016, Train CE:0.000003, Train KL:3.967718, Val MSE:0.000010, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1870, beta = 0.000008, Train MSE: 0.000014, Train CE:0.000003, Train KL:4.046974, Val MSE:0.000009, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1880, beta = 0.000008, Train MSE: 0.000014, Train CE:0.000003, Train KL:4.019114, Val MSE:0.000010, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.00041812033521917703\n",
            "epoch: 1890, beta = 0.000008, Train MSE: 0.000016, Train CE:0.000003, Train KL:3.845658, Val MSE:0.000009, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.00039721431845821814\n",
            "epoch: 1900, beta = 0.000008, Train MSE: 0.000014, Train CE:0.000003, Train KL:3.768612, Val MSE:0.000008, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.0003773536025353072\n",
            "epoch: 1910, beta = 0.000008, Train MSE: 0.000013, Train CE:0.000003, Train KL:3.744953, Val MSE:0.000008, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.0003584859224085418\n",
            "epoch: 1920, beta = 0.000008, Train MSE: 0.000013, Train CE:0.000003, Train KL:3.728336, Val MSE:0.000008, Val CE:0.000002, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1930, beta = 0.000008, Train MSE: 0.000013, Train CE:0.000003, Train KL:3.679234, Val MSE:0.000009, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.0003405616262881147\n",
            "epoch: 1940, beta = 0.000008, Train MSE: 0.000013, Train CE:0.000003, Train KL:3.659818, Val MSE:0.000009, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.00032353354497370894\n",
            "epoch: 1950, beta = 0.000008, Train MSE: 0.000013, Train CE:0.000003, Train KL:3.612635, Val MSE:0.000009, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1960, beta = 0.000008, Train MSE: 0.000012, Train CE:0.000003, Train KL:3.667614, Val MSE:0.000008, Val CE:0.000003, Train ACC:1.000000, Val ACC:1.000000\n",
            "epoch: 1970, beta = 0.000008, Train MSE: 0.000016, Train CE:0.000003, Train KL:3.614014, Val MSE:0.000009, Val CE:0.000002, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.00030735686772502346\n",
            "epoch: 1980, beta = 0.000008, Train MSE: 0.000016, Train CE:0.000003, Train KL:3.602135, Val MSE:0.000008, Val CE:0.000002, Train ACC:1.000000, Val ACC:1.000000\n",
            "Learning rate updated: 0.00029198902433877225\n",
            "epoch: 1990, beta = 0.000008, Train MSE: 0.000012, Train CE:0.000003, Train KL:3.613792, Val MSE:0.000008, Val CE:0.000002, Train ACC:1.000000, Val ACC:1.000000\n",
            "Training time: 31.3991 mins\n",
            "Successfully load and save the model!\n",
            "Successfully save pretrained embeddings in disk!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выше было обучение VAE"
      ],
      "metadata": {
        "id": "LgmLqhDCKOfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### def get_input_train"
      ],
      "metadata": {
        "id": "Rm3mzv-wTfRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_train(dataname = 'adult'):\n",
        "    # dataname = args.dataname\n",
        "\n",
        "    curr_dir = 'tabsyn'#os.path.dirname(os.path.abspath(__file__))\n",
        "    dataset_dir = f'data/{dataname}'\n",
        "\n",
        "    with open(f'{dataset_dir}/info.json', 'r') as f:\n",
        "        info = json.load(f)\n",
        "\n",
        "    ckpt_dir = f'model'\n",
        "    embedding_save_path = f'{ckpt_dir}/train_z.npy'\n",
        "    train_z = torch.tensor(np.load(embedding_save_path)).float()\n",
        "\n",
        "    train_z = train_z[:, 1:, :]\n",
        "    B, num_tokens, token_dim = train_z.size()\n",
        "    in_dim = num_tokens * token_dim\n",
        "\n",
        "    train_z = train_z.view(B, in_dim)\n",
        "\n",
        "    return train_z, curr_dir, dataset_dir, ckpt_dir, info"
      ],
      "metadata": {
        "id": "JqOc9QIWLB4Y"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### class PositionalEmbedding"
      ],
      "metadata": {
        "id": "rB19UR-RTkRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(torch.nn.Module):\n",
        "    def __init__(self, num_channels, max_positions=10000, endpoint=False):\n",
        "        super().__init__()\n",
        "        self.num_channels = num_channels\n",
        "        self.max_positions = max_positions\n",
        "        self.endpoint = endpoint\n",
        "\n",
        "    def forward(self, x):\n",
        "        freqs = torch.arange(start=0, end=self.num_channels//2, dtype=torch.float32, device=x.device)\n",
        "        freqs = freqs / (self.num_channels // 2 - (1 if self.endpoint else 0))\n",
        "        freqs = (1 / self.max_positions) ** freqs\n",
        "        x = x.ger(freqs.to(x.dtype))\n",
        "        x = torch.cat([x.cos(), x.sin()], dim=1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ly6W-NvHNE57"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### class MLPDiffusion"
      ],
      "metadata": {
        "id": "tgLNLRYJTp7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPDiffusion(nn.Module):\n",
        "    def __init__(self, d_in, dim_t = 512):\n",
        "        super().__init__()\n",
        "        self.dim_t = dim_t\n",
        "\n",
        "        self.proj = nn.Linear(d_in, dim_t)\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim_t, dim_t * 2),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(dim_t * 2, dim_t * 2),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(dim_t * 2, dim_t),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(dim_t, d_in),\n",
        "        )\n",
        "\n",
        "        self.map_noise = PositionalEmbedding(num_channels=dim_t)\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(dim_t, dim_t),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(dim_t, dim_t)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, noise_labels, class_labels=None):\n",
        "        emb = self.map_noise(noise_labels)\n",
        "        emb = emb.reshape(emb.shape[0], 2, -1).flip(1).reshape(*emb.shape) # swap sin/cos\n",
        "        emb = self.time_embed(emb)\n",
        "\n",
        "        x = self.proj(x) + emb\n",
        "        return self.mlp(x)"
      ],
      "metadata": {
        "id": "GO1GbsYbM-zM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EDMLoss:\n",
        "    def __init__(self, P_mean=-1.2, P_std=1.2, sigma_data=0.5, hid_dim = 100, gamma=5, opts=None):\n",
        "        self.P_mean = P_mean\n",
        "        self.P_std = P_std\n",
        "        self.sigma_data = sigma_data\n",
        "        self.hid_dim = hid_dim\n",
        "        self.gamma = gamma\n",
        "        self.opts = opts\n",
        "\n",
        "\n",
        "    def __call__(self, denoise_fn, data):\n",
        "\n",
        "        rnd_normal = torch.randn(data.shape[0], device=data.device)\n",
        "        sigma = (rnd_normal * self.P_std + self.P_mean).exp()\n",
        "\n",
        "        weight = (sigma ** 2 + self.sigma_data ** 2) / (sigma * self.sigma_data) ** 2\n",
        "\n",
        "        y = data\n",
        "        n = torch.randn_like(y) * sigma.unsqueeze(1)\n",
        "        D_yn = denoise_fn(y + n, sigma)\n",
        "\n",
        "        target = y\n",
        "        loss = weight.unsqueeze(1) * ((D_yn - target) ** 2)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "TAfCjQDhS3j8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Precond(nn.Module):\n",
        "    def __init__(self,\n",
        "        denoise_fn,\n",
        "        hid_dim,\n",
        "        sigma_min = 0,                # Minimum supported noise level.\n",
        "        sigma_max = float('inf'),     # Maximum supported noise level.\n",
        "        sigma_data = 0.5,              # Expected standard deviation of the training data.\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.sigma_min = sigma_min\n",
        "        self.sigma_max = sigma_max\n",
        "        self.sigma_data = sigma_data\n",
        "        ###########\n",
        "        self.denoise_fn_F = denoise_fn\n",
        "\n",
        "    def forward(self, x, sigma):\n",
        "\n",
        "        x = x.to(torch.float32)\n",
        "\n",
        "        sigma = sigma.to(torch.float32).reshape(-1, 1)\n",
        "        dtype = torch.float32\n",
        "\n",
        "        c_skip = self.sigma_data ** 2 / (sigma ** 2 + self.sigma_data ** 2)\n",
        "        c_out = sigma * self.sigma_data / (sigma ** 2 + self.sigma_data ** 2).sqrt()\n",
        "        c_in = 1 / (self.sigma_data ** 2 + sigma ** 2).sqrt()\n",
        "        c_noise = sigma.log() / 4\n",
        "\n",
        "        x_in = c_in * x\n",
        "        F_x = self.denoise_fn_F((x_in).to(dtype), c_noise.flatten())\n",
        "\n",
        "        assert F_x.dtype == dtype\n",
        "        D_x = c_skip * x + c_out * F_x.to(torch.float32)\n",
        "        return D_x\n",
        "\n",
        "    def round_sigma(self, sigma):\n",
        "        return torch.as_tensor(sigma)"
      ],
      "metadata": {
        "id": "0Nl0tdSuNeVw"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, denoise_fn, hid_dim, P_mean=-1.2, P_std=1.2, sigma_data=0.5, gamma=5, opts=None, pfgmpp = False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.denoise_fn_D = Precond(denoise_fn, hid_dim)\n",
        "        self.loss_fn = EDMLoss(P_mean, P_std, sigma_data, hid_dim=hid_dim, gamma=5, opts=None)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        loss = self.loss_fn(self.denoise_fn_D, x)\n",
        "        return loss.mean(-1).mean()"
      ],
      "metadata": {
        "id": "MXJzpkw0NPHh"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tabsyn_main(device = 'cuda', dataname = 'adult', num_epochs=5):\n",
        "    # device = args.device\n",
        "\n",
        "    train_z, _, _, ckpt_path, _ = get_input_train(dataname)\n",
        "\n",
        "    print(ckpt_path)\n",
        "\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        os.makedirs(ckpt_path)\n",
        "\n",
        "    in_dim = train_z.shape[1]\n",
        "\n",
        "    mean, std = train_z.mean(0), train_z.std(0)\n",
        "\n",
        "    train_z = (train_z - mean) / 2\n",
        "    train_data = train_z\n",
        "\n",
        "\n",
        "    batch_size = 4096\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = True,\n",
        "        num_workers = 4,\n",
        "    )\n",
        "\n",
        "    # num_epochs = 500 + 1\n",
        "\n",
        "    denoise_fn = MLPDiffusion(in_dim, 1024).to(device)\n",
        "    print(denoise_fn)\n",
        "\n",
        "    num_params = sum(p.numel() for p in denoise_fn.parameters())\n",
        "    print(\"the number of parameters\", num_params)\n",
        "\n",
        "    model = Model(denoise_fn = denoise_fn, hid_dim = train_z.shape[1]).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=20, verbose=True)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    patience = 0\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # pbar = tqdm(train_loader, total=len(train_loader))\n",
        "        # pbar.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        batch_loss = 0.0\n",
        "        len_input = 0\n",
        "        for batch in train_loader:#pbar\n",
        "            inputs = batch.float().to(device)\n",
        "            loss = model(inputs)\n",
        "\n",
        "            loss = loss.mean()\n",
        "\n",
        "            batch_loss += loss.item() * len(inputs)\n",
        "            len_input += len(inputs)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # pbar.set_postfix({\"Loss\": loss.item()})\n",
        "\n",
        "        curr_loss = batch_loss/len_input\n",
        "        scheduler.step(curr_loss)\n",
        "\n",
        "        if curr_loss < best_loss:\n",
        "            best_loss = curr_loss\n",
        "            patience = 0\n",
        "            torch.save(model.state_dict(), f'{ckpt_path}/model.pt')\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience == 500:\n",
        "                print('Early stopping')\n",
        "                break\n",
        "\n",
        "        if epoch % 500 == 0:\n",
        "            torch.save(model.state_dict(), f'{ckpt_path}/model_{epoch}.pt')\n",
        "\n",
        "    end_time = time.time()\n",
        "    print('Time: ', end_time - start_time)\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "\n",
        "#     parser = argparse.ArgumentParser(description='Training of TabSyn')\n",
        "\n",
        "#     parser.add_argument('--dataname', type=str, default='adult', help='Name of dataset.')\n",
        "#     parser.add_argument('--gpu', type=int, default=0, help='GPU index.')\n",
        "\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     # check cuda\n",
        "#     if args.gpu != -1 and torch.cuda.is_available():\n",
        "#         args.device = f'cuda:{args.gpu}'\n",
        "#     else:\n",
        "#         args.device = 'cpu'"
      ],
      "metadata": {
        "id": "FfaPfe1QKRy5"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_fn = execute_function('tabsyn', 'train', 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k_k86lEnbs2",
        "outputId": "8f4f3425-d7bc-41e0-e15a-dd7c595d27a0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model\n",
            "MLPDiffusion(\n",
            "  (proj): Linear(in_features=60, out_features=1024, bias=True)\n",
            "  (mlp): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "    (1): SiLU()\n",
            "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "    (3): SiLU()\n",
            "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "    (5): SiLU()\n",
            "    (6): Linear(in_features=1024, out_features=60, bias=True)\n",
            "  )\n",
            "  (map_noise): PositionalEmbedding()\n",
            "  (time_embed): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    (1): SiLU()\n",
            "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "  )\n",
            ")\n",
            "the number of parameters 10616892\n",
            "Time:  930.081550359726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_generate(dataname):\n",
        "    # dataname = args.dataname\n",
        "\n",
        "    curr_dir = 'tabsyn'#os.path.dirname(os.path.abspath(__file__))\n",
        "    dataset_dir = f'data/{dataname}'\n",
        "    ckpt_dir = f'model'\n",
        "\n",
        "    with open(f'{dataset_dir}/info.json', 'r') as f:\n",
        "        info = json.load(f)\n",
        "\n",
        "    task_type = info['task_type']\n",
        "\n",
        "\n",
        "    # ckpt_dir = f'{curr_dir}/ckpt/{dataname}'\n",
        "\n",
        "    _, _, categories, d_numerical, num_inverse, cat_inverse = preprocess(dataset_dir, task_type = task_type, inverse = True)\n",
        "\n",
        "    embedding_save_path = 'model/train_z.npy'#f'{curr_dir}/vae/ckpt/{dataname}/train_z.npy'\n",
        "    train_z = torch.tensor(np.load(embedding_save_path)).float()\n",
        "\n",
        "    train_z = train_z[:, 1:, :]\n",
        "\n",
        "    B, num_tokens, token_dim = train_z.size()\n",
        "    in_dim = num_tokens * token_dim\n",
        "\n",
        "    train_z = train_z.view(B, in_dim)\n",
        "    pre_decoder = Decoder_model(2, d_numerical, categories, 4, n_head = 1, factor = 32)\n",
        "\n",
        "    decoder_save_path = 'model/decoder.pt'#f'{curr_dir}/vae/ckpt/{dataname}/decoder.pt'\n",
        "    pre_decoder.load_state_dict(torch.load(decoder_save_path))\n",
        "\n",
        "    info['pre_decoder'] = pre_decoder\n",
        "    info['token_dim'] = token_dim\n",
        "\n",
        "    return train_z, curr_dir, dataset_dir, ckpt_dir, info, num_inverse, cat_inverse"
      ],
      "metadata": {
        "id": "SNsVkkInZLCY"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import argparse\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "# from tabsyn.model import MLPDiffusion, Model\n",
        "# from tabsyn.latent_utils import get_input_generate, recover_data, split_num_cat_target\n",
        "# from tabsyn.diffusion_utils import sample\n",
        "randn_like=torch.randn_like\n",
        "\n",
        "SIGMA_MIN=0.002\n",
        "SIGMA_MAX=80\n",
        "rho=7\n",
        "S_churn= 1\n",
        "S_min=0\n",
        "S_max=float('inf')\n",
        "S_noise=1\n",
        "@torch.no_grad()\n",
        "def split_num_cat_target(syn_data, info, num_inverse, cat_inverse, device):\n",
        "    task_type = info['task_type']\n",
        "\n",
        "    num_col_idx = info['num_col_idx']\n",
        "    cat_col_idx = info['cat_col_idx']\n",
        "    target_col_idx = info['target_col_idx']\n",
        "\n",
        "    n_num_feat = len(num_col_idx)\n",
        "    n_cat_feat = len(cat_col_idx)\n",
        "\n",
        "    if task_type == 'regression':\n",
        "        n_num_feat += len(target_col_idx)\n",
        "    else:\n",
        "        n_cat_feat += len(target_col_idx)\n",
        "\n",
        "\n",
        "    pre_decoder = info['pre_decoder']\n",
        "    token_dim = info['token_dim']\n",
        "\n",
        "    syn_data = syn_data.reshape(syn_data.shape[0], -1, token_dim)\n",
        "\n",
        "    norm_input = pre_decoder(torch.tensor(syn_data))\n",
        "    x_hat_num, x_hat_cat = norm_input\n",
        "\n",
        "    syn_cat = []\n",
        "    for pred in x_hat_cat:\n",
        "        syn_cat.append(pred.argmax(dim = -1))\n",
        "\n",
        "    syn_num = x_hat_num.cpu().numpy()\n",
        "    syn_cat = torch.stack(syn_cat).t().cpu().numpy()\n",
        "\n",
        "    syn_num = num_inverse(syn_num)\n",
        "    syn_cat = cat_inverse(syn_cat)\n",
        "\n",
        "    if info['task_type'] == 'regression':\n",
        "        syn_target = syn_num[:, :len(target_col_idx)]\n",
        "        syn_num = syn_num[:, len(target_col_idx):]\n",
        "\n",
        "    else:\n",
        "        print(syn_cat.shape)\n",
        "        syn_target = syn_cat[:, :len(target_col_idx)]\n",
        "        syn_cat = syn_cat[:, len(target_col_idx):]\n",
        "\n",
        "    return syn_num, syn_cat, syn_target\n",
        "\n",
        "def recover_data(syn_num, syn_cat, syn_target, info):\n",
        "\n",
        "    num_col_idx = info['num_col_idx']\n",
        "    cat_col_idx = info['cat_col_idx']\n",
        "    target_col_idx = info['target_col_idx']\n",
        "\n",
        "\n",
        "    idx_mapping = info['idx_mapping']\n",
        "    idx_mapping = {int(key): value for key, value in idx_mapping.items()}\n",
        "\n",
        "    syn_df = pd.DataFrame()\n",
        "\n",
        "    if info['task_type'] == 'regression':\n",
        "        for i in range(len(num_col_idx) + len(cat_col_idx) + len(target_col_idx)):\n",
        "            if i in set(num_col_idx):\n",
        "                syn_df[i] = syn_num[:, idx_mapping[i]]\n",
        "            elif i in set(cat_col_idx):\n",
        "                syn_df[i] = syn_cat[:, idx_mapping[i] - len(num_col_idx)]\n",
        "            else:\n",
        "                syn_df[i] = syn_target[:, idx_mapping[i] - len(num_col_idx) - len(cat_col_idx)]\n",
        "\n",
        "\n",
        "    else:\n",
        "        for i in range(len(num_col_idx) + len(cat_col_idx) + len(target_col_idx)):\n",
        "            if i in set(num_col_idx):\n",
        "                syn_df[i] = syn_num[:, idx_mapping[i]]\n",
        "            elif i in set(cat_col_idx):\n",
        "                syn_df[i] = syn_cat[:, idx_mapping[i] - len(num_col_idx)]\n",
        "            else:\n",
        "                syn_df[i] = syn_target[:, idx_mapping[i] - len(num_col_idx) - len(cat_col_idx)]\n",
        "\n",
        "    return syn_df\n",
        "def sample_step(net, num_steps, i, t_cur, t_next, x_next):\n",
        "\n",
        "    x_cur = x_next\n",
        "    # Increase noise temporarily.\n",
        "    gamma = min(S_churn / num_steps, np.sqrt(2) - 1) if S_min <= t_cur <= S_max else 0\n",
        "    t_hat = net.round_sigma(t_cur + gamma * t_cur)\n",
        "    x_hat = x_cur + (t_hat ** 2 - t_cur ** 2).sqrt() * S_noise * randn_like(x_cur)\n",
        "    # Euler step.\n",
        "\n",
        "    denoised = net(x_hat, t_hat).to(torch.float32)\n",
        "    d_cur = (x_hat - denoised) / t_hat\n",
        "    x_next = x_hat + (t_next - t_hat) * d_cur\n",
        "\n",
        "    # Apply 2nd order correction.\n",
        "    if i < num_steps - 1:\n",
        "        denoised = net(x_next, t_next).to(torch.float32)\n",
        "        d_prime = (x_next - denoised) / t_next\n",
        "        x_next = x_hat + (t_next - t_hat) * (0.5 * d_cur + 0.5 * d_prime)\n",
        "\n",
        "    return x_next\n",
        "\n",
        "def sample(net, num_samples, dim, num_steps = 50, device = 'cuda:0'):\n",
        "    latents = torch.randn([num_samples, dim], device=device)\n",
        "\n",
        "    step_indices = torch.arange(num_steps, dtype=torch.float32, device=latents.device)\n",
        "\n",
        "    sigma_min = max(SIGMA_MIN, net.sigma_min)\n",
        "    sigma_max = min(SIGMA_MAX, net.sigma_max)\n",
        "\n",
        "    t_steps = (sigma_max ** (1 / rho) + step_indices / (num_steps - 1) * (\n",
        "                sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n",
        "    t_steps = torch.cat([net.round_sigma(t_steps), torch.zeros_like(t_steps[:1])])\n",
        "\n",
        "    x_next = latents.to(torch.float32) * t_steps[0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):\n",
        "            x_next = sample_step(net, num_steps, i, t_cur, t_next, x_next)\n",
        "\n",
        "    return x_next\n",
        "\n",
        "def sample_main(dataname = 'adult', device = 'cuda', steps = None):\n",
        "    # dataname = args.dataname\n",
        "    # device = args.device\n",
        "    # steps = args.steps\n",
        "    save_path = 'sample/sample.csv'\n",
        "\n",
        "    train_z, _, _, ckpt_path, info, num_inverse, cat_inverse = get_input_generate(dataname)\n",
        "    in_dim = train_z.shape[1]\n",
        "\n",
        "    mean = train_z.mean(0)\n",
        "\n",
        "    denoise_fn = MLPDiffusion(in_dim, 1024).to(device)\n",
        "\n",
        "    model = Model(denoise_fn = denoise_fn, hid_dim = train_z.shape[1]).to(device)\n",
        "\n",
        "    model.load_state_dict(torch.load(f'{ckpt_path}/model.pt'))\n",
        "\n",
        "    '''\n",
        "        Generating samples\n",
        "    '''\n",
        "    start_time = time.time()\n",
        "\n",
        "    num_samples = train_z.shape[0]\n",
        "    sample_dim = in_dim\n",
        "\n",
        "    x_next = sample(model.denoise_fn_D, num_samples, sample_dim)\n",
        "    x_next = x_next * 2 + mean.to(device)\n",
        "\n",
        "    syn_data = x_next.float().cpu().numpy()\n",
        "    syn_num, syn_cat, syn_target = split_num_cat_target(syn_data, info, num_inverse, cat_inverse, device)\n",
        "\n",
        "    syn_df = recover_data(syn_num, syn_cat, syn_target, info)\n",
        "\n",
        "    idx_name_mapping = info['idx_name_mapping']\n",
        "    idx_name_mapping = {int(key): value for key, value in idx_name_mapping.items()}\n",
        "\n",
        "    syn_df.rename(columns = idx_name_mapping, inplace=True)\n",
        "    syn_df.to_csv(save_path, index = False)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print('Time:', end_time - start_time)\n",
        "\n",
        "    print('Saving sampled data to {}'.format(save_path))\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "\n",
        "#     parser = argparse.ArgumentParser(description='Generation')\n",
        "\n",
        "#     parser.add_argument('--dataname', type=str, default='adult', help='Name of dataset.')\n",
        "#     parser.add_argument('--gpu', type=int, default=0, help='GPU index.')\n",
        "#     parser.add_argument('--epoch', type=int, default=None, help='Epoch.')\n",
        "#     parser.add_argument('--steps', type=int, default=None, help='Number of function evaluations.')\n",
        "\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     # check cuda\n",
        "#     if args.gpu != -1 and torch.cuda.is_available():\n",
        "#         args.device = f'cuda:{args.gpu}'\n",
        "#     else:\n",
        "#         args.device = 'cpu'"
      ],
      "metadata": {
        "id": "n7adO4gdCxOf"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_main()"
      ],
      "metadata": {
        "id": "KhnGyx3gICs6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e341b7f7-b883-48b9-dec3-36af192b27bc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No NaNs in numerical features, skipping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-27963ec472e2>:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pre_decoder.load_state_dict(torch.load(decoder_save_path))\n",
            "<ipython-input-43-5581e0579ad8>:153: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(f'{ckpt_path}/model.pt'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32561, 9)\n",
            "Time: 17.42240023612976\n",
            "Saving sampled data to sample/sample.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZGVopmg3a9lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Сравнение на классификаторе"
      ],
      "metadata": {
        "id": "HqC3HUoUs7ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python eval/eval_mle.py --dataname [NAME_OF_DATASET] --model [METHOD_NAME] --path [PATH_TO_SYNTHETIC_DATA]"
      ],
      "metadata": {
        "id": "WP06bXPIyMyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install prdc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLlJ2x5v0uQs",
        "outputId": "0c6d101a-461a-4ef4-8f21-055216e8264b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting prdc\n",
            "  Downloading prdc-0.2-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from prdc) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from prdc) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from prdc) (1.13.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from prdc) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->prdc) (3.5.0)\n",
            "Downloading prdc-0.2-py3-none-any.whl (6.0 kB)\n",
            "Installing collected packages: prdc\n",
            "Successfully installed prdc-0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.metrics import explained_variance_score, mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.utils._testing import ignore_warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import logging\n",
        "from prdc import compute_prdc\n",
        "from tqdm import tqdm\n",
        "\n",
        "CATEGORICAL = \"categorical\"\n",
        "CONTINUOUS = \"continuous\"\n",
        "\n",
        "_MODELS = {\n",
        "    'binclass': [\n",
        "        {\n",
        "            'class': XGBClassifier, # 36\n",
        "            'kwargs': {\n",
        "                 'n_estimators': [10, 50, 100],\n",
        "                 'min_child_weight': [1, 10],\n",
        "                 'max_depth': [5, 10, 20],\n",
        "                 'gamma': [0.0, 1.0],\n",
        "                 'objective': ['binary:logistic'],\n",
        "                 'nthread': [-1],\n",
        "                 'tree_method': ['gpu_hist']\n",
        "            },\n",
        "        }\n",
        "\n",
        "    ],\n",
        "    # 'multiclass': [\n",
        "    #     {\n",
        "    #         'class': XGBClassifier, # 36\n",
        "    #         'kwargs': {\n",
        "    #              'n_estimators': [10, 50, 100],\n",
        "    #              'min_child_weight': [1, 10],\n",
        "    #              'max_depth': [5, 10, 20],\n",
        "    #              'gamma': [0.0, 1.0],\n",
        "    #              'objective': ['binary:logistic'],\n",
        "    #              'nthread': [-1],\n",
        "    #              'tree_method': ['gpu_hist']\n",
        "    #         }\n",
        "    #     }\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "def feat_transform(data, info, label_encoder = None, encoders = None, cmax = None, cmin = None):\n",
        "    num_col_idx = info['num_col_idx']\n",
        "    cat_col_idx = info['cat_col_idx']\n",
        "    target_col_idx = info['target_col_idx']\n",
        "\n",
        "    num_cols = len(num_col_idx + cat_col_idx + target_col_idx)\n",
        "    features = []\n",
        "\n",
        "    if not encoders:\n",
        "        encoders = dict()\n",
        "    for idx in range(num_cols):\n",
        "        col = data[:, idx]\n",
        "\n",
        "        if idx in target_col_idx:\n",
        "\n",
        "            if info['task_type'] != 'regression':\n",
        "\n",
        "                if not label_encoder:\n",
        "                    label_encoder = LabelEncoder()\n",
        "                    label_encoder.fit(col)\n",
        "\n",
        "                encoded_labels = label_encoder.transform(col)\n",
        "                labels = encoded_labels\n",
        "            else:\n",
        "                col = col.astype(np.float32)\n",
        "                labels = col.astype(np.float32)\n",
        "\n",
        "            continue\n",
        "\n",
        "        if idx in num_col_idx:\n",
        "            col = col.astype(np.float32)\n",
        "\n",
        "            if not cmin:\n",
        "                cmin = col.min()\n",
        "\n",
        "            if not cmax:\n",
        "                cmax = col.max()\n",
        "\n",
        "            if cmin >= 0 and cmax >= 1e3:\n",
        "                feature = np.log(np.maximum(col, 1e-2))\n",
        "\n",
        "            else:\n",
        "                feature = (col - cmin) / (cmax - cmin) * 5\n",
        "\n",
        "        elif idx in cat_col_idx:\n",
        "            encoder = encoders.get(idx)\n",
        "            col = col.reshape(-1, 1)\n",
        "            if encoder:\n",
        "                feature = encoder.transform(col)\n",
        "            else:\n",
        "                encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "                encoders[idx] = encoder\n",
        "                feature = encoder.fit_transform(col)\n",
        "\n",
        "\n",
        "        features.append(feature)\n",
        "    features = np.column_stack(features)\n",
        "    return features, labels, label_encoder, encoders, cmax, cmin\n",
        "\n",
        "\n",
        "def prepare_ml_problem(train, test, info):\n",
        "    # test_X, test_y, label_encoder, encoders = feat_transform(test, info)\n",
        "    # train_X, train_y, _, _ = feat_transform(train, info, label_encoder, encoders)\n",
        "\n",
        "    train_X, train_y, label_encoder, encoders, cmax, cmin = feat_transform(train, info)\n",
        "    test_X, test_y, _, _ , _, _ = feat_transform(test, info, label_encoder, encoders, cmax, cmin)\n",
        "\n",
        "    total_train_num = train_X.shape[0]\n",
        "    val_num = int(total_train_num / 9)\n",
        "\n",
        "    total_train_idx = np.arange(total_train_num)\n",
        "    np.random.shuffle(total_train_idx)\n",
        "    train_idx = total_train_idx[val_num:]\n",
        "    val_idx = total_train_idx[:val_num]\n",
        "\n",
        "\n",
        "\n",
        "    # val_X, val_y = train_X[val_idx], train_y[val_idx]\n",
        "    # train_X, train_y = train_X[train_idx], train_y[train_idx]\n",
        "\n",
        "    # model = _MODELS[info['task_type']]\n",
        "\n",
        "    # return train_X, train_y, train_X, train_y, test_X, test_y, model\n",
        "\n",
        "\n",
        "\n",
        "    val_X, val_y = train_X[val_idx], train_y[val_idx]\n",
        "    train_X, train_y = train_X[train_idx], train_y[train_idx]\n",
        "\n",
        "    model = _MODELS[info['task_type']]\n",
        "\n",
        "    return train_X, train_y, val_X, val_y, test_X, test_y, model\n",
        "\n",
        "class FeatureMaker:\n",
        "\n",
        "    def __init__(self, metadata, label_column='label', label_type='int', sample=50000):\n",
        "        self.columns = metadata['columns']\n",
        "        self.label_column = label_column\n",
        "        self.label_type = label_type\n",
        "        self.sample = sample\n",
        "        self.encoders = dict()\n",
        "\n",
        "    def make_features(self, data):\n",
        "        data = data.copy()\n",
        "        np.random.shuffle(data)\n",
        "        data = data[:self.sample]\n",
        "\n",
        "        features = []\n",
        "        labels = []\n",
        "\n",
        "        for index, cinfo in enumerate(self.columns):\n",
        "            col = data[:, index]\n",
        "            if cinfo['name'] == self.label_column:\n",
        "                if self.label_type == 'int':\n",
        "                    labels = col.astype(int)\n",
        "                elif self.label_type == 'float':\n",
        "                    labels = col.astype(float)\n",
        "                else:\n",
        "                    assert 0, 'unkown label type'\n",
        "                continue\n",
        "\n",
        "            if cinfo['type'] == CONTINUOUS:\n",
        "                cmin = cinfo['min']\n",
        "                cmax = cinfo['max']\n",
        "                if cmin >= 0 and cmax >= 1e3:\n",
        "                    feature = np.log(np.maximum(col, 1e-2))\n",
        "\n",
        "                else:\n",
        "                    feature = (col - cmin) / (cmax - cmin) * 5\n",
        "\n",
        "            else:\n",
        "                if cinfo['size'] <= 2:\n",
        "                    feature = col\n",
        "\n",
        "                else:\n",
        "                    encoder = self.encoders.get(index)\n",
        "                    col = col.reshape(-1, 1)\n",
        "                    if encoder:\n",
        "                        feature = encoder.transform(col)\n",
        "                    else:\n",
        "                        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "                        self.encoders[index] = encoder\n",
        "                        feature = encoder.fit_transform(col)\n",
        "\n",
        "            features.append(feature)\n",
        "\n",
        "        features = np.column_stack(features)\n",
        "\n",
        "        return features, labels\n",
        "\n",
        "\n",
        "def _prepare_ml_problem(train, val, test, metadata, eval):\n",
        "    fm = FeatureMaker(metadata)\n",
        "    x_trains, y_trains = [], []\n",
        "\n",
        "    for i in train:\n",
        "        x_train, y_train = fm.make_features(i)\n",
        "        x_trains.append(x_train)\n",
        "        y_trains.append(y_train)\n",
        "\n",
        "    x_val, y_val = fm.make_features(val)\n",
        "    if eval is None:\n",
        "        x_test = None\n",
        "        y_test = None\n",
        "    else:\n",
        "        x_test, y_test = fm.make_features(test)\n",
        "    model = _MODELS[metadata['problem_type']]\n",
        "\n",
        "    return x_trains, y_trains, x_val, y_val, x_test, y_test, model\n",
        "\n",
        "\n",
        "def _weighted_f1(y_test, pred):\n",
        "    report = classification_report(y_test, pred, output_dict=True)\n",
        "    classes = list(report.keys())[:-3]\n",
        "    proportion = [  report[i]['support'] / len(y_test) for i in classes]\n",
        "    weighted_f1 = np.sum(list(map(lambda i, prop: report[i]['f1-score']* (1-prop)/(len(classes)-1), classes, proportion)))\n",
        "    return weighted_f1\n",
        "\n",
        "\n",
        "@ignore_warnings(category=ConvergenceWarning)\n",
        "def _evaluate_multi_classification(train, test, info):\n",
        "    x_trains, y_trains, x_valid, y_valid, x_test, y_test, classifiers = prepare_ml_problem(train, test, info)\n",
        "    best_f1_scores = []\n",
        "    unique_labels = np.unique(y_trains)\n",
        "\n",
        "\n",
        "    best_f1_scores = []\n",
        "    best_weighted_scores = []\n",
        "    best_auroc_scores = []\n",
        "    best_acc_scores = []\n",
        "    best_avg_scores = []\n",
        "\n",
        "    for model_spec in classifiers:\n",
        "        model_class = model_spec['class']\n",
        "        model_kwargs = model_spec.get('kwargs', dict())\n",
        "        model_repr = model_class.__name__\n",
        "\n",
        "        unique_labels = np.unique(y_trains)\n",
        "\n",
        "        param_set = list(ParameterGrid(model_kwargs))\n",
        "\n",
        "        results = []\n",
        "        for param in tqdm(param_set):\n",
        "            model = model_class(**param)\n",
        "\n",
        "            try:\n",
        "                model.fit(x_trains, y_trains)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            if len(unique_labels) != len(np.unique(y_valid)):\n",
        "                pred = [unique_labels[0]] * len(x_valid)\n",
        "                pred_prob = np.array([1.] * len(x_valid))\n",
        "            else:\n",
        "                pred = model.predict(x_valid)\n",
        "                pred_prob = model.predict_proba(x_valid)\n",
        "\n",
        "            macro_f1 = f1_score(y_valid, pred, average='macro')\n",
        "            weighted_f1 = _weighted_f1(y_valid, pred)\n",
        "            acc = accuracy_score(y_valid, pred)\n",
        "\n",
        "            # 3. auroc\n",
        "    #         size = [a[\"size\"] for a in metadata[\"columns\"] if a[\"name\"] == \"label\"][0]\n",
        "            size = len(set(unique_labels))\n",
        "            rest_label = set(range(size)) - set(unique_labels)\n",
        "            tmp = []\n",
        "            j = 0\n",
        "            for i in range(size):\n",
        "                if i in rest_label:\n",
        "                    tmp.append(np.array([0] * y_valid.shape[0])[:,np.newaxis])\n",
        "                else:\n",
        "                    try:\n",
        "                        tmp.append(pred_prob[:,[j]])\n",
        "                    except:\n",
        "                        tmp.append(pred_prob[:, np.newaxis])\n",
        "                    j += 1\n",
        "\n",
        "            roc_auc = roc_auc_score(np.eye(size)[y_valid], np.hstack(tmp), multi_class='ovr')\n",
        "\n",
        "            results.append(\n",
        "                {\n",
        "                    \"name\": model_repr,\n",
        "                    \"param\": param,\n",
        "                    \"macro_f1\": macro_f1,\n",
        "                    \"weighted_f1\": weighted_f1,\n",
        "                    \"roc_auc\": roc_auc,\n",
        "                    \"accuracy\": acc\n",
        "                }\n",
        "            )\n",
        "\n",
        "        results = pd.DataFrame(results)\n",
        "        results['avg'] = results.loc[:, ['macro_f1', 'weighted_f1', 'roc_auc']].mean(axis=1)\n",
        "        best_f1_param = results.param[results.macro_f1.idxmax()]\n",
        "        best_weighted_param = results.param[results.weighted_f1.idxmax()]\n",
        "        best_auroc_param = results.param[results.roc_auc.idxmax()]\n",
        "        best_acc_param = results.param[results.accuracy.idxmax()]\n",
        "        best_avg_param = results.param[results.avg.idxmax()]\n",
        "\n",
        "\n",
        "        # test the best model\n",
        "        results = pd.DataFrame(results)\n",
        "        # best_param = results.param[results.macro_f1.idxmax()]\n",
        "\n",
        "        def _calc(best_model):\n",
        "            best_scores = []\n",
        "\n",
        "            x_train = x_trains\n",
        "            y_train = y_trains\n",
        "\n",
        "            try:\n",
        "                best_model.fit(x_train, y_train)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            if len(unique_labels) != len(np.unique(y_test)):\n",
        "                pred = [unique_labels[0]] * len(x_test)\n",
        "                pred_prob = np.array([1.] * len(x_test))\n",
        "            else:\n",
        "                pred = best_model.predict(x_test)\n",
        "                pred_prob = best_model.predict_proba(x_test)\n",
        "\n",
        "            macro_f1 = f1_score(y_test, pred, average='macro')\n",
        "            weighted_f1 = _weighted_f1(y_test, pred)\n",
        "            acc = accuracy_score(y_test, pred)\n",
        "\n",
        "            # 3. auroc\n",
        "            size = len(set(unique_labels))\n",
        "            rest_label = set(range(size)) - set(unique_labels)\n",
        "            tmp = []\n",
        "            j = 0\n",
        "            for i in range(size):\n",
        "                if i in rest_label:\n",
        "                    tmp.append(np.array([0] * y_test.shape[0])[:,np.newaxis])\n",
        "                else:\n",
        "                    try:\n",
        "                        tmp.append(pred_prob[:,[j]])\n",
        "                    except:\n",
        "                        tmp.append(pred_prob[:, np.newaxis])\n",
        "                    j += 1\n",
        "            roc_auc = roc_auc_score(np.eye(size)[y_test], np.hstack(tmp), multi_class='ovr')\n",
        "\n",
        "            best_scores.append(\n",
        "                {\n",
        "                    \"name\": model_repr,\n",
        "                    \"macro_f1\": macro_f1,\n",
        "                    \"weighted_f1\": weighted_f1,\n",
        "                    \"roc_auc\": roc_auc,\n",
        "                    \"accuracy\": acc\n",
        "                }\n",
        "            )\n",
        "            return pd.DataFrame(best_scores)\n",
        "\n",
        "        def _df(dataframe):\n",
        "            return {\n",
        "                \"name\": model_repr,\n",
        "                \"macro_f1\": dataframe.macro_f1.values[0],\n",
        "                \"roc_auc\": dataframe.roc_auc.values[0],\n",
        "                \"weighted_f1\": dataframe.weighted_f1.values[0],\n",
        "                \"accuracy\": dataframe.accuracy.values[0],\n",
        "            }\n",
        "\n",
        "        best_f1_scores.append(_df(_calc(model_class(**best_f1_param))))\n",
        "        best_weighted_scores.append(_df(_calc(model_class(**best_weighted_param))))\n",
        "        best_auroc_scores.append(_df(_calc(model_class(**best_auroc_param))))\n",
        "        best_acc_scores.append(_df(_calc(model_class(**best_acc_param))))\n",
        "        best_avg_scores.append(_df(_calc(model_class(**best_avg_param))))\n",
        "\n",
        "    return best_f1_scores, best_weighted_scores, best_auroc_scores, best_acc_scores, best_avg_scores\n",
        "\n",
        "@ignore_warnings(category=ConvergenceWarning)\n",
        "def _evaluate_binary_classification(train, test, info):\n",
        "    x_trains, y_trains, x_valid, y_valid, x_test, y_test, classifiers = prepare_ml_problem(train, test, info)\n",
        "\n",
        "    unique_labels = np.unique(y_trains)\n",
        "\n",
        "    best_f1_scores = []\n",
        "    best_weighted_scores = []\n",
        "    best_auroc_scores = []\n",
        "    best_acc_scores = []\n",
        "    best_avg_scores = []\n",
        "\n",
        "    for model_spec in classifiers:\n",
        "\n",
        "        model_class = model_spec['class']\n",
        "        model_kwargs = model_spec.get('kwargs', dict())\n",
        "        model_repr = model_class.__name__\n",
        "\n",
        "        unique_labels = np.unique(y_trains)\n",
        "\n",
        "        param_set = list(ParameterGrid(model_kwargs))\n",
        "\n",
        "        results = []\n",
        "        for param in tqdm(param_set):\n",
        "            model = model_class(**param)\n",
        "\n",
        "            try:\n",
        "                model.fit(x_trains, y_trains)\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "            if len(unique_labels) == 1:\n",
        "                pred = [unique_labels[0]] * len(x_valid)\n",
        "                pred_prob = np.array([1.] * len(x_valid))\n",
        "            else:\n",
        "                pred = model.predict(x_valid)\n",
        "                pred_prob = model.predict_proba(x_valid)\n",
        "\n",
        "            binary_f1 = f1_score(y_valid, pred, average='binary')\n",
        "            weighted_f1 = _weighted_f1(y_valid, pred)\n",
        "            acc = accuracy_score(y_valid, pred)\n",
        "            precision = precision_score(y_valid, pred, average='binary')\n",
        "            recall = recall_score(y_valid, pred, average='binary')\n",
        "            macro_f1 = f1_score(y_valid, pred, average='macro')\n",
        "\n",
        "            # auroc\n",
        "            size = 2\n",
        "            rest_label = set(range(size)) - set(unique_labels)\n",
        "            tmp = []\n",
        "            j = 0\n",
        "            for i in range(size):\n",
        "                if i in rest_label:\n",
        "                    tmp.append(np.array([0] * y_valid.shape[0])[:,np.newaxis])\n",
        "                else:\n",
        "                    try:\n",
        "                        tmp.append(pred_prob[:,[j]])\n",
        "                    except:\n",
        "                        tmp.append(pred_prob[:, np.newaxis])\n",
        "                    j += 1\n",
        "            roc_auc = roc_auc_score(np.eye(size)[y_valid], np.hstack(tmp))\n",
        "\n",
        "            results.append(\n",
        "                {\n",
        "                    \"name\": model_repr,\n",
        "                    \"param\": param,\n",
        "                    \"binary_f1\": binary_f1,\n",
        "                    \"weighted_f1\": weighted_f1,\n",
        "                    \"roc_auc\": roc_auc,\n",
        "                    \"accuracy\": acc,\n",
        "                    \"precision\": precision,\n",
        "                    \"recall\": recall,\n",
        "                    \"macro_f1\": macro_f1\n",
        "                }\n",
        "            )\n",
        "\n",
        "\n",
        "        # test the best model\n",
        "        results = pd.DataFrame(results)\n",
        "        results['avg'] = results.loc[:, ['binary_f1', 'weighted_f1', 'roc_auc']].mean(axis=1)\n",
        "        best_f1_param = results.param[results.binary_f1.idxmax()]\n",
        "        best_weighted_param = results.param[results.weighted_f1.idxmax()]\n",
        "        best_auroc_param = results.param[results.roc_auc.idxmax()]\n",
        "        best_acc_param = results.param[results.accuracy.idxmax()]\n",
        "        best_avg_param = results.param[results.avg.idxmax()]\n",
        "\n",
        "\n",
        "        def _calc(best_model):\n",
        "            best_scores = []\n",
        "\n",
        "            best_model.fit(x_trains, y_trains)\n",
        "\n",
        "            if len(unique_labels) == 1:\n",
        "                pred = [unique_labels[0]] * len(x_test)\n",
        "                pred_prob = np.array([1.] * len(x_test))\n",
        "            else:\n",
        "                pred = best_model.predict(x_test)\n",
        "                pred_prob = best_model.predict_proba(x_test)\n",
        "\n",
        "            binary_f1 = f1_score(y_test, pred, average='binary')\n",
        "            weighted_f1 = _weighted_f1(y_test, pred)\n",
        "            acc = accuracy_score(y_test, pred)\n",
        "            precision = precision_score(y_test, pred, average='binary')\n",
        "            recall = recall_score(y_test, pred, average='binary')\n",
        "            macro_f1 = f1_score(y_test, pred, average='macro')\n",
        "\n",
        "            # auroc\n",
        "            size = 2\n",
        "            rest_label = set(range(size)) - set(unique_labels)\n",
        "            tmp = []\n",
        "            j = 0\n",
        "            for i in range(size):\n",
        "                if i in rest_label:\n",
        "                    tmp.append(np.array([0] * y_test.shape[0])[:,np.newaxis])\n",
        "                else:\n",
        "                    try:\n",
        "                        tmp.append(pred_prob[:,[j]])\n",
        "                    except:\n",
        "                        tmp.append(pred_prob[:, np.newaxis])\n",
        "                    j += 1\n",
        "            try:\n",
        "                roc_auc = roc_auc_score(np.eye(size)[y_test], np.hstack(tmp))\n",
        "            except ValueError:\n",
        "                tmp[1] = tmp[1].reshape(20000, 1)\n",
        "                roc_auc = roc_auc_score(np.eye(size)[y_test], np.hstack(tmp))\n",
        "\n",
        "            best_scores.append(\n",
        "                {\n",
        "                    \"name\": model_repr,\n",
        "                    # \"param\": param,\n",
        "                    \"binary_f1\": binary_f1,\n",
        "                    \"weighted_f1\": weighted_f1,\n",
        "                    \"roc_auc\": roc_auc,\n",
        "                    \"accuracy\": acc,\n",
        "                    \"precision\": precision,\n",
        "                    \"recall\": recall,\n",
        "                    \"macro_f1\": macro_f1\n",
        "                }\n",
        "            )\n",
        "\n",
        "            return pd.DataFrame(best_scores)\n",
        "        def _df(dataframe):\n",
        "            return {\n",
        "                \"name\": model_repr,\n",
        "                \"binary_f1\": dataframe.binary_f1.values[0],\n",
        "                \"roc_auc\": dataframe.roc_auc.values[0],\n",
        "                \"weighted_f1\": dataframe.weighted_f1.values[0],\n",
        "                \"accuracy\": dataframe.accuracy.values[0],\n",
        "            }\n",
        "\n",
        "        best_f1_scores.append(_df(_calc(model_class(**best_f1_param))))\n",
        "        best_weighted_scores.append(_df(_calc(model_class(**best_weighted_param))))\n",
        "        best_auroc_scores.append(_df(_calc(model_class(**best_auroc_param))))\n",
        "        best_acc_scores.append(_df(_calc(model_class(**best_acc_param))))\n",
        "        best_avg_scores.append(_df(_calc(model_class(**best_avg_param))))\n",
        "\n",
        "    return best_f1_scores, best_weighted_scores, best_auroc_scores, best_acc_scores, best_avg_scores\n",
        "\n",
        "@ignore_warnings(category=ConvergenceWarning)\n",
        "def _evaluate_regression(train, test, info):\n",
        "\n",
        "    x_trains, y_trains, x_valid, y_valid, x_test, y_test, regressors = prepare_ml_problem(train, test, info)\n",
        "\n",
        "\n",
        "    best_r2_scores = []\n",
        "    best_ev_scores = []\n",
        "    best_mae_scores = []\n",
        "    best_rmse_scores = []\n",
        "    best_avg_scores = []\n",
        "\n",
        "    y_trains = np.log(np.clip(y_trains, 1, 20000))\n",
        "    y_test = np.log(np.clip(y_test, 1, 20000))\n",
        "\n",
        "    for model_spec in regressors:\n",
        "        model_class = model_spec['class']\n",
        "        model_kwargs = model_spec.get('kwargs', dict())\n",
        "        model_repr = model_class.__name__\n",
        "\n",
        "        param_set = list(ParameterGrid(model_kwargs))\n",
        "\n",
        "        results = []\n",
        "        for param in tqdm(param_set):\n",
        "            model = model_class(**param)\n",
        "            model.fit(x_trains, y_trains)\n",
        "            pred = model.predict(x_valid)\n",
        "\n",
        "            r2 = r2_score(y_valid, pred)\n",
        "            explained_variance = explained_variance_score(y_valid, pred)\n",
        "            mean_squared = mean_squared_error(y_valid, pred)\n",
        "            root_mean_squared = mean_squared_error(y_valid, pred, squared=False)\n",
        "            mean_absolute = mean_absolute_error(y_valid, pred)\n",
        "\n",
        "            results.append(\n",
        "                {\n",
        "                    \"name\": model_repr,\n",
        "                    \"param\": param,\n",
        "                    \"r2\": r2,\n",
        "                    \"explained_variance\": explained_variance,\n",
        "                    \"mean_squared\": mean_squared,\n",
        "                    \"mean_absolute\": mean_absolute,\n",
        "                    \"rmse\": root_mean_squared\n",
        "                }\n",
        "            )\n",
        "\n",
        "        results = pd.DataFrame(results)\n",
        "        # results['avg'] = results.loc[:, ['r2', 'rmse']].mean(axis=1)\n",
        "        best_r2_param = results.param[results.r2.idxmax()]\n",
        "        best_ev_param = results.param[results.explained_variance.idxmax()]\n",
        "        best_mae_param = results.param[results.mean_absolute.idxmin()]\n",
        "        best_rmse_param = results.param[results.rmse.idxmin()]\n",
        "        # best_avg_param = results.param[results.avg.idxmax()]\n",
        "\n",
        "        def _calc(best_model):\n",
        "            best_scores = []\n",
        "            x_train, y_train = x_trains, y_trains\n",
        "\n",
        "            best_model.fit(x_train, y_train)\n",
        "            pred = best_model.predict(x_test)\n",
        "\n",
        "            r2 = r2_score(y_test, pred)\n",
        "            explained_variance = explained_variance_score(y_test, pred)\n",
        "            mean_squared = mean_squared_error(y_test, pred)\n",
        "            root_mean_squared = mean_squared_error(y_test, pred, squared=False)\n",
        "            mean_absolute = mean_absolute_error(y_test, pred)\n",
        "\n",
        "            best_scores.append(\n",
        "                {\n",
        "                    \"name\": model_repr,\n",
        "                    \"param\": param,\n",
        "                    \"r2\": r2,\n",
        "                    \"explained_variance\": explained_variance,\n",
        "                    \"mean_squared\": mean_squared,\n",
        "                    \"mean_absolute\": mean_absolute,\n",
        "                    \"rmse\": root_mean_squared\n",
        "                }\n",
        "            )\n",
        "\n",
        "            return pd.DataFrame(best_scores)\n",
        "\n",
        "        def _df(dataframe):\n",
        "            return {\n",
        "                \"name\": model_repr,\n",
        "                \"r2\": dataframe.r2.values[0].astype(float),\n",
        "                \"explained_variance\": dataframe.explained_variance.values[0].astype(float),\n",
        "                \"MAE\": dataframe.mean_absolute.values[0].astype(float),\n",
        "                \"RMSE\": dataframe.rmse.values[0].astype(float),\n",
        "            }\n",
        "\n",
        "        best_r2_scores.append(_df(_calc(model_class(**best_r2_param))))\n",
        "        best_ev_scores.append(_df(_calc(model_class(**best_ev_param))))\n",
        "        best_mae_scores.append(_df(_calc(model_class(**best_mae_param))))\n",
        "        best_rmse_scores.append(_df(_calc(model_class(**best_rmse_param))))\n",
        "\n",
        "    return best_r2_scores, best_rmse_scores\n",
        "\n",
        "@ignore_warnings(category=ConvergenceWarning)\n",
        "def compute_diversity(train, fake):\n",
        "    nearest_k = 5\n",
        "    if train.shape[0] >= 50000:\n",
        "        num = np.random.randint(0, train.shape[0], 50000)\n",
        "        real_features = train[num]\n",
        "        fake_features_lst = [i[num] for i in fake]\n",
        "    else:\n",
        "        num = train.shape[0]\n",
        "        real_features = train[:num]\n",
        "        fake_features_lst = [i[:num] for i in fake]\n",
        "    scores = []\n",
        "    for i, data in enumerate(fake_features_lst):\n",
        "        fake_features = data\n",
        "        metrics = compute_prdc(real_features=real_features,\n",
        "                        fake_features=fake_features,\n",
        "                        nearest_k=nearest_k)\n",
        "        metrics['i'] = i\n",
        "        scores.append(metrics)\n",
        "    return pd.DataFrame(scores).mean(axis=0), pd.DataFrame(scores).std(axis=0)\n",
        "\n",
        "_EVALUATORS = {\n",
        "    'binclass': _evaluate_binary_classification,\n",
        "    'multiclass': _evaluate_multi_classification,\n",
        "    'regression': _evaluate_regression\n",
        "}\n",
        "\n",
        "def get_evaluator(problem_type):\n",
        "    return _EVALUATORS[problem_type]\n",
        "\n",
        "\n",
        "def compute_scores(train, test, synthesized_data, metadata, eval):\n",
        "    a, b, c = _EVALUATORS[metadata['problem_type']](train=train, test=test, fake=synthesized_data, metadata=metadata, eval=eval)\n",
        "    if eval is None:\n",
        "        return a.mean(axis=0), a.std(axis=0), a[['name','param']]\n",
        "    else:\n",
        "        return a.mean(axis=0), a.std(axis=0)\n"
      ],
      "metadata": {
        "id": "SV0Fq2HW0N_F"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--dataname', type=str, default='adult')\n",
        "# parser.add_argument('--model', type=str, default='real')\n",
        "# parser.add_argument('--path', type=str, default = None, help='The file path of the synthetic data')\n",
        "\n",
        "# args = parser.parse_args()\n",
        "\n",
        "\n",
        "dataname = 'adult'\n",
        "model = 'real'#'synt'\n",
        "train_path = f'synthetic/{dataname}/{model}.csv'\n",
        "# train_path = f'sample/sample.csv'\n",
        "test_path = f'synthetic/{dataname}/test.csv'\n",
        "\n",
        "train = pd.read_csv(train_path).to_numpy()\n",
        "test = pd.read_csv(test_path).to_numpy()\n",
        "\n",
        "with open(f'data/{dataname}/info.json', 'r') as f:\n",
        "    info = json.load(f)\n",
        "\n",
        "task_type = info['task_type']\n",
        "\n",
        "evaluator = get_evaluator(task_type)\n",
        "\n",
        "if task_type == 'regression':\n",
        "    best_r2_scores, best_rmse_scores = evaluator(train, test, info)\n",
        "\n",
        "    overall_scores = {}\n",
        "    for score_name in ['best_r2_scores', 'best_rmse_scores']:\n",
        "        overall_scores[score_name] = {}\n",
        "\n",
        "        scores = eval(score_name)\n",
        "        for method in scores:\n",
        "            name = method['name']\n",
        "            method.pop('name')\n",
        "            overall_scores[score_name][name] = method\n",
        "\n",
        "else:\n",
        "    best_f1_scores, best_weighted_scores, best_auroc_scores, best_acc_scores, best_avg_scores = evaluator(train, test, info)\n",
        "\n",
        "    overall_scores = {}\n",
        "    for score_name in ['best_f1_scores', 'best_weighted_scores', 'best_auroc_scores', 'best_acc_scores', 'best_avg_scores']:\n",
        "        overall_scores[score_name] = {}\n",
        "\n",
        "        scores = eval(score_name)\n",
        "        for method in scores:\n",
        "            name = method['name']\n",
        "            method.pop('name')\n",
        "            overall_scores[score_name][name] = method\n",
        "\n",
        "if not os.path.exists(f'eval/mle/{dataname}'):\n",
        "    os.makedirs(f'eval/mle/{dataname}')\n",
        "\n",
        "save_path = f'eval/mle/{dataname}/{model}.json'\n",
        "print('Saving scores to ', save_path)\n",
        "with open(save_path, \"w\") as json_file:\n",
        "    json.dump(overall_scores, json_file, indent=4, separators=(\", \", \": \"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxHL8Fxvz2KM",
        "outputId": "06d8a032-37a4-44ff-91bd-725b0ff6df85"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/36 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "  3%|▎         | 1/36 [00:00<00:08,  4.24it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:43] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "  6%|▌         | 2/36 [00:00<00:10,  3.25it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:43] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "  8%|▊         | 3/36 [00:01<00:11,  2.75it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:43] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 11%|█         | 4/36 [00:01<00:09,  3.30it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 14%|█▍        | 5/36 [00:01<00:09,  3.38it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 17%|█▋        | 6/36 [00:01<00:09,  3.14it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 19%|█▉        | 7/36 [00:02<00:08,  3.33it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:45] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 22%|██▏       | 8/36 [00:02<00:09,  2.93it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:45] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 25%|██▌       | 9/36 [00:03<00:11,  2.28it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 28%|██▊       | 10/36 [00:03<00:09,  2.65it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 31%|███       | 11/36 [00:03<00:09,  2.65it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:47] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 33%|███▎      | 12/36 [00:04<00:10,  2.36it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:47] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 36%|███▌      | 13/36 [00:04<00:09,  2.33it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:47] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:48] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 39%|███▉      | 14/36 [00:05<00:13,  1.65it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:48] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:50] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 42%|████▏     | 15/36 [00:07<00:19,  1.06it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:50] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 44%|████▍     | 16/36 [00:07<00:14,  1.34it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:50] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 47%|████▋     | 17/36 [00:08<00:13,  1.44it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 50%|█████     | 18/36 [00:09<00:13,  1.33it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:52] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 53%|█████▎    | 19/36 [00:09<00:10,  1.70it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:52] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 56%|█████▌    | 20/36 [00:09<00:07,  2.01it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:52] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 58%|█████▊    | 21/36 [00:10<00:06,  2.23it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:52] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 61%|██████    | 22/36 [00:10<00:05,  2.66it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 64%|██████▍   | 23/36 [00:10<00:04,  2.89it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 67%|██████▋   | 24/36 [00:10<00:04,  2.98it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 69%|██████▉   | 25/36 [00:11<00:03,  3.21it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 72%|███████▏  | 26/36 [00:11<00:03,  3.12it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 75%|███████▌  | 27/36 [00:11<00:03,  2.98it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 78%|███████▊  | 28/36 [00:12<00:02,  3.23it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 81%|████████  | 29/36 [00:12<00:02,  3.23it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 83%|████████▎ | 30/36 [00:12<00:01,  3.11it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 86%|████████▌ | 31/36 [00:13<00:01,  2.98it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 89%|████████▉ | 32/36 [00:13<00:01,  2.70it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 92%|█████████▏| 33/36 [00:14<00:01,  2.45it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 94%|█████████▍| 34/36 [00:14<00:00,  2.69it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            " 97%|█████████▋| 35/36 [00:14<00:00,  2.70it/s]/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "100%|██████████| 36/36 [00:15<00:00,  2.37it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:58] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:58] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:58] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:59] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:03:59] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\n",
            "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving scores to  eval/mle/adult/real.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/eval/mle/adult/synt.json', 'r') as myFile:\n",
        "    synthetic = json.load(myFile)"
      ],
      "metadata": {
        "id": "Whm5MZVA7VEj"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/eval/mle/adult/real.json', 'r') as myFile:\n",
        "    real = json.load(myFile)"
      ],
      "metadata": {
        "id": "ID3YoLMO76uA"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame({key: value[\"XGBClassifier\"] for key, value in real.items()})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "r9J_dk_28J37",
        "outputId": "03d0382a-4c2d-49c6-f825-c434ccba3cca"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             best_f1_scores  best_weighted_scores  best_auroc_scores  \\\n",
              "binary_f1          0.708704              0.708704           0.709768   \n",
              "roc_auc            0.925815              0.925815           0.926897   \n",
              "weighted_f1        0.758366              0.758366           0.759159   \n",
              "accuracy           0.873165              0.873165           0.873165   \n",
              "\n",
              "             best_acc_scores  best_avg_scores  \n",
              "binary_f1           0.708704         0.708704  \n",
              "roc_auc             0.925815         0.925815  \n",
              "weighted_f1         0.758366         0.758366  \n",
              "accuracy            0.873165         0.873165  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-905ef875-9550-453d-9c44-1db87aa25342\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>best_f1_scores</th>\n",
              "      <th>best_weighted_scores</th>\n",
              "      <th>best_auroc_scores</th>\n",
              "      <th>best_acc_scores</th>\n",
              "      <th>best_avg_scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>binary_f1</th>\n",
              "      <td>0.708704</td>\n",
              "      <td>0.708704</td>\n",
              "      <td>0.709768</td>\n",
              "      <td>0.708704</td>\n",
              "      <td>0.708704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roc_auc</th>\n",
              "      <td>0.925815</td>\n",
              "      <td>0.925815</td>\n",
              "      <td>0.926897</td>\n",
              "      <td>0.925815</td>\n",
              "      <td>0.925815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weighted_f1</th>\n",
              "      <td>0.758366</td>\n",
              "      <td>0.758366</td>\n",
              "      <td>0.759159</td>\n",
              "      <td>0.758366</td>\n",
              "      <td>0.758366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>0.873165</td>\n",
              "      <td>0.873165</td>\n",
              "      <td>0.873165</td>\n",
              "      <td>0.873165</td>\n",
              "      <td>0.873165</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-905ef875-9550-453d-9c44-1db87aa25342')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-905ef875-9550-453d-9c44-1db87aa25342 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-905ef875-9550-453d-9c44-1db87aa25342');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7b8d8ee3-1977-4452-b14c-751910a25c5e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7b8d8ee3-1977-4452-b14c-751910a25c5e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7b8d8ee3-1977-4452-b14c-751910a25c5e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"best_f1_scores\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10026688093551513,\n        \"min\": 0.7087036253350261,\n        \"max\": 0.9258148613037404,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.9258148613037404,\n          0.8731650390025183,\n          0.7087036253350261\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_weighted_scores\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10026688093551513,\n        \"min\": 0.7087036253350261,\n        \"max\": 0.9258148613037404,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.9258148613037404,\n          0.8731650390025183,\n          0.7087036253350261\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_auroc_scores\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10012632617066808,\n        \"min\": 0.7097680955727337,\n        \"max\": 0.9268968108945508,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.9268968108945508,\n          0.8731650390025183,\n          0.7097680955727337\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_acc_scores\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10026688093551513,\n        \"min\": 0.7087036253350261,\n        \"max\": 0.9258148613037404,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.9258148613037404,\n          0.8731650390025183,\n          0.7087036253350261\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_avg_scores\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10026688093551513,\n        \"min\": 0.7087036253350261,\n        \"max\": 0.9258148613037404,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.9258148613037404,\n          0.8731650390025183,\n          0.7087036253350261\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame({key: value[\"XGBClassifier\"] for key, value in synthetic.items()})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "8kpTA63-9cvl",
        "outputId": "ccd362c7-ea91-4b7e-e4ba-eb46dd4c7754"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             best_f1_scores  best_weighted_scores  best_auroc_scores  \\\n",
              "binary_f1          0.668779              0.668779           0.668779   \n",
              "roc_auc            0.911552              0.911552           0.911552   \n",
              "weighted_f1        0.725813              0.725813           0.725813   \n",
              "accuracy           0.858731              0.858731           0.858731   \n",
              "\n",
              "             best_acc_scores  best_avg_scores  \n",
              "binary_f1           0.668779         0.668779  \n",
              "roc_auc             0.911552         0.911552  \n",
              "weighted_f1         0.725813         0.725813  \n",
              "accuracy            0.858731         0.858731  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e22287f-9162-4935-8b1c-e4fd7efb6485\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>best_f1_scores</th>\n",
              "      <th>best_weighted_scores</th>\n",
              "      <th>best_auroc_scores</th>\n",
              "      <th>best_acc_scores</th>\n",
              "      <th>best_avg_scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>binary_f1</th>\n",
              "      <td>0.668779</td>\n",
              "      <td>0.668779</td>\n",
              "      <td>0.668779</td>\n",
              "      <td>0.668779</td>\n",
              "      <td>0.668779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roc_auc</th>\n",
              "      <td>0.911552</td>\n",
              "      <td>0.911552</td>\n",
              "      <td>0.911552</td>\n",
              "      <td>0.911552</td>\n",
              "      <td>0.911552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weighted_f1</th>\n",
              "      <td>0.725813</td>\n",
              "      <td>0.725813</td>\n",
              "      <td>0.725813</td>\n",
              "      <td>0.725813</td>\n",
              "      <td>0.725813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>0.858731</td>\n",
              "      <td>0.858731</td>\n",
              "      <td>0.858731</td>\n",
              "      <td>0.858731</td>\n",
              "      <td>0.858731</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e22287f-9162-4935-8b1c-e4fd7efb6485')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2e22287f-9162-4935-8b1c-e4fd7efb6485 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2e22287f-9162-4935-8b1c-e4fd7efb6485');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-272e8d70-ac38-4c34-8853-dcbbeb2b41e2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-272e8d70-ac38-4c34-8853-dcbbeb2b41e2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-272e8d70-ac38-4c34-8853-dcbbeb2b41e2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"best_f1_scores\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11300055909212486,\n        \"min\": 0.668778801843318,\n        \"max\": 0.9115519160372365,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.9115519160372365,\n          0.858731036177139,\n          0.668778801843318\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_weighted_scores\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11300055909212486,\n        \"min\": 0.668778801843318,\n        \"max\": 0.9115519160372365,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.9115519160372365,\n          0.858731036177139,\n          0.668778801843318\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_auroc_scores\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11300055909212486,\n        \"min\": 0.668778801843318,\n        \"max\": 0.9115519160372365,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.9115519160372365,\n          0.858731036177139,\n          0.668778801843318\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_acc_scores\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11300055909212486,\n        \"min\": 0.668778801843318,\n        \"max\": 0.9115519160372365,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.9115519160372365,\n          0.858731036177139,\n          0.668778801843318\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"best_avg_scores\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11300055909212486,\n        \"min\": 0.668778801843318,\n        \"max\": 0.9115519160372365,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.9115519160372365,\n          0.858731036177139,\n          0.668778801843318\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Померим качество!"
      ],
      "metadata": {
        "id": "XrjvUbRigW4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install synthcity"
      ],
      "metadata": {
        "id": "xcTLddwiiRLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "# sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
        "# from utils_train import preprocess, TabularDataset\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from synthcity.metrics import eval_detection, eval_performance, eval_statistical\n",
        "from synthcity.plugins.core.dataloader import GenericDataLoader\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--dataname', type=str, default='adult')\n",
        "parser.add_argument('--model', type=str, default='model')\n",
        "parser.add_argument('--path', type=str, default = None, help='The file path of the synthetic data')\n",
        "\n",
        "\n",
        "\n",
        "dataname = 'adult'\n",
        "model = 'model'\n",
        "\n",
        "syn_path = 'sample/sample.csv'#f'synthetic/{dataname}/.csv'\n",
        "\n",
        "real_path = f'synthetic/{dataname}/real.csv'\n",
        "\n",
        "data_dir = f'data/{dataname}'\n",
        "\n",
        "print(syn_path)\n",
        "\n",
        "\n",
        "with open(f'{data_dir}/info.json', 'r') as f:\n",
        "    info = json.load(f)\n",
        "\n",
        "syn_data = pd.read_csv(syn_path)\n",
        "real_data = pd.read_csv(real_path)\n",
        "\n",
        "\n",
        "''' Special treatment for default dataset and CoDi model '''\n",
        "\n",
        "real_data.columns = range(len(real_data.columns))\n",
        "syn_data.columns = range(len(syn_data.columns))\n",
        "\n",
        "num_col_idx = info['num_col_idx']\n",
        "cat_col_idx = info['cat_col_idx']\n",
        "target_col_idx = info['target_col_idx']\n",
        "if info['task_type'] == 'regression':\n",
        "    num_col_idx += target_col_idx\n",
        "else:\n",
        "    cat_col_idx += target_col_idx\n",
        "\n",
        "num_real_data = real_data[num_col_idx]\n",
        "cat_real_data = real_data[cat_col_idx]\n",
        "\n",
        "num_real_data_np = num_real_data.to_numpy()\n",
        "cat_real_data_np = cat_real_data.to_numpy().astype('str')\n",
        "\n",
        "\n",
        "num_syn_data = syn_data[num_col_idx]\n",
        "cat_syn_data = syn_data[cat_col_idx]\n",
        "\n",
        "num_syn_data_np = num_syn_data.to_numpy()\n",
        "\n",
        "# cat_syn_data_np = np.array\n",
        "cat_syn_data_np = cat_syn_data.to_numpy().astype('str')\n",
        "if (dataname == 'default' or dataname == 'news') and model[:4] == 'codi':\n",
        "    cat_syn_data_np = cat_syn_data.astype('int').to_numpy().astype('str')\n",
        "\n",
        "elif model[:5] == 'great':\n",
        "    if dataname == 'shoppers':\n",
        "        cat_syn_data_np[:, 1] = cat_syn_data[11].astype('int').to_numpy().astype('str')\n",
        "        cat_syn_data_np[:, 2] = cat_syn_data[12].astype('int').to_numpy().astype('str')\n",
        "        cat_syn_data_np[:, 3] = cat_syn_data[13].astype('int').to_numpy().astype('str')\n",
        "\n",
        "        max_data = cat_real_data[14].max()\n",
        "\n",
        "        cat_syn_data.loc[cat_syn_data[14] > max_data, 14] = max_data\n",
        "        # cat_syn_data[14] = cat_syn_data[14].apply(lambda x: threshold if x > max_data else x)\n",
        "\n",
        "        cat_syn_data_np[:, 4] = cat_syn_data[14].astype('int').to_numpy().astype('str')\n",
        "        cat_syn_data_np[:, 4] = cat_syn_data[14].astype('int').to_numpy().astype('str')\n",
        "\n",
        "    elif dataname in ['default', 'faults', 'beijing']:\n",
        "\n",
        "        columns = cat_real_data.columns\n",
        "        for i, col in enumerate(columns):\n",
        "            if (cat_real_data[col].dtype == 'int'):\n",
        "\n",
        "                max_data = cat_real_data[col].max()\n",
        "                min_data = cat_real_data[col].min()\n",
        "\n",
        "                cat_syn_data.loc[cat_syn_data[col] > max_data, col] = max_data\n",
        "                cat_syn_data.loc[cat_syn_data[col] < min_data, col] = min_data\n",
        "\n",
        "                cat_syn_data_np[:, i] = cat_syn_data[col].astype('int').to_numpy().astype('str')\n",
        "\n",
        "    else:\n",
        "        cat_syn_data_np = cat_syn_data.to_numpy().astype('str')\n",
        "\n",
        "else:\n",
        "    cat_syn_data_np = cat_syn_data.to_numpy().astype('str')\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "encoder.fit(cat_real_data_np)\n",
        "\n",
        "\n",
        "cat_real_data_oh = encoder.transform(cat_real_data_np).toarray()\n",
        "cat_syn_data_oh = encoder.transform(cat_syn_data_np).toarray()\n",
        "\n",
        "le_real_data = pd.DataFrame(np.concatenate((num_real_data_np, cat_real_data_oh), axis = 1)).astype(float)\n",
        "le_real_num = pd.DataFrame(num_real_data_np).astype(float)\n",
        "le_real_cat = pd.DataFrame(cat_real_data_oh).astype(float)\n",
        "\n",
        "\n",
        "le_syn_data = pd.DataFrame(np.concatenate((num_syn_data_np, cat_syn_data_oh), axis = 1)).astype(float)\n",
        "le_syn_num = pd.DataFrame(num_syn_data_np).astype(float)\n",
        "le_syn_cat = pd.DataFrame(cat_syn_data_oh).astype(float)\n",
        "\n",
        "np.set_printoptions(precision=4)\n",
        "\n",
        "result = []\n",
        "\n",
        "print('=========== All Features ===========')\n",
        "print('Data shape: ', le_syn_data.shape)\n",
        "\n",
        "X_syn_loader = GenericDataLoader(le_syn_data)\n",
        "X_real_loader = GenericDataLoader(le_real_data)\n",
        "\n",
        "quality_evaluator = eval_statistical.AlphaPrecision()\n",
        "qual_res = quality_evaluator.evaluate(X_real_loader, X_syn_loader)\n",
        "qual_res = {\n",
        "    k: v for (k, v) in qual_res.items() if \"naive\" in k\n",
        "}  # use the naive implementation of AlphaPrecision\n",
        "qual_score = np.mean(list(qual_res.values()))\n",
        "\n",
        "print('alpha precision: {:.6f}, beta recall: {:.6f}'.format(qual_res['delta_precision_alpha_naive'], qual_res['delta_coverage_beta_naive'] ))\n",
        "\n",
        "Alpha_Precision_all = qual_res['delta_precision_alpha_naive']\n",
        "Beta_Recall_all = qual_res['delta_coverage_beta_naive']\n",
        "\n",
        "save_dir = f'eval/quality/{dataname}'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "with open(f'{save_dir}/{model}.txt', 'w') as f:\n",
        "    f.write(f'{Alpha_Precision_all}\\n')\n",
        "    f.write(f'{Beta_Recall_all}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z69ByWyugYeG",
        "outputId": "85765e85-6626-4d2d-9c03-7b1d715e8469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample/sample.csv\n",
            "=========== All Features ===========\n",
            "Data shape:  (32561, 110)\n",
            "alpha precision: 0.987255, beta recall: 0.460533\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U4TZATpLhA9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "U0iqEiRTs7eW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V1A2E9F_s9_2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}