# -*- coding: utf-8 -*-
"""vae_mle.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j1frMWXmA-EgdKYqXhEGndmsnNGz1VKX
"""

import torch.nn as nn
import torch
import torch.nn.init as nn_init
import torch.nn.functional as F
from torch import Tensor
import typing as ty
import math

class Tokenizer(nn.Module):

    def __init__(self, d_numerical, categories, d_token, bias):
        super().__init__()
        if categories is None:
            d_bias = d_numerical
            self.category_offsets = None
            self.category_embeddings = None
        else:
            d_bias = d_numerical + len(categories)
            category_offsets = torch.tensor([0] + categories[:-1]).cumsum(0)
            self.register_buffer('category_offsets', category_offsets)
            self.category_embeddings = nn.Embedding(sum(categories), d_token)
            nn_init.kaiming_uniform_(self.category_embeddings.weight, a=math.sqrt(5))
            print(f'{self.category_embeddings.weight.shape=}')

        # take [CLS] token into account
        self.weight = nn.Parameter(Tensor(d_numerical + 1, d_token))
        self.bias = nn.Parameter(Tensor(d_bias, d_token)) if bias else None
        # The initialization is inspired by nn.Linear
        nn_init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            nn_init.kaiming_uniform_(self.bias, a=math.sqrt(5))

    @property
    def n_tokens(self):
        return len(self.weight) + (
            0 if self.category_offsets is None else len(self.category_offsets)
        )

    def forward(self, x_num, x_cat):
        x_some = x_num if x_cat is None else x_cat
        assert x_some is not None
        x_num = torch.cat(
            [torch.ones(len(x_some), 1, device=x_some.device)]  # [CLS]
            + ([] if x_num is None else [x_num]),
            dim=1,
        )

        x = self.weight[None] * x_num[:, :, None]

        if x_cat is not None:
            x = torch.cat(
                [x, self.category_embeddings(x_cat + self.category_offsets[None])],
                dim=1,
            )
        if self.bias is not None:
            bias = torch.cat(
                [
                    torch.zeros(1, self.bias.shape[1], device=x.device),
                    self.bias,
                ]
            )
            x = x + bias[None]

        return x


class LM(nn.Module):

    def __init__(
        self,
        token: int,
        n_col: int
    ):
        super().__init__()
        self.Flatten = nn.Flatten()
        self.Linear_1 = nn.Linear(token*n_col, token*n_col*8)
        self.Drop = nn.Dropout(p=0.2)
        self.ReLu1 = nn.ReLU()

        self.Linear_4 = nn.Linear( token*n_col*8, token*n_col)
        self.Norm =nn.LayerNorm(token*n_col, eps=1e-05, elementwise_affine=True)

        self.Unflatten = nn.Unflatten(1, (n_col, token))


    def forward(self, x):
        x = self.Flatten(x)
        lin1 = self.Linear_1(x)
        dr = self.Drop(lin1)
        rel1 = self.ReLu1(dr)

        lin4 = self.Linear_4(rel1)
        norm = self.Norm(lin4)

        return self.Unflatten(norm)

class VAE(nn.Module):
    def __init__(self, d_numerical, categories, hid_dim, bias = True):
        super(VAE, self).__init__()

        self.d_numerical = d_numerical
        self.categories = categories
        self.hid_dim = hid_dim
        d_token = hid_dim
        self.d_dim = self.d_numerical + len(self.categories)

        self.Tokenizer = Tokenizer(d_numerical, categories, d_token, bias = bias)#4 = d_token
        self.encoder_mu = LM(hid_dim, (self.d_dim+1))
        self.encoder_logvar = LM(hid_dim,(self.d_dim+1))

        self.decoder = LM(hid_dim,self.d_dim)

    def get_embedding(self, x):
        return self.encoder_mu(x, x).detach()

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x_num, x_cat):
        x = self.Tokenizer(x_num, x_cat)
        mu_z = self.encoder_mu(x)
        std_z = self.encoder_logvar(x)

        z = self.reparameterize(mu_z, std_z)

        h = self.decoder(z[:,1:])

        return h, mu_z, std_z

class Reconstructor(nn.Module):
    def __init__(self, d_numerical, categories, d_token):
        super(Reconstructor, self).__init__()

        self.d_numerical = d_numerical
        self.categories = categories
        self.d_token = d_token

        self.weight = nn.Parameter(Tensor(d_numerical, d_token))
        nn.init.xavier_uniform_(self.weight, gain=1 / math.sqrt(2))
        self.cat_recons = nn.ModuleList()

        for d in categories:
            recon = nn.Linear(d_token, d)
            nn.init.xavier_uniform_(recon.weight, gain=1 / math.sqrt(2))
            self.cat_recons.append(recon)

    def forward(self, h):
        h_num  = h[:, :self.d_numerical]
        h_cat  = h[:, self.d_numerical:]

        recon_x_num = torch.mul(h_num, self.weight.unsqueeze(0)).sum(-1)
        recon_x_cat = []

        for i, recon in enumerate(self.cat_recons):

            recon_x_cat.append(recon(h_cat[:, i]))

        return recon_x_num, recon_x_cat

class Model_VAE(nn.Module):
    def __init__(self, d_numerical, categories, d_token, bias = True):
        super(Model_VAE, self).__init__()
        self.d_numerical = d_numerical
        self.d_categories = len(categories)
        self.d_token = d_token

        self.VAE = VAE(d_numerical, categories, d_token,  bias = bias)
        self.Reconstructor = Reconstructor(d_numerical, categories, d_token)

    def get_embedding(self, x_num, x_cat):
        x = self.Tokenizer(x_num, x_cat)

        return self.VAE.get_embedding(x)

    def forward(self, x_num, x_cat):

        h, mu_z, std_z = self.VAE(x_num, x_cat)
        recon_x_num, recon_x_cat = self.Reconstructor(h)

        return recon_x_num, recon_x_cat, mu_z, std_z

class Encoder_model(nn.Module):
    def __init__(self, d_numerical, categories, d_token, bias = True):
        super(Encoder_model, self).__init__()
        self.d_numerical = d_numerical
        self.d_categories = len(categories)
        self.d_token = d_token

        self.Tokenizer = Tokenizer(d_numerical, categories, d_token, bias);#4
        self.VAE_Encoder = LM(d_token, self.d_numerical + self.d_categories + 1)

    def load_weights(self, Pretrained_VAE):
        self.Tokenizer.load_state_dict(Pretrained_VAE.VAE.Tokenizer.state_dict())
        self.VAE_Encoder.load_state_dict(Pretrained_VAE.VAE.encoder_mu.state_dict())

    def forward(self, x_num, x_cat):
        x = self.Tokenizer(x_num, x_cat)
        z = self.VAE_Encoder(x)

        return z

class Decoder_model(nn.Module):
    def __init__(self, d_numerical, categories, d_token, bias = True):
        super(Decoder_model, self).__init__()
        self.d_numerical = d_numerical
        self.d_categories = len(categories)
        self.d_token = d_token
        self.VAE_Decoder = LM(d_token, self.d_numerical + self.d_categories)
        self.Detokenizer = Reconstructor(d_numerical, categories, d_token)

    def load_weights(self, Pretrained_VAE):
        self.VAE_Decoder.load_state_dict(Pretrained_VAE.VAE.decoder.state_dict())
        self.Detokenizer.load_state_dict(Pretrained_VAE.Reconstructor.state_dict())

    def forward(self, z):

        h = self.VAE_Decoder(z)

        x_hat_num, x_hat_cat = self.Detokenizer(h)
        return x_hat_num, x_hat_cat