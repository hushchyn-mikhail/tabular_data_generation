{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQvmHt975qPb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install ucimlrepo\n",
        "!pip install ml_collections"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/JayoungKim408/STaSy.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzChOjOIienh",
        "outputId": "492ed159-31a0-41ac-8ff4-8c26226f7133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'STaSy'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 61 (delta 10), reused 9 (delta 9), pack-reused 50 (from 1)\u001b[K\n",
            "Receiving objects: 100% (61/61), 448.88 KiB | 16.63 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd STaSy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP1RaOikZWZ1",
        "outputId": "cc3b982c-c577-4cd9-aeb9-b37b88a12128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/STaSy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from prepare_dataset_utils import CATEGORICAL, CONTINUOUS, ORDINAL, verify\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "\n",
        "output_dir = 'tabular_datasets'\n",
        "name = \"adult\"\n",
        "\n",
        "def project_table(data, meta):\n",
        "    values = np.zeros(shape=data.shape, dtype='float32')\n",
        "\n",
        "    for id_, info in enumerate(meta):\n",
        "        if info['type'] == CONTINUOUS:\n",
        "            values[:, id_] = data.iloc[:, id_].values.astype('float32')\n",
        "        else:\n",
        "            mapper = dict([(item, id) for id, item in enumerate(info['i2s'])])\n",
        "            mapped = data.iloc[:, id_].apply(lambda x: mapper[x]).values\n",
        "            values[:, id_] = mapped\n",
        "\n",
        "    return values\n",
        "\n",
        "def main():\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Загрузка датасета\n",
        "    dataset = fetch_ucirepo(id=2)\n",
        "    X = dataset.data.features\n",
        "    y = dataset.data.targets\n",
        "\n",
        "    df = pd.concat([X, y], axis=1)\n",
        "\n",
        "    # Определяем типы столбцов (пример для Adult)\n",
        "    col_type = [\n",
        "        ('age', CONTINUOUS),\n",
        "        ('workclass', CATEGORICAL),\n",
        "        ('fnlwgt', CONTINUOUS),\n",
        "        ('education', CATEGORICAL),\n",
        "        ('education-num', CONTINUOUS),\n",
        "        ('marital-status', CATEGORICAL),\n",
        "        ('occupation', CATEGORICAL),\n",
        "        ('relationship', CATEGORICAL),\n",
        "        ('race', CATEGORICAL),\n",
        "        ('sex', CATEGORICAL),\n",
        "        ('capital-gain', CONTINUOUS),\n",
        "        ('capital-loss', CONTINUOUS),\n",
        "        ('hours-per-week', CONTINUOUS),\n",
        "        ('native-country', CATEGORICAL),\n",
        "        ('income', CATEGORICAL)\n",
        "    ]\n",
        "\n",
        "    df = df.replace('?', np.nan).dropna()\n",
        "\n",
        "    # Создаем метаданные\n",
        "    meta = []\n",
        "    for id_, info in enumerate(col_type):\n",
        "        col_name = info[0]\n",
        "        col_data = df[col_name]\n",
        "\n",
        "        if info[1] == CONTINUOUS:\n",
        "            meta.append({\n",
        "                \"name\": col_name,\n",
        "                \"type\": CONTINUOUS,\n",
        "                \"min\": float(col_data.min()),\n",
        "                \"max\": float(col_data.max())\n",
        "            })\n",
        "        else:\n",
        "            categories = list(col_data.unique())\n",
        "            meta.append({\n",
        "                \"name\": col_name,\n",
        "                \"type\": CATEGORICAL,\n",
        "                \"size\": len(categories),\n",
        "                \"i2s\": categories\n",
        "            })\n",
        "\n",
        "    # Преобразование данных\n",
        "    tdata = project_table(df, meta)\n",
        "\n",
        "    # Конфигурация\n",
        "    config = {\n",
        "        \"columns\": meta,\n",
        "        \"problem_type\": \"binary_classification\"\n",
        "    }\n",
        "\n",
        "    # Разделение данных\n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(tdata)\n",
        "\n",
        "    split_ratio = int(tdata.shape[0] * 0.8)\n",
        "    train_data = tdata[:split_ratio]\n",
        "    test_data = tdata[split_ratio:]\n",
        "\n",
        "    with open(f\"{output_dir}/{name}.json\", \"w\") as f:\n",
        "        json.dump(config, f, indent=4)\n",
        "\n",
        "    np.savez(f\"{output_dir}/{name}.npz\", train=train_data, test=test_data)\n",
        "\n",
        "    verify(f\"{output_dir}/{name}.npz\", f\"{output_dir}/{name}.json\")"
      ],
      "metadata": {
        "id": "3NSrgtKIfaI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "F7ms48_7fjsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --config configs/adult.py --mode train --workdir stasy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSC91ll5aFU8",
        "outputId": "99f7d8d4-ad18-4144-fcaf-856eae92a713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-29 18:05:08.267074: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745949908.287650    1384 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745949908.293966    1384 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-29 18:05:08.313829: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "the number of parameters 46660\n",
            "W0429 18:05:22.503205 136791537222272 utils.py:12] No checkpoint found at stasy/checkpoints-meta/checkpoint.pth. Returned the same state as input\n",
            "I0429 18:05:22.669830 136791537222272 run_lib.py:84] train shape : (36177, 108)\n",
            "I0429 18:05:22.670057 136791537222272 run_lib.py:85] eval.shape : (9045, 108)\n",
            "I0429 18:05:22.670167 136791537222272 run_lib.py:87] batch size: 1000\n",
            "I0429 18:05:22.752126 136791537222272 run_lib.py:90] raw data : Counter({np.float64(0.0): 18124, np.float64(2.0): 9092, np.float64(1.0): 5996, np.float64(3.0): 2965})\n",
            "I0429 18:05:22.771230 136791537222272 run_lib.py:110] DataParallel(\n",
            "  (module): NCSNpp(\n",
            "    (act): ELU(alpha=1.0)\n",
            "    (all_modules): ModuleList(\n",
            "      (0): GaussianFourierProjection()\n",
            "      (1): Linear(in_features=16, out_features=32, bias=True)\n",
            "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (3): ConcatSquashLinear(\n",
            "        (_layer): Linear(in_features=108, out_features=32, bias=True)\n",
            "        (_hyper_bias): Linear(in_features=1, out_features=32, bias=False)\n",
            "        (_hyper_gate): Linear(in_features=1, out_features=32, bias=True)\n",
            "      )\n",
            "      (4): ELU(alpha=1.0)\n",
            "      (5): ConcatSquashLinear(\n",
            "        (_layer): Linear(in_features=140, out_features=64, bias=True)\n",
            "        (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)\n",
            "        (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)\n",
            "      )\n",
            "      (6): ELU(alpha=1.0)\n",
            "      (7): ConcatSquashLinear(\n",
            "        (_layer): Linear(in_features=204, out_features=32, bias=True)\n",
            "        (_hyper_bias): Linear(in_features=1, out_features=32, bias=False)\n",
            "        (_hyper_gate): Linear(in_features=1, out_features=32, bias=True)\n",
            "      )\n",
            "      (8): ELU(alpha=1.0)\n",
            "      (9): Linear(in_features=236, out_features=108, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "I0429 18:05:22.771825 136791537222272 run_lib.py:132] Starting training loop at epoch 0.\n",
            "/content/STaSy/losses.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  q_alpha = torch.tensor(alpha0 + torch.log( torch.tensor(1+ 0.0001718*state['step']* (1-alpha0), dtype=torch.float32) )).clamp_(max=1).to(nll.device)\n",
            "/content/STaSy/losses.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  q_beta = torch.tensor(beta0 + torch.log( torch.tensor(1+ 0.0001718*state['step']* (1-beta0), dtype=torch.float32) )).clamp_(max=1).to(nll.device)\n",
            "I0429 18:05:23.220479 136791537222272 losses.py:148] q_alpha: 0.30000001192092896, q_beta: 0.949999988079071\n",
            "I0429 18:05:23.695363 136791537222272 losses.py:157] alpha: 0.9773567914962769, beta: 337.1765441894531\n",
            "I0429 18:05:23.721516 136791537222272 losses.py:158] 1 samples: 301 / 1000\n",
            "I0429 18:05:23.721927 136791537222272 losses.py:159] weighted samples: 648 / 1000\n",
            "I0429 18:05:23.722185 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.243558 136791537222272 losses.py:148] q_alpha: 0.3001202940940857, q_beta: 0.9500085711479187\n",
            "I0429 18:05:24.249674 136791537222272 losses.py:157] alpha: 0.9994564652442932, beta: 285.9529724121094\n",
            "I0429 18:05:24.250436 136791537222272 losses.py:158] 1 samples: 301 / 1000\n",
            "I0429 18:05:24.250749 136791537222272 losses.py:159] weighted samples: 648 / 1000\n",
            "I0429 18:05:24.251053 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.268134 136791537222272 losses.py:148] q_alpha: 0.30024054646492004, q_beta: 0.9500171542167664\n",
            "I0429 18:05:24.270329 136791537222272 losses.py:157] alpha: 0.9786422252655029, beta: 364.6722106933594\n",
            "I0429 18:05:24.270615 136791537222272 losses.py:158] 1 samples: 301 / 1000\n",
            "I0429 18:05:24.270907 136791537222272 losses.py:159] weighted samples: 648 / 1000\n",
            "I0429 18:05:24.271160 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.285854 136791537222272 losses.py:148] q_alpha: 0.30036067962646484, q_beta: 0.950025737285614\n",
            "I0429 18:05:24.288008 136791537222272 losses.py:157] alpha: 0.9838123321533203, beta: 279.0684509277344\n",
            "I0429 18:05:24.288293 136791537222272 losses.py:158] 1 samples: 302 / 1000\n",
            "I0429 18:05:24.288535 136791537222272 losses.py:159] weighted samples: 647 / 1000\n",
            "I0429 18:05:24.288789 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.317252 136791537222272 losses.py:148] q_alpha: 0.3004809021949768, q_beta: 0.9500343203544617\n",
            "I0429 18:05:24.319475 136791537222272 losses.py:157] alpha: 0.9757707715034485, beta: 320.6861877441406\n",
            "I0429 18:05:24.319744 136791537222272 losses.py:158] 1 samples: 302 / 1000\n",
            "I0429 18:05:24.320027 136791537222272 losses.py:159] weighted samples: 647 / 1000\n",
            "I0429 18:05:24.320272 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.334660 136791537222272 losses.py:148] q_alpha: 0.30060112476348877, q_beta: 0.9500429034233093\n",
            "I0429 18:05:24.336929 136791537222272 losses.py:157] alpha: 0.9751455783843994, beta: 216.09344482421875\n",
            "I0429 18:05:24.337225 136791537222272 losses.py:158] 1 samples: 302 / 1000\n",
            "I0429 18:05:24.337520 136791537222272 losses.py:159] weighted samples: 647 / 1000\n",
            "I0429 18:05:24.337761 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.352502 136791537222272 losses.py:148] q_alpha: 0.30072131752967834, q_beta: 0.950051486492157\n",
            "I0429 18:05:24.355024 136791537222272 losses.py:157] alpha: 0.9803311228752136, beta: 264.7118835449219\n",
            "I0429 18:05:24.355309 136791537222272 losses.py:158] 1 samples: 302 / 1000\n",
            "I0429 18:05:24.355567 136791537222272 losses.py:159] weighted samples: 647 / 1000\n",
            "I0429 18:05:24.355796 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.370881 136791537222272 losses.py:148] q_alpha: 0.3008415102958679, q_beta: 0.9500600695610046\n",
            "I0429 18:05:24.373200 136791537222272 losses.py:157] alpha: 0.9791305661201477, beta: 306.1389465332031\n",
            "I0429 18:05:24.373476 136791537222272 losses.py:158] 1 samples: 302 / 1000\n",
            "I0429 18:05:24.373755 136791537222272 losses.py:159] weighted samples: 647 / 1000\n",
            "I0429 18:05:24.374000 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.389231 136791537222272 losses.py:148] q_alpha: 0.3009616732597351, q_beta: 0.9500686526298523\n",
            "I0429 18:05:24.391520 136791537222272 losses.py:157] alpha: 0.9768528938293457, beta: 236.69366455078125\n",
            "I0429 18:05:24.391804 136791537222272 losses.py:158] 1 samples: 302 / 1000\n",
            "I0429 18:05:24.392092 136791537222272 losses.py:159] weighted samples: 647 / 1000\n",
            "I0429 18:05:24.392318 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.408422 136791537222272 losses.py:148] q_alpha: 0.30108171701431274, q_beta: 0.9500773549079895\n",
            "I0429 18:05:24.410705 136791537222272 losses.py:157] alpha: 0.9767236113548279, beta: 275.3147888183594\n",
            "I0429 18:05:24.410991 136791537222272 losses.py:158] 1 samples: 302 / 1000\n",
            "I0429 18:05:24.411288 136791537222272 losses.py:159] weighted samples: 647 / 1000\n",
            "I0429 18:05:24.411519 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.426903 136791537222272 losses.py:148] q_alpha: 0.30120187997817993, q_beta: 0.9500859379768372\n",
            "I0429 18:05:24.429183 136791537222272 losses.py:157] alpha: 0.9842351675033569, beta: 298.3666076660156\n",
            "I0429 18:05:24.429455 136791537222272 losses.py:158] 1 samples: 302 / 1000\n",
            "I0429 18:05:24.429732 136791537222272 losses.py:159] weighted samples: 647 / 1000\n",
            "I0429 18:05:24.429977 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.445401 136791537222272 losses.py:148] q_alpha: 0.30132201313972473, q_beta: 0.9500945210456848\n",
            "I0429 18:05:24.447782 136791537222272 losses.py:157] alpha: 0.9848504662513733, beta: 248.1520233154297\n",
            "I0429 18:05:24.448083 136791537222272 losses.py:158] 1 samples: 303 / 1000\n",
            "I0429 18:05:24.448368 136791537222272 losses.py:159] weighted samples: 646 / 1000\n",
            "I0429 18:05:24.448614 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.472158 136791537222272 losses.py:148] q_alpha: 0.30144211649894714, q_beta: 0.9501031041145325\n",
            "I0429 18:05:24.476561 136791537222272 losses.py:157] alpha: 0.99164879322052, beta: 367.3521728515625\n",
            "I0429 18:05:24.479294 136791537222272 losses.py:158] 1 samples: 303 / 1000\n",
            "I0429 18:05:24.479573 136791537222272 losses.py:159] weighted samples: 646 / 1000\n",
            "I0429 18:05:24.479811 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.495728 136791537222272 losses.py:148] q_alpha: 0.30156221985816956, q_beta: 0.9501116871833801\n",
            "I0429 18:05:24.498010 136791537222272 losses.py:157] alpha: 0.9773889183998108, beta: 255.1080322265625\n",
            "I0429 18:05:24.498306 136791537222272 losses.py:158] 1 samples: 303 / 1000\n",
            "I0429 18:05:24.498572 136791537222272 losses.py:159] weighted samples: 646 / 1000\n",
            "I0429 18:05:24.498807 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.517166 136791537222272 losses.py:148] q_alpha: 0.30168217420578003, q_beta: 0.9501202702522278\n",
            "I0429 18:05:24.519270 136791537222272 losses.py:157] alpha: 0.9910153746604919, beta: 310.8404541015625\n",
            "I0429 18:05:24.519531 136791537222272 losses.py:158] 1 samples: 303 / 1000\n",
            "I0429 18:05:24.519816 136791537222272 losses.py:159] weighted samples: 646 / 1000\n",
            "I0429 18:05:24.520047 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.534843 136791537222272 losses.py:148] q_alpha: 0.30180224776268005, q_beta: 0.9501288533210754\n",
            "I0429 18:05:24.537454 136791537222272 losses.py:157] alpha: 0.9937096238136292, beta: 245.05577087402344\n",
            "I0429 18:05:24.537737 136791537222272 losses.py:158] 1 samples: 303 / 1000\n",
            "I0429 18:05:24.538010 136791537222272 losses.py:159] weighted samples: 646 / 1000\n",
            "I0429 18:05:24.538271 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.554472 136791537222272 losses.py:148] q_alpha: 0.3019223213195801, q_beta: 0.9501374363899231\n",
            "I0429 18:05:24.556726 136791537222272 losses.py:157] alpha: 0.9928554892539978, beta: 328.6778564453125\n",
            "I0429 18:05:24.557033 136791537222272 losses.py:158] 1 samples: 303 / 1000\n",
            "I0429 18:05:24.557382 136791537222272 losses.py:159] weighted samples: 646 / 1000\n",
            "I0429 18:05:24.557653 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.574812 136791537222272 losses.py:148] q_alpha: 0.3020423650741577, q_beta: 0.9501460194587708\n",
            "I0429 18:05:24.577000 136791537222272 losses.py:157] alpha: 0.9937281608581543, beta: 321.9261474609375\n",
            "I0429 18:05:24.577294 136791537222272 losses.py:158] 1 samples: 303 / 1000\n",
            "I0429 18:05:24.577559 136791537222272 losses.py:159] weighted samples: 646 / 1000\n",
            "I0429 18:05:24.577792 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.592230 136791537222272 losses.py:148] q_alpha: 0.30216237902641296, q_beta: 0.9501546025276184\n",
            "I0429 18:05:24.594435 136791537222272 losses.py:157] alpha: 0.9750627279281616, beta: 245.19944763183594\n",
            "I0429 18:05:24.594704 136791537222272 losses.py:158] 1 samples: 304 / 1000\n",
            "I0429 18:05:24.594989 136791537222272 losses.py:159] weighted samples: 645 / 1000\n",
            "I0429 18:05:24.595234 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.609944 136791537222272 losses.py:148] q_alpha: 0.30228230357170105, q_beta: 0.9501631855964661\n",
            "I0429 18:05:24.612219 136791537222272 losses.py:157] alpha: 0.9889829158782959, beta: 286.92059326171875\n",
            "I0429 18:05:24.612478 136791537222272 losses.py:158] 1 samples: 303 / 1000\n",
            "I0429 18:05:24.612730 136791537222272 losses.py:159] weighted samples: 646 / 1000\n",
            "I0429 18:05:24.612950 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.627473 136791537222272 losses.py:148] q_alpha: 0.3024022877216339, q_beta: 0.9501717686653137\n",
            "I0429 18:05:24.629659 136791537222272 losses.py:157] alpha: 0.9832450151443481, beta: 475.3459777832031\n",
            "I0429 18:05:24.629932 136791537222272 losses.py:158] 1 samples: 304 / 1000\n",
            "I0429 18:05:24.630256 136791537222272 losses.py:159] weighted samples: 645 / 1000\n",
            "I0429 18:05:24.630537 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.645712 136791537222272 losses.py:148] q_alpha: 0.3025222718715668, q_beta: 0.9501803517341614\n",
            "I0429 18:05:24.647898 136791537222272 losses.py:157] alpha: 1.0015889406204224, beta: 390.95989990234375\n",
            "I0429 18:05:24.648166 136791537222272 losses.py:158] 1 samples: 304 / 1000\n",
            "I0429 18:05:24.648418 136791537222272 losses.py:159] weighted samples: 645 / 1000\n",
            "I0429 18:05:24.648654 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.663212 136791537222272 losses.py:148] q_alpha: 0.30264225602149963, q_beta: 0.950188934803009\n",
            "I0429 18:05:24.665384 136791537222272 losses.py:157] alpha: 0.9762295484542847, beta: 316.3104553222656\n",
            "I0429 18:05:24.665634 136791537222272 losses.py:158] 1 samples: 304 / 1000\n",
            "I0429 18:05:24.665889 136791537222272 losses.py:159] weighted samples: 645 / 1000\n",
            "I0429 18:05:24.666150 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.680494 136791537222272 losses.py:148] q_alpha: 0.3027622103691101, q_beta: 0.9501975178718567\n",
            "I0429 18:05:24.682724 136791537222272 losses.py:157] alpha: 0.974804699420929, beta: 336.54986572265625\n",
            "I0429 18:05:24.683003 136791537222272 losses.py:158] 1 samples: 304 / 1000\n",
            "I0429 18:05:24.683370 136791537222272 losses.py:159] weighted samples: 645 / 1000\n",
            "I0429 18:05:24.683638 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.698702 136791537222272 losses.py:148] q_alpha: 0.3028821349143982, q_beta: 0.9502061009407043\n",
            "I0429 18:05:24.700983 136791537222272 losses.py:157] alpha: 0.9796879291534424, beta: 294.7484436035156\n",
            "I0429 18:05:24.701265 136791537222272 losses.py:158] 1 samples: 304 / 1000\n",
            "I0429 18:05:24.701529 136791537222272 losses.py:159] weighted samples: 645 / 1000\n",
            "I0429 18:05:24.701756 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.715997 136791537222272 losses.py:148] q_alpha: 0.3030019700527191, q_beta: 0.950214684009552\n",
            "I0429 18:05:24.718215 136791537222272 losses.py:157] alpha: 0.9831121563911438, beta: 180.57005310058594\n",
            "I0429 18:05:24.718499 136791537222272 losses.py:158] 1 samples: 304 / 1000\n",
            "I0429 18:05:24.718766 136791537222272 losses.py:159] weighted samples: 645 / 1000\n",
            "I0429 18:05:24.719000 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.734229 136791537222272 losses.py:148] q_alpha: 0.3031218647956848, q_beta: 0.9502233862876892\n",
            "I0429 18:05:24.736420 136791537222272 losses.py:157] alpha: 0.978306770324707, beta: 256.17401123046875\n",
            "I0429 18:05:24.736721 136791537222272 losses.py:158] 1 samples: 304 / 1000\n",
            "I0429 18:05:24.736997 136791537222272 losses.py:159] weighted samples: 645 / 1000\n",
            "I0429 18:05:24.737248 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.754526 136791537222272 losses.py:148] q_alpha: 0.3032417893409729, q_beta: 0.9502319693565369\n",
            "I0429 18:05:24.757243 136791537222272 losses.py:157] alpha: 0.9661763310432434, beta: 291.6808166503906\n",
            "I0429 18:05:24.757542 136791537222272 losses.py:158] 1 samples: 304 / 1000\n",
            "I0429 18:05:24.757865 136791537222272 losses.py:159] weighted samples: 645 / 1000\n",
            "I0429 18:05:24.758151 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.774469 136791537222272 losses.py:148] q_alpha: 0.3033616542816162, q_beta: 0.9502405524253845\n",
            "I0429 18:05:24.776990 136791537222272 losses.py:157] alpha: 0.9855296611785889, beta: 241.4188690185547\n",
            "I0429 18:05:24.777324 136791537222272 losses.py:158] 1 samples: 305 / 1000\n",
            "I0429 18:05:24.777661 136791537222272 losses.py:159] weighted samples: 644 / 1000\n",
            "I0429 18:05:24.777919 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.793962 136791537222272 losses.py:148] q_alpha: 0.3034815192222595, q_beta: 0.9502490758895874\n",
            "I0429 18:05:24.796219 136791537222272 losses.py:157] alpha: 0.9775593280792236, beta: 256.802734375\n",
            "I0429 18:05:24.796477 136791537222272 losses.py:158] 1 samples: 305 / 1000\n",
            "I0429 18:05:24.796761 136791537222272 losses.py:159] weighted samples: 644 / 1000\n",
            "I0429 18:05:24.796985 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.812356 136791537222272 losses.py:148] q_alpha: 0.3036012649536133, q_beta: 0.9502576589584351\n",
            "I0429 18:05:24.814600 136791537222272 losses.py:157] alpha: 0.9914866089820862, beta: 255.04229736328125\n",
            "I0429 18:05:24.814886 136791537222272 losses.py:158] 1 samples: 305 / 1000\n",
            "I0429 18:05:24.815162 136791537222272 losses.py:159] weighted samples: 644 / 1000\n",
            "I0429 18:05:24.815380 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.829658 136791537222272 losses.py:148] q_alpha: 0.3037211000919342, q_beta: 0.9502662420272827\n",
            "I0429 18:05:24.831851 136791537222272 losses.py:157] alpha: 0.9772619605064392, beta: 219.71702575683594\n",
            "I0429 18:05:24.832123 136791537222272 losses.py:158] 1 samples: 305 / 1000\n",
            "I0429 18:05:24.832385 136791537222272 losses.py:159] weighted samples: 644 / 1000\n",
            "I0429 18:05:24.832602 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.846647 136791537222272 losses.py:148] q_alpha: 0.3038409352302551, q_beta: 0.9502748250961304\n",
            "I0429 18:05:24.848693 136791537222272 losses.py:157] alpha: 0.9745919704437256, beta: 291.3427734375\n",
            "I0429 18:05:24.848963 136791537222272 losses.py:158] 1 samples: 305 / 1000\n",
            "I0429 18:05:24.849243 136791537222272 losses.py:159] weighted samples: 644 / 1000\n",
            "I0429 18:05:24.849473 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.863309 136791537222272 losses.py:148] q_alpha: 0.30396074056625366, q_beta: 0.950283408164978\n",
            "I0429 18:05:24.865899 136791537222272 losses.py:157] alpha: 0.9884148240089417, beta: 224.49534606933594\n",
            "I0429 18:05:24.866192 136791537222272 losses.py:158] 1 samples: 305 / 1000\n",
            "I0429 18:05:24.866435 136791537222272 losses.py:159] weighted samples: 644 / 1000\n",
            "I0429 18:05:24.866640 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.880596 136791537222272 losses.py:148] q_alpha: 0.3040805459022522, q_beta: 0.9502919912338257\n",
            "I0429 18:05:24.882688 136791537222272 losses.py:157] alpha: 0.9788094162940979, beta: 240.47267150878906\n",
            "I0429 18:05:24.882954 136791537222272 losses.py:158] 1 samples: 305 / 1000\n",
            "I0429 18:05:24.883228 136791537222272 losses.py:159] weighted samples: 644 / 1000\n",
            "I0429 18:05:24.883446 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.897438 136791537222272 losses.py:148] q_alpha: 0.3042002320289612, q_beta: 0.9503005743026733\n",
            "I0429 18:05:24.899556 136791537222272 losses.py:157] alpha: 0.9896051287651062, beta: 308.63592529296875\n",
            "I0429 18:05:24.899824 136791537222272 losses.py:158] 1 samples: 305 / 1000\n",
            "I0429 18:05:24.900122 136791537222272 losses.py:159] weighted samples: 644 / 1000\n",
            "I0429 18:05:24.900347 136791537222272 losses.py:160] 0 samples: 51 / 1000\n",
            "I0429 18:05:24.910148 136791537222272 losses.py:148] q_alpha: 0.30431997776031494, q_beta: 0.950309157371521\n",
            "I0429 18:05:24.912198 136791537222272 losses.py:157] alpha: 1.0185397863388062, beta: 395.66156005859375\n",
            "I0429 18:05:24.912432 136791537222272 losses.py:158] 1 samples: 55 / 177\n",
            "I0429 18:05:24.912667 136791537222272 losses.py:159] weighted samples: 112 / 177\n",
            "I0429 18:05:24.912875 136791537222272 losses.py:160] 0 samples: 10 / 177\n",
            "I0429 18:05:24.922589 136791537222272 run_lib.py:145] epoch: 0, iter: 36, training_loss: 9.42012e+00\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/STaSy/main.py\", line 92, in <module>\n",
            "    app.run(main)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/absl/app.py\", line 308, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/absl/app.py\", line 254, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "             ^^^^^^^^^^\n",
            "  File \"/content/STaSy/main.py\", line 59, in main\n",
            "    run_lib.train(FLAGS.config, FLAGS.workdir)\n",
            "  File \"/content/STaSy/run_lib.py\", line 150, in train\n",
            "    sample, n = sampling_fn(score_model, sampling_shape=sampling_shape)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/STaSy/sampling.py\", line 496, in ode_sampler\n",
            "    x = denoise_update_fn(model, x)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/STaSy/sampling.py\", line 448, in denoise_update_fn\n",
            "    _, x = predictor_obj.update_fn(x, vec_eps)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/STaSy/sampling.py\", line 196, in update_fn\n",
            "    f, G = self.rsde.discretize(x, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/STaSy/sde_lib.py\", line 105, in discretize\n",
            "    f, G = discretize_fn(x, t)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/STaSy/sde_lib.py\", line 264, in discretize\n",
            "    self.discrete_sigmas[timestep - 1].to(t.device))\n",
            "    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
            "RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)\n"
          ]
        }
      ]
    }
  ]
}